{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop:\n",
    "# 1. Create torch dataset from the data containing the query, positive, negative example.\n",
    "# 2. Create a dataloader from the dataset.\n",
    "# 3. Run a single batch through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following packages are already present in the pyproject.toml and will be skipped:\n",
      "\n",
      "  • \u001b[36mpandas\u001b[39m\n",
      "\n",
      "If you want to update it to the latest compatible version, you can use `poetry update package`.\n",
      "If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.\n",
      "\n",
      "Nothing to add.\n"
     ]
    }
   ],
   "source": [
    "!poetry add pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "train = pd.read_parquet(\"train.parquet\")\n",
    "test = pd.read_parquet(\"test.parquet\")\n",
    "validate = pd.read_parquet(\"validate.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82326, 6) (9650, 6) (10047, 6)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape, validate.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = pd.concat([train, test, validate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102023\n"
     ]
    }
   ],
   "source": [
    "total_size = train.shape[0] + test.shape[0] + validate.shape[0]\n",
    "print(total_size)\n",
    "# Rerun this code only if you want to generate new indices.\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# np.random.seed(seed=999999)\n",
    "# indices = np.random.randint(0, total_size, total_size)\n",
    "# train_indices = indices[0:train.shape[0]]\n",
    "# test_indices = indices[train.shape[0]:train.shape[0]+test.shape[0]]\n",
    "# validate_indices = indices[train.shape[0]+test.shape[0]:]\n",
    "# all_indices = {'train': train_indices.tolist(), 'test': test_indices.tolist(), 'validate': validate_indices.tolist()}\n",
    "# print(f\"Length of all indices: {sum(len(v) for k, v in all_indices.items())}\")\n",
    "# # write these to a pickle file\n",
    "\n",
    "# with open('indices.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_indices, f)\n",
    "\n",
    "# create train, test, validation datasets based on query_ids\n",
    "# query_ids = full_dataset['query_id']\n",
    "# train_query_ids = query_ids[0:train.shape[0]]\n",
    "# test_query_ids = query_ids[train.shape[0]:train.shape[0]+test.shape[0]]\n",
    "# validate_query_ids = query_ids[train.shape[0]+test.shape[0]:]\n",
    "# print(train_query_ids.shape, test_query_ids.shape, validate_query_ids.shape)\n",
    "# all_query_ids  = {'train': train_query_ids.tolist(), 'test': test_query_ids.tolist(), 'validate': validate_query_ids.tolist()}\n",
    "# # write these to a pickle file\n",
    "\n",
    "# with open('query_ids.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_query_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82326 training indices [36233, 13668, 19591, 66699, 34957, 34882, 23273, 27926, 90079, 68436]\n",
      "9650 test indices [27928, 5317, 97860, 83225, 90861, 42316, 42625, 98527, 74133, 95026]\n",
      "10047 validate indices [94011, 72401, 60227, 70263, 35765, 47026, 31472, 3385, 21289, 545]\n",
      "82326 training query ids [19699, 19700, 19701, 19702, 19703, 19704, 19705, 19706, 19707, 19708]\n",
      "9650 test query ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "10047 validate query ids [9652, 9653, 9654, 9655, 9656, 9657, 9658, 9659, 9660, 9661]\n"
     ]
    }
   ],
   "source": [
    "# Unpickle and test\n",
    "with open(\"indices.pkl\", \"rb\") as f:\n",
    "    all_indices = pickle.load(f)\n",
    "\n",
    "    print(f\"{len(all_indices['train'])} training indices {all_indices['train'][0:10]}\")\n",
    "    print(f\"{len(all_indices['test'])} test indices {all_indices['test'][0:10]}\")\n",
    "    print(\n",
    "        f\"{len(all_indices['validate'])} validate indices {all_indices['validate'][0:10]}\"\n",
    "    )\n",
    "\n",
    "with open(\"query_ids.pkl\", \"rb\") as f:\n",
    "    all_query_ids = pickle.load(f)\n",
    "    print(\n",
    "        f\"{len(all_query_ids['train'])} training query ids {all_query_ids['train'][0:10]}\"\n",
    "    )\n",
    "    print(f\"{len(all_query_ids['test'])} test query ids {all_query_ids['test'][0:10]}\")\n",
    "    print(\n",
    "        f\"{len(all_query_ids['validate'])} validate query ids {all_query_ids['validate'][0:10]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is there a limit to roth ira contributions'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train[['passages', 'query']].iloc[0]\n",
    "train_dataset = full_dataset.iloc[all_indices[\"train\"]]\n",
    "test_dataset = full_dataset.iloc[all_indices[\"test\"]]\n",
    "validate_dataset = full_dataset.iloc[all_indices[\"validate\"]]\n",
    "\n",
    "train_dataset.head()[\"query\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "\n",
    "def pprint(text):\n",
    "    print(f\"\\r{text}\", end=\"\")\n",
    "\n",
    "\n",
    "def create_lookups(full_dataset):\n",
    "    print(f\"dataset shape: {full_dataset.shape}\")\n",
    "    all_urls = full_dataset[\"passages\"].apply(lambda x: x[\"url\"]).tolist()\n",
    "    unique_urls = set([item for sublist in all_urls for item in sublist])\n",
    "    print(f\"Total number of urls: {sum(len(i) for i in all_urls)}\")\n",
    "    print(f\"Total number of unique urls: {len(unique_urls)}\")\n",
    "\n",
    "    # Use an md5 hash for the urls for deterministic mapping\n",
    "    def generate_md5_hash(s):\n",
    "        return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    ids_to_urls = {generate_md5_hash(url): url for url in unique_urls}\n",
    "\n",
    "    urls_to_ids = {url: i for i, url in ids_to_urls.items()}\n",
    "    print(f\"Total number of hashed urls: {len(ids_to_urls)}\")\n",
    "\n",
    "    query_ids = full_dataset[\"query_id\"].tolist()\n",
    "    assert len(query_ids) == len(set(query_ids))\n",
    "    print(f\"Total number of queries: {len(query_ids)}\")\n",
    "    return ids_to_urls, urls_to_ids\n",
    "\n",
    "def add_hashed_urls(dataset, urls_to_ids):\n",
    "    dataset.loc[:, \"hashed_urls\"] = dataset[\"passages\"].progress_apply(\n",
    "        lambda x: np.array(list(set([urls_to_ids[url] for url in x[\"url\"]])))\n",
    "    )\n",
    "def get_triples(dataset):\n",
    "    return dataset[[\"query_id\", \"hashed_urls\", \"irrelevant_urls\"]].values.tolist()\n",
    "    # for i, row in (\n",
    "    #     enumerate(dataset.iterrows()\n",
    "    #     if not create_small\n",
    "    #     else dataset.head(10).iterrows())\n",
    "    # ):\n",
    "    #     relevant_url_ids = row[1][\"hashed_urls\"]\n",
    "    #     query_id = row[1][\"query_id\"]\n",
    "    #     # relevant_url_ids = np.array(list(set([urls_to_ids[url] for url in urls])))\n",
    "    #     irrelevant_url_ids = np.setdiff1d(\n",
    "    #         master_url_id_set, relevant_url_ids, assume_unique=True\n",
    "    #     )\n",
    "    #     sampled_ids = np.random.choice(\n",
    "    #         irrelevant_url_ids, size=len(relevant_url_ids), replace=False\n",
    "    #     )\n",
    "\n",
    "    #     triple = (query_id, list(relevant_url_ids), sampled_ids.tolist())\n",
    "    #     triples.append(triple)\n",
    "    #     if (i+1) % 1000 == 0:\n",
    "    #         pprint(f\"Processed {i} rows\")\n",
    "    # print(f\"Query ID, Query: {query_id}, {row[1]['query']}\")\n",
    "    # print(f\"R {NL.join([ids_to_urls[i]+NL+TAB+str(i) for i in relevant_url_ids])}\")\n",
    "    # print(f\"IR: {NL.join([ids_to_urls[i]+NL+TAB+str(i) for i in irrelevant_url_ids])}\")\n",
    "\n",
    "\n",
    "# test_triples = create_triples(test_dataset, ids_to_urls, urls_to_ids, create_small=False)\n",
    "# validate_triples = create_triples(validate_dataset, ids_to_urls, urls_to_ids, create_small=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape: (102023, 6)\n",
      "Total number of urls: 837729\n",
      "Total number of unique urls: 456487\n",
      "Total number of hashed urls: 456487\n",
      "Total number of queries: 102023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82326/82326 [00:00<00:00, 169606.21it/s]\n",
      "/var/folders/fk/dlmj8gqd4ms2jgbkv_33skth0000gn/T/ipykernel_77110/2147649127.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset.loc[:, \"hashed_urls\"] = dataset[\"passages\"].progress_apply(\n",
      "100%|██████████| 9650/9650 [00:00<00:00, 165736.61it/s]\n",
      "/var/folders/fk/dlmj8gqd4ms2jgbkv_33skth0000gn/T/ipykernel_77110/2147649127.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset.loc[:, \"hashed_urls\"] = dataset[\"passages\"].progress_apply(\n",
      "100%|██████████| 10047/10047 [00:00<00:00, 164394.28it/s]\n",
      "/var/folders/fk/dlmj8gqd4ms2jgbkv_33skth0000gn/T/ipykernel_77110/2147649127.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset.loc[:, \"hashed_urls\"] = dataset[\"passages\"].progress_apply(\n"
     ]
    }
   ],
   "source": [
    "ids_to_urls, urls_to_ids = create_lookups(full_dataset)\n",
    "add_hashed_urls(train_dataset, urls_to_ids)\n",
    "add_hashed_urls(test_dataset, urls_to_ids)\n",
    "add_hashed_urls(validate_dataset, urls_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_url_id_set = np.array(\n",
    "    list(set(ids_to_urls.keys()))\n",
    ")  # Define or load your master_url_id_set here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to be applied to each row, modified to accept master_url_id_set\n",
    "# def process_row(x, master_url_id_set, is_deterministic=True):\n",
    "#     if is_deterministic:\n",
    "#         np.random.seed(seed=999999)\n",
    "#     return np.random.choice(\n",
    "#         np.setdiff1d(list(master_url_id_set), x, assume_unique=True),\n",
    "#         size=len(x),\n",
    "#         replace=False,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset for demonstration; in practice, adjust based on your dataset size\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "import utils \n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "importlib.reload(utils.data_utils)\n",
    "from utils.data_utils import add_negative_samples\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sid/workspace/mlx-week3/.venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Processing Batches: 100%|██████████| 10/10 [12:47<00:00, 76.77s/it]  \n"
     ]
    }
   ],
   "source": [
    "training_dataset_copy = train_dataset.copy()\n",
    "add_negative_samples(training_dataset_copy, ids_to_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82326\n",
      "82326\n",
      "[7, 9, 5, 5, 8, 5, 7, 6, 6, 7]\n",
      "[7, 9, 5, 5, 8, 5, 7, 6, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "s1 = list([len(i[1][\"negative_sample_urls\"]) for i in training_dataset_copy.iterrows()])\n",
    "s2 = list([len(i[1][\"hashed_urls\"]) for i in training_dataset_copy.iterrows()])\n",
    "print(len(s1))\n",
    "print(len(s2))\n",
    "print(s1[0:10])\n",
    "print(s2[0:10])\n",
    "assert(s1 == s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_copy['foo'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashed_urls</th>\n",
       "      <th>foo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[51456f6eae52e682e8cc2877123547df, bca175d72f3...</td>\n",
       "      <td>[571f1a386a9a574c3750f1a7a2f6364f, 9673999d279...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[04ca78de96350a09ee9fd245e78a7e37, f838014c83f...</td>\n",
       "      <td>[9e8e3d021a183f718446ddff6830d503, 3ace24808e8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[6704ac0b665bdaf085eb48219f0b1020, 822e547be7e...</td>\n",
       "      <td>[702a0a6341656ff131532a4525d606de, 02cd00e55df...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[cb557a209e44ff35a51e9fbacee5a83f, eae01750da5...</td>\n",
       "      <td>[2bff03edbe5ab67b9a2e761c0512b23e, 3cfc773f943...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[5300a014d854940c2eb9c02ea2f815d8, da210bbbc49...</td>\n",
       "      <td>[3e8f7cde406e84b8643c69dd02d79568, 2d7877a2c02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[211f58d4d9db05be831fcfb33a012d16, 22078d289ad...</td>\n",
       "      <td>[0e1f4fc4e4d1d670ec537ac9ed0c5827, 9618a0be8f6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[057d0e7fb52d630dc947cdef442e056f, 0d91b002501...</td>\n",
       "      <td>[859266e60a8734f72acc5ce43147f884, 6d4c665bfd8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[c4d92775f3e99a779547e2b442763411, 4715748b687...</td>\n",
       "      <td>[62ec92685fdab9966388887a36cfdc40, 374315d07d0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[41005582f659e1512d0cca98dd5cbeec, c986b37f1bc...</td>\n",
       "      <td>[e31744eb82ceff23984dcac73f1a6534, efb495dde4e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[83d312b824fe56cbc6370cf916c84dd8, e0b702e2c5a...</td>\n",
       "      <td>[7f662db47a788af0cc9243529789397b, 23033bac8e8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          hashed_urls  \\\n",
       "0   [51456f6eae52e682e8cc2877123547df, bca175d72f3...   \n",
       "1   [04ca78de96350a09ee9fd245e78a7e37, f838014c83f...   \n",
       "2   [6704ac0b665bdaf085eb48219f0b1020, 822e547be7e...   \n",
       "3   [cb557a209e44ff35a51e9fbacee5a83f, eae01750da5...   \n",
       "4   [5300a014d854940c2eb9c02ea2f815d8, da210bbbc49...   \n",
       "..                                                ...   \n",
       "95  [211f58d4d9db05be831fcfb33a012d16, 22078d289ad...   \n",
       "96  [057d0e7fb52d630dc947cdef442e056f, 0d91b002501...   \n",
       "97  [c4d92775f3e99a779547e2b442763411, 4715748b687...   \n",
       "98  [41005582f659e1512d0cca98dd5cbeec, c986b37f1bc...   \n",
       "99  [83d312b824fe56cbc6370cf916c84dd8, e0b702e2c5a...   \n",
       "\n",
       "                                                  foo  \n",
       "0   [571f1a386a9a574c3750f1a7a2f6364f, 9673999d279...  \n",
       "1   [9e8e3d021a183f718446ddff6830d503, 3ace24808e8...  \n",
       "2   [702a0a6341656ff131532a4525d606de, 02cd00e55df...  \n",
       "3   [2bff03edbe5ab67b9a2e761c0512b23e, 3cfc773f943...  \n",
       "4   [3e8f7cde406e84b8643c69dd02d79568, 2d7877a2c02...  \n",
       "..                                                ...  \n",
       "95  [0e1f4fc4e4d1d670ec537ac9ed0c5827, 9618a0be8f6...  \n",
       "96  [859266e60a8734f72acc5ce43147f884, 6d4c665bfd8...  \n",
       "97  [62ec92685fdab9966388887a36cfdc40, 374315d07d0...  \n",
       "98  [e31744eb82ceff23984dcac73f1a6534, efb495dde4e...  \n",
       "99  [7f662db47a788af0cc9243529789397b, 23033bac8e8...  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_small_copy[['hashed_urls', 'foo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36233    NaN\n",
       "13668    NaN\n",
       "19591    NaN\n",
       "66699    NaN\n",
       "34957    NaN\n",
       "        ... \n",
       "44604    NaN\n",
       "46834    NaN\n",
       "30625    NaN\n",
       "52077    NaN\n",
       "17170    NaN\n",
       "Name: foo, Length: 100, dtype: object"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "9\n",
      "5\n",
      "5\n",
      "8\n",
      "5\n",
      "7\n",
      "6\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for i in train_small_copy.iterrows():\n",
    "    print(len(i[1][\"hashed_urls\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36233    [61e5e0c43ef0319a7260b3e3220146b5, fc8720e57bb...\n",
       "13668    [f4889a24a909185fbb72258490958cdb, 861e276448b...\n",
       "19591    [822e547be7e7b79f352e207eaffa86d7, e96b7521621...\n",
       "66699    [1cffcc6fc8d2ddbf9bfeb3d6603012e1, c5265538d18...\n",
       "34957    [77192cd6d774fe94b3bcce6c1f262aa3, 45a60f9c21a...\n",
       "                               ...                        \n",
       "17456    [aa2a48d8adca51b69746485f4b6c6b6d, 80d1f4443dd...\n",
       "41326    [822a654d71d10488a2d1425521138cc9, 32546314895...\n",
       "16526    [9f5c90b9a8e31ff4b38e09ce99203da9, b452e7d117e...\n",
       "59902    [5b89caaac9b474181104e77a4cb28bd0, 3ab7f7ab359...\n",
       "43475    [44492cc90599eccde4246b519fc1853f, 12e21816924...\n",
       "Name: passages, Length: 82326, dtype: object"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"passages\"].apply(\n",
    "    lambda x: np.array(list(set([urls_to_ids[url] for url in x[\"url\"]])))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplet_dataset(which: str):\n",
    "    \"\"\"\n",
    "    Structure:\n",
    "    query, positive, negative\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"spm_AllTexts.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
