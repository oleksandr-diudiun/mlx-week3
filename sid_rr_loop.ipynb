{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop:\n",
    "# 1. Create torch dataset from the data containing the query, positive, negative example.\n",
    "# 2. Create a dataloader from the dataset.\n",
    "# 3. Run a single batch through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following packages are already present in the pyproject.toml and will be skipped:\n",
      "\n",
      "  • \u001b[36mpandas\u001b[39m\n",
      "\n",
      "If you want to update it to the latest compatible version, you can use `poetry update package`.\n",
      "If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.\n",
      "\n",
      "Nothing to add.\n"
     ]
    }
   ],
   "source": [
    "!poetry add pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "train = pd.read_parquet(\"train.parquet\")\n",
    "test = pd.read_parquet(\"test.parquet\")\n",
    "validate = pd.read_parquet(\"validate.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82326, 6) (9650, 6) (10047, 6)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape, validate.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = pd.concat([train, test, validate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102023\n"
     ]
    }
   ],
   "source": [
    "total_size = train.shape[0] + test.shape[0] + validate.shape[0]\n",
    "print(total_size)\n",
    "# Rerun this code only if you want to generate new indices.\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# np.random.seed(seed=999999)\n",
    "# indices = np.random.randint(0, total_size, total_size)\n",
    "# train_indices = indices[0:train.shape[0]]\n",
    "# test_indices = indices[train.shape[0]:train.shape[0]+test.shape[0]]\n",
    "# validate_indices = indices[train.shape[0]+test.shape[0]:]\n",
    "# all_indices = {'train': train_indices.tolist(), 'test': test_indices.tolist(), 'validate': validate_indices.tolist()}\n",
    "# print(f\"Length of all indices: {sum(len(v) for k, v in all_indices.items())}\")\n",
    "# # write these to a pickle file\n",
    "\n",
    "# with open('indices.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_indices, f)\n",
    "\n",
    "# create train, test, validation datasets based on query_ids\n",
    "# query_ids = full_dataset['query_id']\n",
    "# train_query_ids = query_ids[0:train.shape[0]]\n",
    "# test_query_ids = query_ids[train.shape[0]:train.shape[0]+test.shape[0]]\n",
    "# validate_query_ids = query_ids[train.shape[0]+test.shape[0]:]\n",
    "# print(train_query_ids.shape, test_query_ids.shape, validate_query_ids.shape)\n",
    "# all_query_ids  = {'train': train_query_ids.tolist(), 'test': test_query_ids.tolist(), 'validate': validate_query_ids.tolist()}\n",
    "# # write these to a pickle file\n",
    "\n",
    "# with open('query_ids.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_query_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82326 training indices [36233, 13668, 19591, 66699, 34957, 34882, 23273, 27926, 90079, 68436]\n",
      "9650 test indices [27928, 5317, 97860, 83225, 90861, 42316, 42625, 98527, 74133, 95026]\n",
      "10047 validate indices [94011, 72401, 60227, 70263, 35765, 47026, 31472, 3385, 21289, 545]\n",
      "82326 training query ids [19699, 19700, 19701, 19702, 19703, 19704, 19705, 19706, 19707, 19708]\n",
      "9650 test query ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "10047 validate query ids [9652, 9653, 9654, 9655, 9656, 9657, 9658, 9659, 9660, 9661]\n"
     ]
    }
   ],
   "source": [
    "# Unpickle and test\n",
    "with open(\"indices.pkl\", \"rb\") as f:\n",
    "    all_indices = pickle.load(f)\n",
    "\n",
    "    print(f\"{len(all_indices['train'])} training indices {all_indices['train'][0:10]}\")\n",
    "    print(f\"{len(all_indices['test'])} test indices {all_indices['test'][0:10]}\")\n",
    "    print(\n",
    "        f\"{len(all_indices['validate'])} validate indices {all_indices['validate'][0:10]}\"\n",
    "    )\n",
    "\n",
    "with open(\"query_ids.pkl\", \"rb\") as f:\n",
    "    all_query_ids = pickle.load(f)\n",
    "    print(\n",
    "        f\"{len(all_query_ids['train'])} training query ids {all_query_ids['train'][0:10]}\"\n",
    "    )\n",
    "    print(f\"{len(all_query_ids['test'])} test query ids {all_query_ids['test'][0:10]}\")\n",
    "    print(\n",
    "        f\"{len(all_query_ids['validate'])} validate query ids {all_query_ids['validate'][0:10]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is there a limit to roth ira contributions'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train[['passages', 'query']].iloc[0]\n",
    "train_dataset = full_dataset.iloc[all_indices[\"train\"]]\n",
    "test_dataset = full_dataset.iloc[all_indices[\"test\"]]\n",
    "validate_dataset = full_dataset.iloc[all_indices[\"validate\"]]\n",
    "\n",
    "train_dataset.head()[\"query\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "\n",
    "def pprint(text):\n",
    "    print(f\"\\r{text}\", end=\"\")\n",
    "\n",
    "\n",
    "def create_lookups(full_dataset):\n",
    "    print(f\"dataset shape: {full_dataset.shape}\")\n",
    "    all_urls = full_dataset[\"passages\"].apply(lambda x: x[\"url\"]).tolist()\n",
    "    unique_urls = set([item for sublist in all_urls for item in sublist])\n",
    "    print(f\"Total number of urls: {sum(len(i) for i in all_urls)}\")\n",
    "    print(f\"Total number of unique urls: {len(unique_urls)}\")\n",
    "\n",
    "    # Use an md5 hash for the urls for deterministic mapping\n",
    "    def generate_md5_hash(s):\n",
    "        return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    ids_to_urls = {generate_md5_hash(url): url for url in unique_urls}\n",
    "\n",
    "    urls_to_ids = {url: i for i, url in ids_to_urls.items()}\n",
    "    print(f\"Total number of hashed urls: {len(ids_to_urls)}\")\n",
    "\n",
    "    query_ids = full_dataset[\"query_id\"].tolist()\n",
    "    assert len(query_ids) == len(set(query_ids))\n",
    "    print(f\"Total number of queries: {len(query_ids)}\")\n",
    "    return ids_to_urls, urls_to_ids\n",
    "\n",
    "def add_hashed_urls(dataset, urls_to_ids):\n",
    "    dataset.loc[:, \"hashed_urls\"] = dataset[\"passages\"].progress_apply(\n",
    "        lambda x: np.array(list(set([urls_to_ids[url] for url in x[\"url\"]])))\n",
    "    )\n",
    "def get_triples(dataset):\n",
    "    return dataset[[\"query_id\", \"hashed_urls\", \"irrelevant_urls\"]].values.tolist()\n",
    "    # for i, row in (\n",
    "    #     enumerate(dataset.iterrows()\n",
    "    #     if not create_small\n",
    "    #     else dataset.head(10).iterrows())\n",
    "    # ):\n",
    "    #     relevant_url_ids = row[1][\"hashed_urls\"]\n",
    "    #     query_id = row[1][\"query_id\"]\n",
    "    #     # relevant_url_ids = np.array(list(set([urls_to_ids[url] for url in urls])))\n",
    "    #     irrelevant_url_ids = np.setdiff1d(\n",
    "    #         master_url_id_set, relevant_url_ids, assume_unique=True\n",
    "    #     )\n",
    "    #     sampled_ids = np.random.choice(\n",
    "    #         irrelevant_url_ids, size=len(relevant_url_ids), replace=False\n",
    "    #     )\n",
    "\n",
    "    #     triple = (query_id, list(relevant_url_ids), sampled_ids.tolist())\n",
    "    #     triples.append(triple)\n",
    "    #     if (i+1) % 1000 == 0:\n",
    "    #         pprint(f\"Processed {i} rows\")\n",
    "    # print(f\"Query ID, Query: {query_id}, {row[1]['query']}\")\n",
    "    # print(f\"R {NL.join([ids_to_urls[i]+NL+TAB+str(i) for i in relevant_url_ids])}\")\n",
    "    # print(f\"IR: {NL.join([ids_to_urls[i]+NL+TAB+str(i) for i in irrelevant_url_ids])}\")\n",
    "\n",
    "\n",
    "# test_triples = create_triples(test_dataset, ids_to_urls, urls_to_ids, create_small=False)\n",
    "# validate_triples = create_triples(validate_dataset, ids_to_urls, urls_to_ids, create_small=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape: (102023, 6)\n",
      "Total number of urls: 837729\n",
      "Total number of unique urls: 456487\n",
      "Total number of hashed urls: 456487\n",
      "Total number of queries: 102023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82326/82326 [00:00<00:00, 169606.21it/s]\n",
      "/var/folders/fk/dlmj8gqd4ms2jgbkv_33skth0000gn/T/ipykernel_77110/2147649127.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset.loc[:, \"hashed_urls\"] = dataset[\"passages\"].progress_apply(\n",
      "100%|██████████| 9650/9650 [00:00<00:00, 165736.61it/s]\n",
      "/var/folders/fk/dlmj8gqd4ms2jgbkv_33skth0000gn/T/ipykernel_77110/2147649127.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset.loc[:, \"hashed_urls\"] = dataset[\"passages\"].progress_apply(\n",
      "100%|██████████| 10047/10047 [00:00<00:00, 164394.28it/s]\n",
      "/var/folders/fk/dlmj8gqd4ms2jgbkv_33skth0000gn/T/ipykernel_77110/2147649127.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset.loc[:, \"hashed_urls\"] = dataset[\"passages\"].progress_apply(\n"
     ]
    }
   ],
   "source": [
    "ids_to_urls, urls_to_ids = create_lookups(full_dataset)\n",
    "add_hashed_urls(train_dataset, urls_to_ids)\n",
    "add_hashed_urls(test_dataset, urls_to_ids)\n",
    "add_hashed_urls(validate_dataset, urls_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_url_id_set = np.array(\n",
    "    list(set(ids_to_urls.keys()))\n",
    ")  # Define or load your master_url_id_set here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset for demonstration; in practice, adjust based on your dataset size\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "import utils \n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "importlib.reload(utils.data_utils)\n",
    "from utils.data_utils import add_negative_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sid/workspace/mlx-week3/.venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Processing Batches: 100%|██████████| 10/10 [12:47<00:00, 76.77s/it]  \n"
     ]
    }
   ],
   "source": [
    "training_dataset_copy = train_dataset.copy()\n",
    "add_negative_samples(training_dataset_copy, ids_to_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sid/workspace/mlx-week3/.venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Processing Batches: 100%|██████████| 10/10 [01:29<00:00,  8.98s/it]\n"
     ]
    }
   ],
   "source": [
    "test_dataset_copy = test_dataset.copy()\n",
    "add_negative_samples(test_dataset_copy, ids_to_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sid/workspace/mlx-week3/.venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Processing Batches: 100%|██████████| 10/10 [01:38<00:00,  9.87s/it]\n"
     ]
    }
   ],
   "source": [
    "validate_dataset_copy = validate_dataset.copy()\n",
    "add_negative_samples(validate_dataset_copy, ids_to_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('train_triplets.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_columns = [\"query_id\", \"hashed_urls\", \"negative_sample_urls\"]\n",
    "training_dataset_copy[triple_columns].to_parquet('training_dataset_triplets.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_copy[triple_columns].to_parquet('test_dataset_triplets.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_dataset_copy[triple_columns].to_parquet('validate_dataset_triplets.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82326\n",
      "82326\n",
      "[7, 9, 5, 5, 8, 5, 7, 6, 6, 7]\n",
      "[7, 9, 5, 5, 8, 5, 7, 6, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# There is a unit test to test this in the tests/ dir.\n",
    "s1 = list([len(i[1][\"negative_sample_urls\"]) for i in training_dataset_copy.iterrows()])\n",
    "s2 = list([len(i[1][\"hashed_urls\"]) for i in training_dataset_copy.iterrows()])\n",
    "print(len(s1))\n",
    "print(len(s2))\n",
    "print(s1[0:10])\n",
    "print(s2[0:10])\n",
    "assert(s1 == s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplet_dataset(which: str):\n",
    "    \"\"\"\n",
    "    Structure:\n",
    "    query, positive, negative\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"spm_AllTexts.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
