{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop:\n",
    "# 1. Create torch dataset from the data containing the query, positive, negative example.\n",
    "# 2. Create a dataloader from the dataset.\n",
    "# 3. Run a single batch through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following packages are already present in the pyproject.toml and will be skipped:\n",
      "\n",
      "  • \u001b[36mpandas\u001b[39m\n",
      "\n",
      "If you want to update it to the latest compatible version, you can use `poetry update package`.\n",
      "If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.\n",
      "\n",
      "Nothing to add.\n"
     ]
    }
   ],
   "source": [
    "!poetry add pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "train = pd.read_parquet(\"train.parquet\")\n",
    "test = pd.read_parquet(\"test.parquet\")\n",
    "validate = pd.read_parquet(\"validate.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82326, 6) (9650, 6) (10047, 6)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape, validate.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = pd.concat([train, test, validate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answers</th>\n",
       "      <th>passages</th>\n",
       "      <th>query</th>\n",
       "      <th>query_id</th>\n",
       "      <th>query_type</th>\n",
       "      <th>wellFormedAnswers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Results-Based Accountability is a disciplined...</td>\n",
       "      <td>{'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]...</td>\n",
       "      <td>what is rba</td>\n",
       "      <td>19699</td>\n",
       "      <td>description</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[20-25 minutes]</td>\n",
       "      <td>{'is_selected': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]...</td>\n",
       "      <td>how long do you need for sydney and surroundin...</td>\n",
       "      <td>19701</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             answers  \\\n",
       "0  [Results-Based Accountability is a disciplined...   \n",
       "2                                    [20-25 minutes]   \n",
       "\n",
       "                                            passages  \\\n",
       "0  {'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]...   \n",
       "2  {'is_selected': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]...   \n",
       "\n",
       "                                               query  query_id   query_type  \\\n",
       "0                                        what is rba     19699  description   \n",
       "2  how long do you need for sydney and surroundin...     19701      numeric   \n",
       "\n",
       "  wellFormedAnswers  \n",
       "0                []  \n",
       "2                []  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset[full_dataset[\"query_id\"].isin([19699, 19701])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Query IDs for each dataset to use on later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102023\n"
     ]
    }
   ],
   "source": [
    "total_size = train.shape[0] + test.shape[0] + validate.shape[0]\n",
    "print(total_size)\n",
    "# Rerun this code only if you want to generate new indices.\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# np.random.seed(seed=999999)\n",
    "# indices = np.random.randint(0, total_size, total_size)\n",
    "# train_indices = indices[0:train.shape[0]]\n",
    "# test_indices = indices[train.shape[0]:train.shape[0]+test.shape[0]]\n",
    "# validate_indices = indices[train.shape[0]+test.shape[0]:]\n",
    "# all_indices = {'train': train_indices.tolist(), 'test': test_indices.tolist(), 'validate': validate_indices.tolist()}\n",
    "# print(f\"Length of all indices: {sum(len(v) for k, v in all_indices.items())}\")\n",
    "# # write these to a pickle file\n",
    "\n",
    "# with open('indices.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_indices, f)\n",
    "\n",
    "# create train, test, validation datasets based on query_ids\n",
    "# query_ids = full_dataset['query_id']\n",
    "# train_query_ids = query_ids[0:train.shape[0]]\n",
    "# test_query_ids = query_ids[train.shape[0]:train.shape[0]+test.shape[0]]\n",
    "# validate_query_ids = query_ids[train.shape[0]+test.shape[0]:]\n",
    "# print(train_query_ids.shape, test_query_ids.shape, validate_query_ids.shape)\n",
    "# all_query_ids  = {'train': train_query_ids.tolist(), 'test': test_query_ids.tolist(), 'validate': validate_query_ids.tolist()}\n",
    "# # write these to a pickle file\n",
    "\n",
    "# with open('query_ids.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_query_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82326 training indices [36233, 13668, 19591, 66699, 34957, 34882, 23273, 27926, 90079, 68436]\n",
      "9650 test indices [27928, 5317, 97860, 83225, 90861, 42316, 42625, 98527, 74133, 95026]\n",
      "10047 validate indices [94011, 72401, 60227, 70263, 35765, 47026, 31472, 3385, 21289, 545]\n",
      "82326 training query ids [19699, 19700, 19701, 19702, 19703, 19704, 19705, 19706, 19707, 19708]\n",
      "9650 test query ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "10047 validate query ids [9652, 9653, 9654, 9655, 9656, 9657, 9658, 9659, 9660, 9661]\n"
     ]
    }
   ],
   "source": [
    "# Unpickle and test\n",
    "with open(\"indices.pkl\", \"rb\") as f:\n",
    "    all_indices = pickle.load(f)\n",
    "\n",
    "    print(f\"{len(all_indices['train'])} training indices {all_indices['train'][0:10]}\")\n",
    "    print(f\"{len(all_indices['test'])} test indices {all_indices['test'][0:10]}\")\n",
    "    print(\n",
    "        f\"{len(all_indices['validate'])} validate indices {all_indices['validate'][0:10]}\"\n",
    "    )\n",
    "\n",
    "with open(\"query_ids.pkl\", \"rb\") as f:\n",
    "    all_query_ids = pickle.load(f)\n",
    "    print(\n",
    "        f\"{len(all_query_ids['train'])} training query ids {all_query_ids['train'][0:10]}\"\n",
    "    )\n",
    "    print(f\"{len(all_query_ids['test'])} test query ids {all_query_ids['test'][0:10]}\")\n",
    "    print(\n",
    "        f\"{len(all_query_ids['validate'])} validate query ids {all_query_ids['validate'][0:10]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is rba'"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train[['passages', 'query']].iloc[0]\n",
    "train_dataset = full_dataset[full_dataset.query_id.isin(all_query_ids[\"train\"])]\n",
    "test_dataset = full_dataset[full_dataset.query_id.isin(all_query_ids[\"test\"])]\n",
    "validate_dataset = full_dataset[full_dataset.query_id.isin(all_query_ids[\"validate\"])]\n",
    "\n",
    "train_dataset.head()[\"query\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(f\"{np.all(train.query_id.value_counts() == 1)}\")\n",
    "print(f\"{np.all(train_dataset.query_id.value_counts() == 1)}\")\n",
    "assert np.all(train.query_id.value_counts() == 1)\n",
    "assert np.all(train_dataset.query_id.value_counts() == 1)\n",
    "assert np.all(validate.query_id.value_counts() == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answers</th>\n",
       "      <th>passages</th>\n",
       "      <th>query</th>\n",
       "      <th>query_id</th>\n",
       "      <th>query_type</th>\n",
       "      <th>wellFormedAnswers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Results-Based Accountability is a disciplined...</td>\n",
       "      <td>{'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]...</td>\n",
       "      <td>what is rba</td>\n",
       "      <td>19699</td>\n",
       "      <td>description</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Yes]</td>\n",
       "      <td>{'is_selected': [0, 1, 0, 0, 0, 0, 0], 'passag...</td>\n",
       "      <td>was ronald reagan a democrat</td>\n",
       "      <td>19700</td>\n",
       "      <td>description</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[20-25 minutes]</td>\n",
       "      <td>{'is_selected': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]...</td>\n",
       "      <td>how long do you need for sydney and surroundin...</td>\n",
       "      <td>19701</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[$11 to $22 per square foot]</td>\n",
       "      <td>{'is_selected': [0, 0, 0, 0, 0, 0, 0, 0, 1], '...</td>\n",
       "      <td>price to install tile in shower</td>\n",
       "      <td>19702</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Due to symptoms in the body]</td>\n",
       "      <td>{'is_selected': [0, 0, 1, 0, 0, 0, 0, 0], 'pas...</td>\n",
       "      <td>why conversion observed in body</td>\n",
       "      <td>19703</td>\n",
       "      <td>description</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             answers  \\\n",
       "0  [Results-Based Accountability is a disciplined...   \n",
       "1                                              [Yes]   \n",
       "2                                    [20-25 minutes]   \n",
       "3                       [$11 to $22 per square foot]   \n",
       "4                      [Due to symptoms in the body]   \n",
       "\n",
       "                                            passages  \\\n",
       "0  {'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]...   \n",
       "1  {'is_selected': [0, 1, 0, 0, 0, 0, 0], 'passag...   \n",
       "2  {'is_selected': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]...   \n",
       "3  {'is_selected': [0, 0, 0, 0, 0, 0, 0, 0, 1], '...   \n",
       "4  {'is_selected': [0, 0, 1, 0, 0, 0, 0, 0], 'pas...   \n",
       "\n",
       "                                               query  query_id   query_type  \\\n",
       "0                                        what is rba     19699  description   \n",
       "1                       was ronald reagan a democrat     19700  description   \n",
       "2  how long do you need for sydney and surroundin...     19701      numeric   \n",
       "3                    price to install tile in shower     19702      numeric   \n",
       "4                    why conversion observed in body     19703  description   \n",
       "\n",
       "  wellFormedAnswers  \n",
       "0                []  \n",
       "1                []  \n",
       "2                []  \n",
       "3                []  \n",
       "4                []  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import create_lookups, add_hashed_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape: (102023, 6)\n",
      "Total number of urls: 837729\n",
      "Total number of unique urls: 456487\n",
      "Total number of hashed urls: 456487\n",
      "Total number of queries: 102023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82326/82326 [00:00<00:00, 197706.24it/s]\n",
      "/var/folders/fk/dlmj8gqd4ms2jgbkv_33skth0000gn/T/ipykernel_77110/1830918971.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset.loc[:, \"hashed_urls\"] = dataset[\"passages\"].progress_apply(\n",
      "100%|██████████| 9650/9650 [00:00<00:00, 183872.95it/s]\n",
      "/var/folders/fk/dlmj8gqd4ms2jgbkv_33skth0000gn/T/ipykernel_77110/1830918971.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset.loc[:, \"hashed_urls\"] = dataset[\"passages\"].progress_apply(\n",
      "100%|██████████| 10047/10047 [00:00<00:00, 193494.37it/s]\n",
      "/var/folders/fk/dlmj8gqd4ms2jgbkv_33skth0000gn/T/ipykernel_77110/1830918971.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset.loc[:, \"hashed_urls\"] = dataset[\"passages\"].progress_apply(\n"
     ]
    }
   ],
   "source": [
    "ids_to_urls, urls_to_ids, url_to_doctext_mapping = create_lookups(full_dataset)\n",
    "add_hashed_urls(train_dataset, urls_to_ids)\n",
    "add_hashed_urls(test_dataset, urls_to_ids)\n",
    "add_hashed_urls(validate_dataset, urls_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"url_to_doctext_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(url_to_doctext_mapping, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['https://en.wikipedia.org/wiki/Reserve_Bank_of_Australia',\n",
       "       'https://en.wikipedia.org/wiki/Reserve_Bank_of_Australia',\n",
       "       'http://acronyms.thefreedictionary.com/RBA',\n",
       "       'https://www.slimvapepen.com/rebuildable-atomizer-rba/',\n",
       "       'http://rba-africa.com/about/what-is-rba/',\n",
       "       'http://resultsleadership.org/what-is-results-based-accountability-rba/',\n",
       "       'http://rba-africa.com/about/what-is-rba/',\n",
       "       'http://searchsecurity.techtarget.com/definition/risk-based-authentication-RBA',\n",
       "       'https://www.slimvapepen.com/rebuildable-atomizer-rba/',\n",
       "       'http://www.rbaconsulting.com/'], dtype=object)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"passages\"].iloc[0][\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example, you could contribute $2,500 to a Roth IRA with one provider and $3,000 with a second provider. You cannot contribute $5,500 to the first Roth IRA and $5,500 to the second provider. Your total contribution limit is $5,500 for all accounts for the tax year. Individuals may contribute money to a Roth IRA from January 1 of the current tax year to April 15 of the following tax year. Contributions are made with post-tax dollars. You pay tax today so that in retirement no tax is charged to you in retirement.\n",
      "c2f05ed2ec73e1b97799c70e4cdbed8f\n",
      "http://rba-africa.com/about/what-is-rba/\n"
     ]
    }
   ],
   "source": [
    "print(url_to_doctext_mapping[\"51456f6eae52e682e8cc2877123547df\"])\n",
    "print(urls_to_ids[\"http://rba-africa.com/about/what-is-rba/\"])\n",
    "print(ids_to_urls[\"c2f05ed2ec73e1b97799c70e4cdbed8f\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_url_id_set = np.array(\n",
    "    list(set(ids_to_urls.keys()))\n",
    ")  # Define or load your master_url_id_set here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset for demonstration; in practice, adjust based on your dataset size\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "import utils\n",
    "import importlib\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(utils.data_utils)\n",
    "from utils.data_utils import add_negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sid/workspace/mlx-week3/.venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Processing Batches: 100%|██████████| 10/10 [11:02<00:00, 66.24s/it]  \n"
     ]
    }
   ],
   "source": [
    "training_dataset_copy = train_dataset.copy()\n",
    "add_negative_samples(training_dataset_copy, ids_to_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(training_dataset_copy.query_id.value_counts() == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sid/workspace/mlx-week3/.venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Processing Batches: 100%|██████████| 10/10 [01:18<00:00,  7.88s/it]\n"
     ]
    }
   ],
   "source": [
    "test_dataset_copy = test_dataset.copy()\n",
    "add_negative_samples(test_dataset_copy, ids_to_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert np.all(test_dataset_copy.query_id.value_counts() == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sid/workspace/mlx-week3/.venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Processing Batches: 100%|██████████| 10/10 [01:29<00:00,  8.98s/it]\n"
     ]
    }
   ],
   "source": [
    "validate_dataset_copy = validate_dataset.copy()\n",
    "add_negative_samples(validate_dataset_copy, ids_to_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert np.all(validate_dataset_copy.query_id.value_counts() == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>hashed_urls</th>\n",
       "      <th>negative_sample_urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19699</td>\n",
       "      <td>what is rba</td>\n",
       "      <td>[58b2b8024507a2126d53e59a87820ad8, affa5fde381...</td>\n",
       "      <td>[a7509f1dc6a82e1058cac78d284eee49, 9673999d279...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19700</td>\n",
       "      <td>was ronald reagan a democrat</td>\n",
       "      <td>[a2d179699e02966bdf9faa65b992daab, f097b1f42e4...</td>\n",
       "      <td>[06aa259b817892b5290b4b037c376591, 79b11837191...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19701</td>\n",
       "      <td>how long do you need for sydney and surroundin...</td>\n",
       "      <td>[70185a71a8f71d8d1029a71419d5cabf, f8e326ab289...</td>\n",
       "      <td>[6c364273217b774565d18c413d7b0095, 4e96bb1cbd3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19702</td>\n",
       "      <td>price to install tile in shower</td>\n",
       "      <td>[49abbb4ce58adb0395a9ca4e1846239c, 0a1266e71ec...</td>\n",
       "      <td>[75dc8659a62b8bff44831a44f18463aa, 70d696bab58...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19703</td>\n",
       "      <td>why conversion observed in body</td>\n",
       "      <td>[de25b567b657116e9617dabbf554ac44, ea0e0b397cf...</td>\n",
       "      <td>[bf10f48ebac0dd6ece976b659f25d061, 2606c7238d4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id                                              query  \\\n",
       "0     19699                                        what is rba   \n",
       "1     19700                       was ronald reagan a democrat   \n",
       "2     19701  how long do you need for sydney and surroundin...   \n",
       "3     19702                    price to install tile in shower   \n",
       "4     19703                    why conversion observed in body   \n",
       "\n",
       "                                         hashed_urls  \\\n",
       "0  [58b2b8024507a2126d53e59a87820ad8, affa5fde381...   \n",
       "1  [a2d179699e02966bdf9faa65b992daab, f097b1f42e4...   \n",
       "2  [70185a71a8f71d8d1029a71419d5cabf, f8e326ab289...   \n",
       "3  [49abbb4ce58adb0395a9ca4e1846239c, 0a1266e71ec...   \n",
       "4  [de25b567b657116e9617dabbf554ac44, ea0e0b397cf...   \n",
       "\n",
       "                                negative_sample_urls  \n",
       "0  [a7509f1dc6a82e1058cac78d284eee49, 9673999d279...  \n",
       "1  [06aa259b817892b5290b4b037c376591, 79b11837191...  \n",
       "2  [6c364273217b774565d18c413d7b0095, 4e96bb1cbd3...  \n",
       "3  [75dc8659a62b8bff44831a44f18463aa, 70d696bab58...  \n",
       "4  [bf10f48ebac0dd6ece976b659f25d061, 2606c7238d4...  "
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset_copy[triple_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_columns = [\"query_id\", \"query\", \"hashed_urls\", \"negative_sample_urls\"]\n",
    "training_dataset_copy[triple_columns].to_parquet(\n",
    "    \"training_dataset_triplets.parquet\", engine=\"pyarrow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_copy[triple_columns].to_parquet(\n",
    "    \"test_dataset_triplets.parquet\", engine=\"pyarrow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_dataset_copy[triple_columns].to_parquet(\n",
    "    \"validate_dataset_triplets.parquet\", engine=\"pyarrow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82326\n",
      "82326\n",
      "[7, 6, 6, 6, 7, 8, 4, 6, 7, 5]\n",
      "[7, 6, 6, 6, 7, 8, 4, 6, 7, 5]\n"
     ]
    }
   ],
   "source": [
    "# There is a unit test to test this in the tests/ dir.\n",
    "s1 = list([len(i[1][\"negative_sample_urls\"]) for i in training_dataset_copy.iterrows()])\n",
    "s2 = list([len(i[1][\"hashed_urls\"]) for i in training_dataset_copy.iterrows()])\n",
    "print(len(s1))\n",
    "print(len(s2))\n",
    "print(s1[0:10])\n",
    "print(s2[0:10])\n",
    "assert s1 == s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triples -> Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"spm_AllTexts.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    min_count=20,\n",
    "    window=10,\n",
    "    vector_size=EMBEDDING_SIZE,\n",
    "    sample=6e-5,\n",
    "    alpha=0.03,\n",
    "    min_alpha=0.0007,\n",
    "    negative=20,\n",
    "    workers=multiprocessing.cpu_count() - 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the tripleID's to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(utils.data_utils)\n",
    "from utils.data_utils import triples_to_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/82326 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 emebddings with [44, 70, 103, 146, 147, 60, 145]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 44 at dim 1 (got 70)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[500], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_triples \u001b[38;5;241m=\u001b[39m \u001b[43mtriples_to_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_dataset_copy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtriple_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl_to_doctext_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw2v_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEMBEDDING_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/mlx-week3/utils/data_utils.py:188\u001b[0m, in \u001b[0;36mtriples_to_embeddings\u001b[0;34m(triples_df, url_to_doctext_mapping, sentence_piece_model, w2v_model, embedding_size)\u001b[0m\n\u001b[1;32m    180\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m to_embedding(sentence_piece_model, row[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m], w2v_model)\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(embedding_hashed_urls)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m emebddings with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[\u001b[38;5;28mlen\u001b[39m(i)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mi\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39membedding_hashed_urls]\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    184\u001b[0m     triples\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    185\u001b[0m         Triple(\n\u001b[1;32m    186\u001b[0m             row[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    187\u001b[0m             torch\u001b[38;5;241m.\u001b[39mtensor(query_embedding, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m--> 188\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_hashed_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[1;32m    189\u001b[0m             torch\u001b[38;5;241m.\u001b[39mtensor(embedding_negative_sample_urls, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    190\u001b[0m         )\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m triples\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 44 at dim 1 (got 70)"
     ]
    }
   ],
   "source": [
    "training_triples = triples_to_embeddings(\n",
    "    training_dataset_copy[triple_columns],\n",
    "    url_to_doctext_mapping,\n",
    "    sp,\n",
    "    w2v_model,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(training_triples[0][2]))\n",
    "print(len(training_triples[0][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9650/9650 [02:43<00:00, 59.18it/s]\n"
     ]
    }
   ],
   "source": [
    "testing_triples = triples_to_embeddings(\n",
    "    test_dataset_copy[triple_columns],\n",
    "    url_to_doctext_mapping,\n",
    "    sp,\n",
    "    w2v_model,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "triple(query_id=0, query_embedding=tensor([[-3.8118e-01, -1.7516e+00,  1.1853e-01, -4.4650e+00, -5.0346e-01,\n",
       "          1.0803e+00,  8.5209e-01, -1.2138e+00,  1.0042e+00, -3.7721e+00,\n",
       "          6.2760e-01,  1.0943e+00,  3.9942e+00, -5.9516e+00, -1.6203e+00,\n",
       "          1.4496e+00, -3.9064e-01,  1.7156e+00, -1.1008e+00, -1.8131e-01,\n",
       "         -1.4462e+00,  8.8895e-01, -3.1144e+00, -3.4060e+00, -1.8267e+00,\n",
       "          2.0124e-01, -7.8651e-01, -2.7603e+00,  3.0784e+00, -1.1899e+00,\n",
       "         -3.9205e-01, -2.7277e+00, -2.7045e+00,  1.5575e+00, -3.1577e+00,\n",
       "          2.8354e+00,  3.1483e-02, -1.8919e+00, -1.5236e+00,  3.5899e-01,\n",
       "          3.4359e+00, -3.3176e-01,  1.9203e+00, -1.9457e+00,  2.2316e-01,\n",
       "          3.1599e+00,  2.6357e-01, -2.9806e+00, -5.2368e+00, -4.7591e-01,\n",
       "          1.7809e+00, -1.9811e+00, -1.1383e+00, -3.5465e+00, -2.2566e+00,\n",
       "          3.6638e+00, -3.2768e+00,  3.7936e+00,  3.2374e-02,  9.3251e-01,\n",
       "          1.9666e+00, -2.3154e+00, -1.9297e+00, -3.5151e-01, -1.5234e+00,\n",
       "         -2.0748e+00,  3.4039e-01,  4.7203e-01, -2.6269e+00, -1.3426e-01,\n",
       "          3.5210e+00,  2.0937e+00,  2.8762e+00,  1.9404e+00,  2.0809e+00,\n",
       "         -1.5350e+00, -1.2855e+00,  4.5738e+00,  1.1639e+00, -8.8752e-01,\n",
       "         -4.0362e+00,  1.1438e+00,  1.4536e+00,  5.5334e+00,  2.2468e+00,\n",
       "          1.3750e+00, -2.6675e-01, -2.4972e+00, -1.2674e+00, -1.4723e-01,\n",
       "         -1.6828e+00,  3.0166e+00,  8.4955e-01,  1.2390e+00, -5.5306e-02,\n",
       "          2.2797e+00, -2.9345e-02, -2.2701e+00, -5.1144e-01, -1.8668e+00,\n",
       "          1.8471e+00, -2.5600e+00, -7.1480e-01,  4.0385e+00, -1.2752e+00,\n",
       "         -7.6725e-01,  1.1655e+00, -2.2770e+00, -2.4624e+00,  4.3348e-01,\n",
       "          2.1258e+00,  1.0976e+00,  1.7635e+00,  3.6193e+00, -3.0775e+00,\n",
       "         -2.3251e+00, -1.8468e+00, -4.7228e-01, -3.6132e+00,  5.4541e+00,\n",
       "          5.2729e+00,  1.7323e+00,  6.3218e+00,  7.0561e-01, -1.1074e+00,\n",
       "         -1.6847e+00, -7.0890e-01,  7.7383e-01],\n",
       "        [ 4.6941e-01,  2.4938e+00,  5.1746e+00,  2.2682e+00, -1.6623e+00,\n",
       "         -2.3104e+00, -3.0890e+00, -4.5820e-01,  9.0758e-01,  2.6162e+00,\n",
       "          2.7328e+00,  1.2322e+00,  2.2418e-01,  2.2054e+00, -4.2789e+00,\n",
       "          1.3685e-01, -2.2000e+00, -3.3751e+00,  2.1451e+00,  1.6338e-01,\n",
       "          1.9429e+00,  1.8089e+00, -7.9237e-02, -4.5540e-01, -3.2849e+00,\n",
       "          2.1522e+00, -1.7064e-01, -1.6443e+00, -2.0336e+00, -2.0799e+00,\n",
       "         -4.0626e+00, -6.1607e-02,  5.5116e-01, -3.3200e-01, -1.7364e+00,\n",
       "          6.7664e-01,  1.5006e+00,  1.6482e+00, -6.2708e-01,  1.9639e+00,\n",
       "         -4.2388e+00,  3.2835e+00, -2.2103e+00, -4.3263e-02,  6.7318e-01,\n",
       "         -1.4850e-01, -1.8929e+00, -9.6167e-01, -1.4808e-01,  1.6970e+00,\n",
       "         -1.1347e+00,  1.0617e+00, -1.2247e+00, -2.5609e-01,  3.0478e+00,\n",
       "         -1.3774e-01,  2.4651e+00,  2.1204e+00,  2.1882e+00, -6.6857e-01,\n",
       "          2.8231e+00, -1.8136e-01,  6.9220e-02,  8.8469e-01,  1.3997e+00,\n",
       "         -1.9973e+00,  1.2231e+00, -2.3085e+00,  2.0180e-03, -2.0719e+00,\n",
       "         -4.2398e-01,  7.4586e+00,  1.3967e+00,  3.9163e+00, -4.4157e-01,\n",
       "         -4.0603e-01,  3.7925e+00,  3.7244e+00,  5.2406e-01, -2.0183e+00,\n",
       "         -1.4407e+00, -1.9249e+00,  4.4199e+00,  3.6904e+00, -9.7869e-01,\n",
       "          1.9736e+00,  1.9742e+00,  1.0801e+00,  3.9479e+00,  2.6338e+00,\n",
       "         -2.6153e+00, -3.3302e+00,  3.8895e+00,  1.4669e+00,  4.3034e+00,\n",
       "          2.3859e+00,  2.2517e+00, -2.0884e+00, -6.8380e-01, -1.8742e+00,\n",
       "          2.8061e+00,  4.5075e+00, -1.5873e+00,  2.4872e+00, -2.9814e-01,\n",
       "          1.8588e+00, -1.0972e+00,  1.3074e+00,  4.5243e-01,  3.2952e-01,\n",
       "          1.4676e-01,  2.4682e+00,  1.4281e+00, -1.9844e+00, -1.2969e-01,\n",
       "          1.8904e+00, -1.3944e+00, -2.7344e+00,  3.0515e-01, -3.2985e+00,\n",
       "          1.7182e+00, -2.1616e+00,  2.7419e+00, -2.3500e+00, -2.5053e+00,\n",
       "          2.4316e+00,  3.1754e+00,  1.2263e+00],\n",
       "        [-4.3903e+00,  2.7219e+00,  1.0316e+00, -4.8088e+00,  6.8657e-01,\n",
       "          3.6391e+00,  9.0972e-01,  2.1946e+00, -4.5310e+00,  1.2226e+00,\n",
       "         -6.8329e-01,  6.8095e+00, -4.7799e+00, -5.1743e+00,  3.2346e+00,\n",
       "          3.0979e-01,  1.4646e+00, -8.2829e-01, -3.9452e+00, -1.5841e+00,\n",
       "          3.7009e+00,  2.7685e+00,  3.7258e+00, -6.0372e+00,  1.3848e+00,\n",
       "          1.9947e+00, -2.5668e+00, -5.5299e-01, -4.1540e-01, -2.7301e+00,\n",
       "          1.7968e+00, -3.3780e+00,  4.6231e+00,  3.1870e+00, -1.3935e+00,\n",
       "          7.0591e-01,  4.6726e+00, -1.1646e+00, -5.1369e-01,  1.7701e+00,\n",
       "          7.8858e-01, -2.5495e+00, -2.6191e+00, -8.1905e-01,  5.0906e+00,\n",
       "          2.9658e+00,  1.2082e+00,  1.9351e+00, -1.6555e+00,  6.5468e-03,\n",
       "         -4.8467e+00, -1.5196e+00, -4.1264e+00,  9.0456e-01, -3.1332e+00,\n",
       "          6.4371e-01,  1.7633e+00,  9.3521e+00, -2.5561e+00, -2.5408e+00,\n",
       "         -1.0955e+00, -3.1105e+00, -2.6220e+00,  1.1156e+00,  4.5961e+00,\n",
       "          3.4229e-01,  3.7617e+00, -4.0049e-01, -2.2201e+00, -1.0402e+00,\n",
       "          1.1516e+00,  2.1554e+00,  8.1439e-01,  4.8410e+00, -1.3981e-01,\n",
       "          1.5401e+00,  3.5608e+00,  1.1561e-01,  1.7030e+00, -3.2391e+00,\n",
       "          3.8442e+00,  8.2220e+00,  3.1383e-01,  3.9538e+00,  2.3777e+00,\n",
       "         -3.7772e-01,  1.4606e+00,  3.9071e-01, -2.9737e+00, -3.9843e+00,\n",
       "         -2.8555e+00,  2.0618e+00,  3.2540e+00, -1.8415e-01, -1.0623e+00,\n",
       "          3.6125e+00,  3.1303e+00,  1.5148e+00,  1.1169e-01, -1.5760e+00,\n",
       "         -8.9956e+00, -6.3680e-01,  1.1703e+00,  4.6111e+00,  6.8938e-01,\n",
       "         -4.2425e+00,  6.9681e-01,  4.7176e+00,  4.0743e+00, -2.4369e+00,\n",
       "          2.5492e+00, -3.0636e+00, -9.7204e-01, -6.4254e+00, -1.6607e-01,\n",
       "         -3.3196e+00, -6.8262e+00, -3.1762e+00, -7.3790e-01, -2.7399e+00,\n",
       "          3.9873e+00, -4.8464e+00, -3.1065e-01, -2.5865e+00, -6.3175e+00,\n",
       "          1.3237e+00, -1.4460e+00, -1.3303e+00],\n",
       "        [-4.5582e-01, -2.3128e+00,  7.5686e-02, -1.9109e+00,  2.7747e-01,\n",
       "         -5.7919e-01,  7.7615e-01, -2.3346e+00, -3.9456e+00, -1.3741e+00,\n",
       "          5.2666e-01,  2.1669e+00,  1.3652e-01, -1.5108e+00,  3.9986e+00,\n",
       "          5.3245e-01, -2.2962e-01,  2.3652e+00,  5.3507e-01,  1.1672e+00,\n",
       "          8.8829e-01,  2.7599e+00,  5.9684e-01, -3.4213e+00,  2.8629e+00,\n",
       "         -1.5159e+00, -4.0011e+00,  7.0454e-02,  1.6662e-01, -1.1283e+00,\n",
       "          1.6130e+00,  1.7997e+00, -3.0210e-02,  8.0456e-02, -2.3497e+00,\n",
       "         -1.8930e+00,  1.3633e+00,  2.6303e+00,  1.6147e+00,  2.3151e+00,\n",
       "         -1.3515e+00,  1.2124e+00, -2.8826e+00,  1.3949e+00,  1.8413e+00,\n",
       "          1.0054e+00,  2.1447e+00,  3.9966e+00, -3.7852e+00,  3.9934e-01,\n",
       "         -8.7541e-01, -6.2988e-01, -1.2503e+00, -4.3337e-01, -4.2958e+00,\n",
       "          4.8286e+00, -2.0235e+00,  1.0393e+00,  8.9837e-01,  1.1910e+00,\n",
       "          1.7309e+00,  2.9507e+00, -6.6623e-01,  1.0562e+00, -4.0048e+00,\n",
       "          3.4089e+00, -2.0375e+00, -9.1281e-01, -1.6099e+00,  1.6409e+00,\n",
       "          2.4354e+00, -7.3917e+00,  3.0114e-01,  1.3223e+00, -8.7985e-01,\n",
       "          1.0717e+00, -2.4821e+00,  3.2163e+00,  4.3622e+00,  1.2215e+00,\n",
       "          9.6215e-01,  3.6157e+00, -1.2755e+00,  1.5863e+00,  3.9752e+00,\n",
       "         -2.8349e+00, -3.5866e+00, -6.8771e-01,  4.3008e-01,  2.6213e+00,\n",
       "          4.8614e-01,  1.1952e+00,  1.9797e+00,  9.9667e-03, -6.3851e+00,\n",
       "          8.2317e-01,  5.1579e+00, -2.5946e+00,  5.4369e+00,  1.2307e+00,\n",
       "         -4.3145e+00, -4.7789e-01, -1.1560e+00,  3.1953e+00, -4.5946e+00,\n",
       "          2.4551e+00, -1.9838e+00,  4.4346e-01,  2.4151e+00,  1.0676e+00,\n",
       "         -2.9199e+00, -1.0160e+00,  3.3193e+00,  6.5433e-01,  1.6818e+00,\n",
       "          4.8811e-01,  2.4763e+00,  2.3551e+00,  2.1613e+00,  1.1118e+00,\n",
       "          1.4350e-01,  8.5395e-01,  1.5027e+00, -1.4221e+00, -2.0072e+00,\n",
       "          2.1496e-01, -3.3940e+00,  2.1459e+00],\n",
       "        [-3.6037e+00,  4.8138e+00,  7.6099e-01, -2.0176e+00,  3.7198e+00,\n",
       "          3.4774e+00,  1.5199e+00,  4.6282e-01,  4.6487e+00,  1.4535e+00,\n",
       "          3.2278e-02, -5.3500e-01,  1.1792e+00,  1.9659e+00,  1.5528e+00,\n",
       "         -1.2532e+00,  3.8530e+00,  1.8875e-02,  2.8175e+00, -3.4123e+00,\n",
       "         -2.8912e+00, -2.0217e+00,  3.9167e+00, -4.9682e+00,  1.0973e+00,\n",
       "         -7.0851e-01,  1.0658e+00, -5.8160e-01, -2.1375e+00, -9.4122e+00,\n",
       "         -1.0388e+00,  4.5726e+00, -7.8685e-01,  3.1234e+00,  1.9145e+00,\n",
       "          1.2966e+00,  1.2656e+00,  2.0650e+00,  1.6113e+00, -5.1144e+00,\n",
       "         -5.9210e-01,  3.6922e-01,  1.4100e+00, -2.8242e+00, -3.9958e-01,\n",
       "          4.3035e+00,  5.2822e+00,  5.7528e-01, -3.2898e+00, -7.5043e+00,\n",
       "         -4.2025e+00, -2.7872e+00,  8.8601e-01,  6.0953e-01, -2.5731e+00,\n",
       "         -3.8212e+00, -1.9604e+00, -9.6045e-01, -1.7154e+00,  1.8900e+00,\n",
       "         -8.1265e-01,  1.9984e+00,  3.8008e-01, -5.5809e+00, -4.6302e-01,\n",
       "         -5.1283e-01, -4.5281e-02, -2.5878e-01, -5.0565e+00, -1.8473e-01,\n",
       "         -8.9245e-01,  4.7438e+00, -1.5036e+00, -5.8551e+00,  1.4342e+00,\n",
       "          1.2940e+00,  2.5167e+00,  1.7028e+00, -3.9735e-01,  1.3335e+00,\n",
       "          3.1218e+00,  4.2375e+00,  2.7720e+00,  1.4414e+00,  2.2633e+00,\n",
       "         -2.7056e+00,  3.3830e+00, -1.9068e+00, -1.3245e+00,  1.8285e+00,\n",
       "         -1.9320e+00,  4.5708e-01,  1.3549e+00, -2.9760e+00,  1.3918e+00,\n",
       "          1.5055e+00, -3.9458e-01,  1.8499e-01, -4.7548e+00, -9.9001e-02,\n",
       "          1.7523e+00,  5.0683e+00, -2.1245e+00,  1.6326e+00, -1.2698e+00,\n",
       "         -2.6534e+00,  1.0700e+00, -6.4627e+00, -2.3937e+00, -1.2089e+00,\n",
       "         -7.8995e-01,  2.2085e-01,  1.2804e+00,  3.0285e+00,  2.6889e+00,\n",
       "          4.3416e-01, -8.7198e-01, -1.4593e+00,  1.1493e+00,  1.1216e+00,\n",
       "          3.0293e+00,  1.3461e+00, -4.3094e-01, -3.8603e+00, -1.5844e+00,\n",
       "         -2.3111e-01, -2.4306e+00,  4.3025e+00]]), relevant_doc_embedding=[tensor([[ 1.3617, -4.3830, -0.7546,  ...,  1.5185,  1.5083, -1.6054],\n",
       "        [ 2.2143, -1.0826, -0.3356,  ...,  0.6621,  0.2382,  0.0121],\n",
       "        [-1.0622,  3.9259, -0.4881,  ..., -0.2323, -3.8939,  1.3447],\n",
       "        ...,\n",
       "        [-0.1814, -1.7546,  0.2153,  ..., -3.6233, -0.9456, -0.6901],\n",
       "        [ 1.2393,  0.6342, -3.1868,  ...,  1.9782,  2.6660,  0.3662],\n",
       "        [-0.8964,  0.3709,  0.4004,  ..., -0.2515,  0.2022, -0.3431]]), tensor([[ 1.3617, -4.3830, -0.7546,  ...,  1.5185,  1.5083, -1.6054],\n",
       "        [-0.1988, -0.6115,  0.5423,  ..., -2.4864,  1.6975,  0.3173],\n",
       "        [ 4.7292, -2.9882,  5.9812,  ...,  2.5379,  2.2926,  1.3514],\n",
       "        ...,\n",
       "        [-1.9073, -0.7790,  0.8633,  ..., -0.2148,  0.7905, -0.0267],\n",
       "        [-1.7229, -0.1739,  0.2016,  ...,  1.8597, -3.6384,  2.1286],\n",
       "        [-0.8964,  0.3709,  0.4004,  ..., -0.2515,  0.2022, -0.3431]]), tensor([[ 1.3617, -4.3830, -0.7546,  ...,  1.5185,  1.5083, -1.6054],\n",
       "        [-1.4337, -0.9551,  1.8776,  ..., -0.5414,  0.4585,  0.4249],\n",
       "        [ 0.7738,  0.1720, -3.7457,  ...,  4.3715,  0.3410,  0.4123],\n",
       "        ...,\n",
       "        [ 4.7227, -4.2382,  0.4745,  ...,  2.3963, -0.2874, -0.3599],\n",
       "        [-2.6428, -2.2471, -3.2222,  ..., -7.3019,  0.0549,  5.0805],\n",
       "        [-0.8964,  0.3709,  0.4004,  ..., -0.2515,  0.2022, -0.3431]]), tensor([[-1.4214,  1.6713, -0.6308,  ..., -0.3888,  0.3596, -0.4961],\n",
       "        [ 1.3617, -4.3830, -0.7546,  ...,  1.5185,  1.5083, -1.6054],\n",
       "        [-1.9263, -5.0721, -1.8653,  ...,  0.2732, -1.4727, -3.1224],\n",
       "        ...,\n",
       "        [-2.2033,  0.5486,  1.3528,  ...,  0.1503, -0.0379, -0.8645],\n",
       "        [-7.4775, -4.6068,  0.9858,  ..., -6.8382,  2.1365, -1.1685],\n",
       "        [-0.8964,  0.3709,  0.4004,  ..., -0.2515,  0.2022, -0.3431]]), tensor([[ 1.3617, -4.3830, -0.7546,  ...,  1.5185,  1.5083, -1.6054],\n",
       "        [ 1.8101, -0.5315, -2.4290,  ...,  2.0721, -1.6711, -0.7528],\n",
       "        [-0.1352, -1.2693,  3.6938,  ...,  0.1577,  2.0258, -0.5303],\n",
       "        ...,\n",
       "        [ 0.4694,  2.4938,  5.1746,  ...,  2.4316,  3.1754,  1.2263],\n",
       "        [-0.0690, -1.4845,  2.0217,  ...,  2.1281, -0.0398,  1.8040],\n",
       "        [-0.8964,  0.3709,  0.4004,  ..., -0.2515,  0.2022, -0.3431]]), tensor([[ 1.3617, -4.3830, -0.7546,  ...,  1.5185,  1.5083, -1.6054],\n",
       "        [-1.0244, -4.0075,  3.8588,  ...,  0.1741, -1.6752, -3.5233],\n",
       "        [ 0.3845, -1.5470, -5.0820,  ..., -0.0612,  5.1938, -2.3784],\n",
       "        ...,\n",
       "        [ 2.5543,  4.3740,  2.9426,  ...,  0.8870,  0.2000,  2.1343],\n",
       "        [ 3.5035,  2.0362, -1.6309,  ..., -0.8028, -0.1041,  0.6449],\n",
       "        [-0.8964,  0.3709,  0.4004,  ..., -0.2515,  0.2022, -0.3431]])], irrelevant_doc_embedding=[tensor([[ 1.3617, -4.3830, -0.7546,  ...,  1.5185,  1.5083, -1.6054],\n",
       "        [ 0.0928, -2.2278, -2.0850,  ...,  2.2393, -0.9530, -2.3498],\n",
       "        [ 1.6611,  2.0938,  1.5928,  ...,  1.6341,  2.1433, -1.7254],\n",
       "        ...,\n",
       "        [-0.2246,  3.1233, -2.8974,  ...,  1.6710,  2.3183,  1.4840],\n",
       "        [ 1.8065,  0.2805, -4.2940,  ...,  0.9884,  1.5628,  3.7435],\n",
       "        [-0.8964,  0.3709,  0.4004,  ..., -0.2515,  0.2022, -0.3431]]), tensor([[-0.3282,  1.6173,  0.3119,  ..., -1.5802, -1.8832,  0.6476],\n",
       "        [-0.1255,  2.7753,  0.4820,  ..., -1.2715, -1.9709,  1.2743],\n",
       "        [-1.3267,  0.4620, -1.5523,  ...,  0.9767, -2.6303,  1.9669],\n",
       "        ...,\n",
       "        [-2.6653,  3.0701, -1.2536,  ...,  1.3778,  1.9424, -1.9347],\n",
       "        [ 1.0189, -1.1450, -0.4186,  ..., -0.4635, -0.3761, -0.2265],\n",
       "        [ 0.8118,  1.8590, -0.5643,  ...,  1.3024,  3.3661, -0.5718]]), tensor([[ 1.3617, -4.3830, -0.7546,  ...,  1.5185,  1.5083, -1.6054],\n",
       "        [-1.2076, -1.2640, -5.6558,  ..., -2.4074, -3.6520,  0.7314],\n",
       "        [-1.5685, -0.4549, -1.9339,  ...,  0.4096, -1.0562,  1.2584],\n",
       "        ...,\n",
       "        [-0.2225, -0.6675,  7.2545,  ..., -1.1131, -3.5653,  0.0921],\n",
       "        [ 2.2143, -1.0826, -0.3356,  ...,  0.6621,  0.2382,  0.0121],\n",
       "        [-0.8964,  0.3709,  0.4004,  ..., -0.2515,  0.2022, -0.3431]]), tensor([[ 1.3617, -4.3830, -0.7546,  ...,  1.5185,  1.5083, -1.6054],\n",
       "        [ 0.8900, -5.6346,  0.3695,  ..., -0.6321, -1.3495, -2.0051],\n",
       "        [ 1.3617, -4.3830, -0.7546,  ...,  1.5185,  1.5083, -1.6054],\n",
       "        ...,\n",
       "        [ 5.3970, -0.3511,  0.1671,  ..., -2.2905,  0.7925,  7.5731],\n",
       "        [-0.6147, -2.4668,  2.4523,  ..., -1.2984,  0.1491,  0.3923],\n",
       "        [-0.8964,  0.3709,  0.4004,  ..., -0.2515,  0.2022, -0.3431]]), tensor([[ 1.3617, -4.3830, -0.7546,  ...,  1.5185,  1.5083, -1.6054],\n",
       "        [ 0.2723, -0.7864, -2.9485,  ...,  6.4354,  1.3929, -0.9969],\n",
       "        [ 1.0189, -1.1450, -0.4186,  ..., -0.4635, -0.3761, -0.2265],\n",
       "        ...,\n",
       "        [-1.5059, -0.0500, -1.4283,  ..., -1.0162, -0.1732, -0.7052],\n",
       "        [ 0.5370,  3.1569,  0.3946,  ..., -4.9834,  0.2742,  1.3542],\n",
       "        [-0.8964,  0.3709,  0.4004,  ..., -0.2515,  0.2022, -0.3431]]), tensor([[ 1.3617, -4.3830, -0.7546,  ...,  1.5185,  1.5083, -1.6054],\n",
       "        [ 1.5012, -1.1671,  0.0063,  ...,  0.5451, -0.4994, -0.8484],\n",
       "        [ 2.8714, -0.5279, -3.1933,  ...,  3.3928, -4.3496,  0.8477],\n",
       "        ...,\n",
       "        [-2.9337, -0.2067, -1.0508,  ..., -4.2844, -1.2290, -0.0101],\n",
       "        [-3.6792, -0.8275,  3.5852,  ...,  3.7027,  5.2121,  1.1592],\n",
       "        [-0.8964,  0.3709,  0.4004,  ..., -0.2515,  0.2022, -0.3431]])])"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_triples[0].query_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10047/10047 [00:21<00:00, 471.36it/s]\n"
     ]
    }
   ],
   "source": [
    "validation_triples = triples_to_embeddings(\n",
    "    validate_dataset_copy[triple_columns],\n",
    "    url_to_doctext_mapping,\n",
    "    sp,\n",
    "    w2v_model,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle these to use for training\n",
    "import pickle\n",
    "\n",
    "with open(\"training_triples.pkl\", \"wb\") as f:\n",
    "    pickle.dump([tuple(i) for i in training_triples], f)\n",
    "with open(\"testing_triples.pkl\", \"wb\") as f:\n",
    "    pickle.dump([tuple(i) for i in testing_triples], f)\n",
    "with open(\"validation_triples.pkl\", \"wb\") as f:\n",
    "    pickle.dump([tuple(i) for i in validation_triples], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch dataset from the triples\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82326"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82326, 9)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query_id\n",
       "62367    7\n",
       "10515    7\n",
       "16294    7\n",
       "5458     6\n",
       "38694    6\n",
       "        ..\n",
       "26998    1\n",
       "13036    1\n",
       "37958    1\n",
       "52395    1\n",
       "36237    1\n",
       "Name: count, Length: 56417, dtype: int64"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_dataset_copy[(training_dataset_copy.query_id == 55966)]\n",
    "np.all(train.query_id.value_counts().values == 1)\n",
    "training_dataset_copy.query_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answers</th>\n",
       "      <th>passages</th>\n",
       "      <th>query</th>\n",
       "      <th>query_id</th>\n",
       "      <th>query_type</th>\n",
       "      <th>wellFormedAnswers</th>\n",
       "      <th>hashed_urls</th>\n",
       "      <th>negative_sample_urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Results-Based Accountability is a disciplined...</td>\n",
       "      <td>{'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]...</td>\n",
       "      <td>what is rba</td>\n",
       "      <td>19699</td>\n",
       "      <td>description</td>\n",
       "      <td>[]</td>\n",
       "      <td>[58b2b8024507a2126d53e59a87820ad8, affa5fde381...</td>\n",
       "      <td>[a7509f1dc6a82e1058cac78d284eee49, 9673999d279...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Yes]</td>\n",
       "      <td>{'is_selected': [0, 1, 0, 0, 0, 0, 0], 'passag...</td>\n",
       "      <td>was ronald reagan a democrat</td>\n",
       "      <td>19700</td>\n",
       "      <td>description</td>\n",
       "      <td>[]</td>\n",
       "      <td>[a2d179699e02966bdf9faa65b992daab, f097b1f42e4...</td>\n",
       "      <td>[06aa259b817892b5290b4b037c376591, 79b11837191...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[20-25 minutes]</td>\n",
       "      <td>{'is_selected': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]...</td>\n",
       "      <td>how long do you need for sydney and surroundin...</td>\n",
       "      <td>19701</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[]</td>\n",
       "      <td>[70185a71a8f71d8d1029a71419d5cabf, f8e326ab289...</td>\n",
       "      <td>[6c364273217b774565d18c413d7b0095, 4e96bb1cbd3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[$11 to $22 per square foot]</td>\n",
       "      <td>{'is_selected': [0, 0, 0, 0, 0, 0, 0, 0, 1], '...</td>\n",
       "      <td>price to install tile in shower</td>\n",
       "      <td>19702</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[]</td>\n",
       "      <td>[49abbb4ce58adb0395a9ca4e1846239c, 0a1266e71ec...</td>\n",
       "      <td>[75dc8659a62b8bff44831a44f18463aa, 70d696bab58...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Due to symptoms in the body]</td>\n",
       "      <td>{'is_selected': [0, 0, 1, 0, 0, 0, 0, 0], 'pas...</td>\n",
       "      <td>why conversion observed in body</td>\n",
       "      <td>19703</td>\n",
       "      <td>description</td>\n",
       "      <td>[]</td>\n",
       "      <td>[de25b567b657116e9617dabbf554ac44, ea0e0b397cf...</td>\n",
       "      <td>[bf10f48ebac0dd6ece976b659f25d061, 2606c7238d4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             answers  \\\n",
       "0  [Results-Based Accountability is a disciplined...   \n",
       "1                                              [Yes]   \n",
       "2                                    [20-25 minutes]   \n",
       "3                       [$11 to $22 per square foot]   \n",
       "4                      [Due to symptoms in the body]   \n",
       "\n",
       "                                            passages  \\\n",
       "0  {'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]...   \n",
       "1  {'is_selected': [0, 1, 0, 0, 0, 0, 0], 'passag...   \n",
       "2  {'is_selected': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]...   \n",
       "3  {'is_selected': [0, 0, 0, 0, 0, 0, 0, 0, 1], '...   \n",
       "4  {'is_selected': [0, 0, 1, 0, 0, 0, 0, 0], 'pas...   \n",
       "\n",
       "                                               query  query_id   query_type  \\\n",
       "0                                        what is rba     19699  description   \n",
       "1                       was ronald reagan a democrat     19700  description   \n",
       "2  how long do you need for sydney and surroundin...     19701      numeric   \n",
       "3                    price to install tile in shower     19702      numeric   \n",
       "4                    why conversion observed in body     19703  description   \n",
       "\n",
       "  wellFormedAnswers                                        hashed_urls  \\\n",
       "0                []  [58b2b8024507a2126d53e59a87820ad8, affa5fde381...   \n",
       "1                []  [a2d179699e02966bdf9faa65b992daab, f097b1f42e4...   \n",
       "2                []  [70185a71a8f71d8d1029a71419d5cabf, f8e326ab289...   \n",
       "3                []  [49abbb4ce58adb0395a9ca4e1846239c, 0a1266e71ec...   \n",
       "4                []  [de25b567b657116e9617dabbf554ac44, ea0e0b397cf...   \n",
       "\n",
       "                                negative_sample_urls  \n",
       "0  [a7509f1dc6a82e1058cac78d284eee49, 9673999d279...  \n",
       "1  [06aa259b817892b5290b4b037c376591, 79b11837191...  \n",
       "2  [6c364273217b774565d18c413d7b0095, 4e96bb1cbd3...  \n",
       "3  [75dc8659a62b8bff44831a44f18463aa, 70d696bab58...  \n",
       "4  [bf10f48ebac0dd6ece976b659f25d061, 2606c7238d4...  "
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th triple with query_id: 19699\n",
      "\t#Relevant Docs = 7, Irrelevant Docs = 7\n",
      "\tAll are of shaped: (128,)? True\n",
      "\tThe query text is ['what is rba']\n",
      "\tHere is the relevant docs, Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. Th...\n",
      "\t\tThe Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank ...\n",
      "\t\tRBA Recognized with the 2014 Microsoft US Regional Partner of the ... by PR Newswire. Contract Award...\n",
      "\t\tThe inner workings of a rebuildable atomizer are surprisingly simple. The coil inside the RBA is mad...\n",
      "\t\tResults-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action...\n",
      "\t\tResults-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action...\n",
      "\t\tRBA uses a data-driven, decision-making process to help communities and organizations get beyond tal...\n",
      "\t\tvs. NetIQ Identity Manager. Risk-based authentication (RBA) is a method of applying varying levels o...\n",
      "\t\tA rebuildable atomizer (RBA), often referred to as simply a “rebuildable,” is just a special type of...\n",
      "\t\tGet To Know Us. RBA is a digital and technology consultancy with roots in strategy, design and techn...\n",
      "\t Here are the irrelevant docs, Take note of what you eat. Certain foods triggers a migraine attack in about ten percent of migraine...\n",
      "\t\t1 The Caribs were much more warlike than the Arawaks. 2  The Caribs focused on making boats and weap...\n",
      "\t\tIt's not often that you can find an NBA team on which the oldest player is only 30 years old. That w...\n",
      "\t\tNext, some more definitions: Primitive Character = Plesiomorphy A character which is in the state sh...\n",
      "\t\tThe prime reason why household ducks do not soar into the air is that their body mass is more than i...\n",
      "\t\twe value partnerships. Our relationships with our suppliers are closely connected with the great sho...\n",
      "\t\t(United States). A Master Electrician earns an average wage of $25.84 per hour. Most people in this ...\n",
      "1th triple with query_id: 19700\n",
      "\t#Relevant Docs = 6, Irrelevant Docs = 6\n",
      "\tAll are of shaped: (128,)? True\n",
      "\tThe query text is ['was ronald reagan a democrat']\n",
      "\tHere is the relevant docs, In his younger years, Ronald Reagan was a member of the Democratic Party and campaigned for Democrat...\n",
      "\t\tFrom Wikipedia, the free encyclopedia. A Reagan Democrat is a traditionally Democratic voter in the ...\n",
      "\t\tRonald Reagan began his political life in the Democratic Party, but as he became more and more conse...\n",
      "\t\tRonald Wilson Reagan (/ˈrɒnəld ˈwɪlsən ˈreɪɡən/ ; February 6, 1911 – June 5, 2004) was an American p...\n",
      "\t\tWhen Reagan was a 'liberal Democrat'. In 1948, a very different sounding Ronald Reagan campaigned on...\n",
      "\t\tRonald Reagan (1911-2004), a former actor and California governor, served as the 40th U.S. president...\n",
      "\t\t1984 Re-Election. In November 1984, Ronald Reagan was re-elected in a landslide, defeating Democrati...\n",
      "\t Here are the irrelevant docs, Last year this name was ranked as one of the top 1000 most popular names within its gender. Keep in ...\n",
      "\t\tthe mcl is usually injured when the outside of the knee joint is struck causing the outside of the k...\n",
      "\t\tNo -- tadalafil is the active ingredient in Cialis, not a generic version of it. Although people oft...\n",
      "\t\tThe Liver Function Test includes several blood tests measuring specific proteins and liver enzymes i...\n",
      "\t\tZadkiel the Archangel. Zadkiel Righteousness of God is the archangel of prayer, freedom, benevolence...\n",
      "\t\tFunctional anatomy of the skeletal muscle and muscle fibers. When we think of muscles, we visualize ...\n",
      "2th triple with query_id: 19701\n",
      "\t#Relevant Docs = 6, Irrelevant Docs = 6\n",
      "\tAll are of shaped: (128,)? True\n",
      "\tThe query text is ['how long do you need for sydney and surrounding areas']\n",
      "\tHere is the relevant docs, Sydney, New South Wales, Australia is located in a coastal basin bordered by the Pacific Ocean to th...\n",
      "\t\tThis itinerary will have you crossing the country to take in the Great Barrier Reef, Australia’s ico...\n",
      "\t\tThe Sydney central business district, Sydney harbour and outer suburbs from the West. North Sydney '...\n",
      "\t\t1 Taxis to the city centre should cost approximately $40 (including tolls), and more to other Sydney...\n",
      "\t\tSydney is the capital city of the Australian state of New South Wales, and Australia's largest city....\n",
      "\t\tSydney Attractions. Sydney is home to some of Australia’s most iconic attractions. The Sydney Opera ...\n",
      "\t\tSydney lies on a submergent coastline, where the ocean level has risen to flood deep river valleys (...\n",
      "\t\tOn your right across College Street, in the sandstone building on the corner, is the Australian Muse...\n",
      "\t\tThe Rocks. This is a complete listing of the suburbs and localities in the greater Sydney area in al...\n",
      "\t\tSydney is a very large city, and we haven't spent much time outside of the inner suburbs, apart from...\n",
      "\t Here are the irrelevant docs, LOCATION: Lajes Field, Azores, Portugal The Azores are a group of islands located in the North Atlan...\n",
      "\t\tJaundice is the medical term that describes yellowing of the skin and eyes. This condition forms whe...\n",
      "\t\tThe climate of the Anatolian Plateau (Central Anatolian region) is a steppe climate (there is a grea...\n",
      "\t\tAnswers. Best Answer: This is a type of derivative trade: INTEREST RATE CORRIDOR An interest rate co...\n",
      "\t\tThe World's most comprehensive professionally edited abbreviations and acronyms database. All tradem...\n",
      "\t\tThe tissue that is usually well vascularized and has an extensive extracellular matrix is called? al...\n",
      "3th triple with query_id: 19702\n",
      "\t#Relevant Docs = 6, Irrelevant Docs = 6\n",
      "\tAll are of shaped: (128,)? True\n",
      "\tThe query text is ['price to install tile in shower']\n",
      "\tHere is the relevant docs, In regards to tile installation costs, consumers can expect to pay an average of $25 per square foot...\n",
      "\t\t1 Polished nickel faucets-Average cost is $400 each plus four hours of installation; 2  Install cera...\n",
      "\t\tEnhancement and improvement costs. 1  Polished nickel faucets-Average cost is $400 each plus four ho...\n",
      "\t\tGranite shower tile is available at an average of $3.49 per square foot to $6.99 per square foot. Po...\n",
      "\t\t1 Higher-end tile such as granite or marble is going to cost more than a standard ceramic tile. 2  O...\n",
      "\t\tOur free calculator uses recent, trusted data to estimate costs for your Bathroom Floor Tile Install...\n",
      "\t\tThe cost for a typical small bathroom remodel will range from about $4,000 to $12,000 with the avera...\n",
      "\t\tThe national average for a new shower installation project, when performed by a capable contractor, ...\n",
      "\t\t1 Install ceramic tile floor to match shower-Average prices for installation are between $11 to $22 ...\n",
      "\t Here are the irrelevant docs, Dip a soft cloth into the solution and rub the fingerprints on the appliance with it. Alternatively,...\n",
      "\t\tTake the popular Fanimation ceiling fan The Huxley for example, which dimensions indicate the blades...\n",
      "\t\tIn the cacti this might be mescaline, which is found in Lophophora williamsii, Opuntia cylindrical (...\n",
      "\t\tThe distance from Mandalay Bay to Venetian is 2.04 miles (3.28 kilometers) in straight line. Walking...\n",
      "\t\tStore cooked crabs and picked meat in the coldest part of your refrigerator or meat keeper at 32 deg...\n",
      "\t\tYes, garlic causes more gastro-intestinal distress than nearly any other food product. Gas, bloating...\n",
      "4th triple with query_id: 19703\n",
      "\t#Relevant Docs = 7, Irrelevant Docs = 7\n",
      "\tAll are of shaped: (128,)? True\n",
      "\tThe query text is ['why conversion observed in body']\n",
      "\tHere is the relevant docs, Conclusions: In adult body CT, dose to an organ fully encompassed by the primary radiation beam can ...\n",
      "\t\tThe brain uses loads and loads of energy which is a major challenge for your metabolism-the organ wh...\n",
      "\t\tConversion disorder is a type of somatoform disorder where physical symptoms or signs are present th...\n",
      "\t\tConversion disorder is a mental condition in which a person has blindness, paralysis, or other nervo...\n",
      "\t\tConversion disorder symptoms may appear suddenly after a stressful event or trauma, whether physical...\n",
      "\t\tGlucose. Glucose is a carbohydrate, and is the most important simple sugar in human metabolism. Gluc...\n",
      "\t\tSometimes people with conversion disorder have tremors or symptoms that resemble fainting spells or ...\n",
      "\t\tGlucose is a carbohydrate, and is the most important simple sugar in human metabolism. Glucose is ca...\n",
      "\t Here are the irrelevant docs, Overview. As with most legendary Pokemon, Celebi is blessed with great base stats all around. It als...\n",
      "\t\tCalcium Carbonate (Limestone) Calcium carbonate, the chief component of limestone, is a widely used ...\n",
      "\t\tHealth Benefits of Minerals. Below is a list of some of the minerals found in the body, including th...\n",
      "\t\tAs you might imagine, the $3.99 hosting will offer fewer features than the $7.99 package. The two ma...\n",
      "\t\tIf you eat high quantities of processed foods, sugar, caffeine, and fried foods, chances are, your s...\n",
      "\t\tHuge inside shakeup as Google creates 'Alphabet' umbrella company. Let it never be said that Google ...\n",
      "\t\tAnswers. Best Answer: After completing a course of study in Respiratory Therapy, you can take the vo...\n"
     ]
    }
   ],
   "source": [
    "for i, training_triple in enumerate(training_triples[0:5]):\n",
    "    print(f\"{i}th triple with query_id: {training_triple[0]}\")\n",
    "    print(\n",
    "        f\"\\t#Relevant Docs = {len(training_triple.relevant_doc_embedding)}, Irrelevant Docs = {len(training_triple.irrelevant_doc_embedding)}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"\\tAll are of shaped: {training_triple.relevant_doc_embedding[0].shape}? {all([i.shape == training_triple.relevant_doc_embedding[0].shape for i in training_triple.relevant_doc_embedding])}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"\\tThe query text is {training_dataset_copy[training_dataset_copy.query_id == training_triple.query_id]['query'].values}\"\n",
    "    )\n",
    "    relevant_text = training_dataset_copy[\n",
    "        training_dataset_copy.query_id == training_triple[0]\n",
    "    ][\"passages\"].values[0][\"passage_text\"]\n",
    "    irrelevant_urls_ids = training_dataset_copy[\n",
    "        training_dataset_copy.query_id == training_triple[0]\n",
    "    ][\"negative_sample_urls\"].values[0]\n",
    "    irrelavant_text = [url_to_doctext_mapping[i] for i in irrelevant_urls_ids]\n",
    "\n",
    "    # id -> URL ->\n",
    "    # The relevan\n",
    "    NL = \"\\n\"\n",
    "    TAB = \"\\t\"\n",
    "    print(\n",
    "        f\"\\tHere is the relevant docs, { (NL+TAB+TAB).join([str(i)[0:100] + '...' for i in relevant_text])}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"\\t Here are the irrelevant docs, { (NL+TAB+TAB).join([str(i)[0:100] + '...' for i in irrelavant_text])}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Overview. As with most legendary Pokemon, Celebi is blessed with great base stats all around. It also has a good and synergetic typing, an awesome ability that lets it act as a status absorber, and a phenomenal movepool. These traits allow Celebi to play many roles, depending on your team's needs. Heatran is possibly the best Celebi partner out there as it has almost flawless defensive synergy with it, with Rock being the only type that the two don't resist together, and also beats many Pokemon that can set up on Celebi or OHKO it, such as Scizor, Forretress, Ferrothorn, and Skarmory.\",\n",
       " 'Calcium Carbonate (Limestone) Calcium carbonate, the chief component of limestone, is a widely used amendment to neutralize soil acidity and to supply calcium (Ca) for plant nutrition. The term “lime” can refer to several products, but for agricultural use it generally refers to ground limestone. ',\n",
       " 'Health Benefits of Minerals. Below is a list of some of the minerals found in the body, including their associated benefits. Click on the mineral for more details, or click on the specific condition or disease for home remedies. Copper: This common mineral improves brain function, soothes arthritis, helps in skin care, eliminates throat infections, corrects hemoglobin deficiency, prevents heart diseases, and boosts immunity. It is commonly associated with the uptake of iron and the facilitation of a properly functioning circulatory system.',\n",
       " 'As you might imagine, the $3.99 hosting will offer fewer features than the $7.99 package. The two major considerations when choosing a hosting package are bandwith and storage space. How much you need of each depends on the type of website you are building. If you are building a trypical 100 page or less website, Bandwith refers to how much traffic or visitors your website can handle. Simply put, your bandwith is the amount of gigabytes of data your website can transfer from your hosting provider to your users in a one month period. The price of hosting can vary quite dramatically, from a couple of dollars to several hundred a month. So where do you fit in? For typical small business or online entrepreneur, quality feature-filled website hosting can be purchased for $3.99 – $7.99 per month.',\n",
       " 'If you eat high quantities of processed foods, sugar, caffeine, and fried foods, chances are, your skin is not going to be looking its most radiant. But, loading up on fruits and veggies, whole grains, and vitamin-rich foods makes for a healthy body and, in turn, healthier, more radiant skin.',\n",
       " \"Huge inside shakeup as Google creates 'Alphabet' umbrella company. Let it never be said that Google is boring. Co-founder Larry Page today announced the creation of Alphabet, a new corporation that will comprise Google and other previously Google-held properties to better allow them to act and grow independently. The change was announced across Google's... We will rigorously handle capital allocation and work to make sure each business is executing well. On August 10, 2015 Google's Larry Page announced the formation of a new umbrella corporation named Alphabet.\",\n",
       " 'Answers. Best Answer: After completing a course of study in Respiratory Therapy, you can take the voluntary exam to become a Certified Respiratory Therapist (CRT) and after that you can take two (2) more national exams to become a Registered Respiratory Therapist (RRT).. Submit. · just now. Report Abuse. after completing 2 yrs of respiratory school, u can take your boards for the crt-certified respiratory therapist. u have to pass the crt in order to become an rrt-registered respiratory therapist, which u have to take another boards and a simulation test.']"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irrelavant_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Unzip the batch\n",
    "    query_embs, rel_docs_embs, irrel_docs_embs = zip(*batch)\n",
    "\n",
    "    # Pad sequences within each batch to the same length\n",
    "    # Assuming query_embs, rel_docs_embs, and irrel_docs_embs are lists of tensors\n",
    "    query_embs_padded = pad_sequence(query_embs, batch_first=True, padding_value=0)\n",
    "    rel_docs_embs_padded = [\n",
    "        pad_sequence(docs, batch_first=True, padding_value=0)\n",
    "        for docs in zip(*rel_docs_embs)\n",
    "    ]\n",
    "    irrel_docs_embs_padded = [\n",
    "        pad_sequence(docs, batch_first=True, padding_value=0)\n",
    "        for docs in zip(*irrel_docs_embs)\n",
    "    ]\n",
    "\n",
    "    return query_embs_padded, rel_docs_embs_padded, irrel_docs_embs_padded\n",
    "\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        # No need to unroll the sequences here\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # Returns the total number of triplets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _, query_embedding, relevant_docs_embeddings, irrelevant_docs_embeddings = (\n",
    "            self.data[idx]\n",
    "        )\n",
    "        # No need to iterate through the sequences; return them as they are\n",
    "        return (\n",
    "            torch.tensor(query_embedding),\n",
    "            torch.tensor(relevant_docs_embeddings),\n",
    "            torch.tensor(irrelevant_docs_embeddings),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_triplet_dataset = TripletDataset(training_triples)\n",
    "test_triplet_dataset = TripletDataset(testing_triples)\n",
    "validate_triplet_dataset = TripletDataset(validation_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 128])"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.tensor(training_triples[0][2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query is at index 1 and has Embedding dimension 128\n",
      "Relevant document is len= 7\n",
      "Each relevant doc embedding is len= 128\n",
      "# Irrelvant docs are the same for each relevant doc=  7\n",
      "# these are too having embedding dimension= 128\n",
      "Query is at index 1 and has Embedding dimension 128\n",
      "Relevant document is len= 6\n",
      "Each relevant doc embedding is len= 128\n",
      "# Irrelvant docs are the same for each relevant doc=  6\n",
      "# these are too having embedding dimension= 128\n",
      "Query is at index 1 and has Embedding dimension 128\n",
      "Relevant document is len= 6\n",
      "Each relevant doc embedding is len= 128\n",
      "# Irrelvant docs are the same for each relevant doc=  6\n",
      "# these are too having embedding dimension= 128\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\n",
    "        \"Query is at index 1 and has Embedding dimension\", len(training_triples[i][1])\n",
    "    )\n",
    "    print(\n",
    "        \"# Relevant document is len=\", len(training_triples[i].relevant_doc_embedding)\n",
    "    )\n",
    "    print(\n",
    "        \"Each relevant doc embedding is len=\",\n",
    "        len(training_triples[i].relevant_doc_embedding[0]),\n",
    "    )\n",
    "    print(\n",
    "        \"# Irrelvant docs are the same for each relevant doc= \",\n",
    "        len(training_triples[i].irrelevant_doc_embedding),\n",
    "    )\n",
    "    print(\n",
    "        \"# these are too having embedding dimension=\",\n",
    "        len(training_triples[0].irrelevant_doc_embedding[0]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(\n",
    "    training_triplet_dataset, batch_size=20, shuffle=False, collate_fn=custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 128])\n",
      "20 torch.Size([20, 128])\n",
      "None\n",
      "torch.Size([20, 128])\n"
     ]
    }
   ],
   "source": [
    "for i, (q, rel, irrel) in enumerate(training_dataloader):\n",
    "    print(q.shape)\n",
    "    print(print(len(rel[0]), rel[0].shape))\n",
    "    print(irrel[0].shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the data through two tower RNN, simple RNNs to\n",
    "# start. No LSTM or GRUs with a margin and triplet loss.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RNNTower(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNTower, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size, hidden_size=hidden_size, batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x is of shape (batch, seq_len, input_size)\n",
    "        print(x.shape)\n",
    "        _, hidden = self.rnn(x)\n",
    "        # hidden is of shape (num_layers * num_directions, batch, hidden_size)\n",
    "        # For the simple RNN, num_layers * num_directions = 1\n",
    "        print(hidden.shape)\n",
    "        hidden = hidden.squeeze(0)  # Now: (batch, hidden_size)\n",
    "        print(hidden.shape)\n",
    "        output = self.fc(hidden)\n",
    "        return output\n",
    "\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
    "    print(anchor.shape, positive.shape, negative.shape)\n",
    "    distance_positive = (anchor - positive).pow(2).sum(1)\n",
    "    distance_negative = (anchor - negative).pow(2).sum(1)\n",
    "    losses = torch.relu(distance_positive - distance_negative + margin)\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming D is the dimensionality of your embeddings\n",
    "input_size = EMBEDDING_SIZE  # Dimensionality of the input\n",
    "hidden_size = 256  # Size of the RNN's hidden layer\n",
    "output_size = 64  # Size of the final outout\n",
    "\n",
    "query_rnn = RNNTower(input_size, hidden_size, output_size)\n",
    "doc_rnn = RNNTower(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEXT: Training loop for the rnn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(\n",
    "    list(query_rnn.parameters()) + list(doc_rnn.parameters()), lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 128])\n",
      "torch.Size([1, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([12, 128])\n",
      "torch.Size([1, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([12, 128])\n",
      "torch.Size([1, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([64]) torch.Size([64]) torch.Size([64])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[446], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m pos_doc_output \u001b[38;5;241m=\u001b[39m doc_rnn(pos_doc_emb)\n\u001b[1;32m      9\u001b[0m neg_doc_output \u001b[38;5;241m=\u001b[39m doc_rnn(neg_doc_emb)\n\u001b[0;32m---> 11\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtriplet_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_doc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_doc_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[443], line 30\u001b[0m, in \u001b[0;36mtriplet_loss\u001b[0;34m(anchor, positive, negative, margin)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtriplet_loss\u001b[39m(anchor, positive, negative, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(anchor\u001b[38;5;241m.\u001b[39mshape, positive\u001b[38;5;241m.\u001b[39mshape, negative\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 30\u001b[0m     distance_positive \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43manchor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpositive\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     distance_negative \u001b[38;5;241m=\u001b[39m (anchor \u001b[38;5;241m-\u001b[39m negative)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m     losses \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(distance_positive \u001b[38;5;241m-\u001b[39m distance_negative \u001b[38;5;241m+\u001b[39m margin)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    for query_emb, pos_doc_emb, neg_doc_emb in training_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Assuming embeddings are of shape (batch_size, seq_len, input_size)\n",
    "        # You might need to adjust the shape based on how your data is structured\n",
    "        query_output = query_rnn(query_emb)\n",
    "        pos_doc_output = doc_rnn(pos_doc_emb)\n",
    "        neg_doc_output = doc_rnn(neg_doc_emb)\n",
    "\n",
    "        loss = triplet_loss(query_output, pos_doc_output, neg_doc_output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0029e-01, 1.5726e-01, 9.5655e-02, 2.4245e-02, 4.0921e-02, 3.1036e-02,\n",
       "        2.5124e-02, 3.4916e-01, 8.3306e-02, 9.9364e-02, 1.7097e-01, 1.5423e-01,\n",
       "        3.3385e-02, 1.1975e-02, 3.9449e-03, 3.1547e-02, 3.8071e-02, 7.3160e-03,\n",
       "        1.0696e-01, 1.0372e-02, 2.1252e-01, 3.5784e-02, 4.9183e-01, 6.0580e-02,\n",
       "        2.9841e-02, 1.3620e-01, 3.1525e-01, 1.9255e-02, 4.8204e-02, 4.2308e-02,\n",
       "        4.0516e-02, 1.9204e-01, 8.7218e-02, 4.2212e-02, 2.8077e-02, 8.6364e-04,\n",
       "        8.6861e-02, 1.0265e-03, 4.2759e-02, 1.7970e-02, 8.3220e-02, 2.5751e-02,\n",
       "        2.7908e-01, 5.2570e-02, 3.4769e-02, 9.2672e-02, 1.2490e-01, 4.5189e-04,\n",
       "        7.1303e-03, 2.9351e-01, 9.6823e-02, 1.0253e-02, 5.6388e-02, 1.4068e-04,\n",
       "        1.4050e-04, 2.7060e-01, 9.5076e-02, 4.2174e-05, 2.5622e-01, 3.4718e-02,\n",
       "        1.1543e-04, 5.9856e-02, 4.7871e-02, 5.9104e-04],\n",
       "       grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(query_output - pos_doc_output).pow(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [torch.randn(1, 4) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "tensor([[1.2411, 0.6259, 1.1159]])\n",
      "tensor([[[1.2411, 0.6259, 1.1159]]])\n",
      ".\n",
      "tensor([[0.6223, 1.2001, 0.7883]])\n",
      "tensor([[[0.6223, 1.2001, 0.7883]]])\n",
      ".\n",
      "tensor([[ 0.6911, -1.6744,  0.8264]])\n",
      "tensor([[[ 0.6911, -1.6744,  0.8264]]])\n",
      ".\n",
      "tensor([[-0.0656, -1.2043,  0.6886]])\n",
      "tensor([[[-0.0656, -1.2043,  0.6886]]])\n",
      ".\n",
      "tensor([[0.3123, 0.6284, 0.6254]])\n",
      "tensor([[[0.3123, 0.6284, 0.6254]]])\n"
     ]
    }
   ],
   "source": [
    "for i in inputs:\n",
    "    print(\".\")\n",
    "    print(i)\n",
    "    print(i.view(1, 1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8364, -0.7349,  0.2562,  1.7806]],\n",
       "\n",
       "        [[-0.1919, -1.2498,  0.9293, -0.8176]],\n",
       "\n",
       "        [[-0.1253, -1.9675, -0.9701,  0.7632]],\n",
       "\n",
       "        [[-0.1750,  0.2369,  0.0190, -0.4398]],\n",
       "\n",
       "        [[-1.0326, -0.0065,  1.7244, -0.4539]]])"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "# [\n",
    "#  [[1, 2, 3]],\n",
    "#     [[4, 5, 6]],\n",
    "#     [[7, 8, 9]],\n",
    "#     [[10, 11, 12]],\n",
    "#     [[13, 14, 15]]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = nn.Embedding(5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e(torch.tensor([1, 2, 3])).shape  # 3*10, 3 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
