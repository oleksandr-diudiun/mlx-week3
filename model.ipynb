{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Data Processing Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# SentencePiece\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "import gensim.downloader\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model Creation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_sentence_generator(df):\n",
    "    for sentence in df:\n",
    "        yield sentence\n",
    "        \n",
    "def get_sentence_tokens(sentence, sp):\n",
    "    '''Gets the word tokens of a given sentence by tokenising'''\n",
    "    tokens = sp.encode_as_pieces(sentence)\n",
    "    return tokens\n",
    "\n",
    "def get_avg_vector_for_sent(tokens, model):\n",
    "    '''Uses the Word2Vec LUT to get the vector representation of a tokens \n",
    "       in a sentence and takes the mean'''\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            vector += model.wv[token]\n",
    "    if len(tokens) > 0: \n",
    "        vector /= len(tokens)\n",
    "    return vector\n",
    "    \n",
    "def sentence_to_vector(sentence, model,sp):\n",
    "    '''Returns the vector representation of a sentence\n",
    "       by taking the average of the Word2Vec representations of the \n",
    "       words / subwords'''\n",
    "    tokens = get_sentence_tokens(sentence,sp)\n",
    "    vector = get_avg_vector_for_sent(tokens, model)\n",
    "    return vector\n",
    "\n",
    "def train_fasttext_model(tokenized_texts, vector_size=25):\n",
    "    fasttext_model = FastText(sentences=tokenized_texts, vector_size=vector_size, window=5, min_count=1, workers=4, sg=1)  # sg=1 means skip-gram\n",
    "    return fasttext_model\n",
    "\n",
    "def fasttext_sentence_to_vector(sentence, model):\n",
    "    '''Directly uses FastText to get the vector representation of a sentence'''\n",
    "    # Splitting the sentence into words; this could be replaced or refined as needed\n",
    "    tokens = sentence.split()  \n",
    "    vector = np.zeros(model.vector_size)\n",
    "    for token in tokens:\n",
    "        vector += model.wv[token]  # FastText will handle OOV words using n-grams\n",
    "    if len(tokens) > 0: \n",
    "        vector /= len(tokens)\n",
    "    return vector\n",
    "\n",
    "def ngrams(word, min_n=3, max_n=6):\n",
    "    \"\"\"Generate n-grams for a word.\"\"\"\n",
    "    extended_word = f\"<{word}>\"\n",
    "    ngrams = [extended_word[i:i+n] for n in range(min_n, max_n+1) for i in range(len(extended_word)-n+1)]\n",
    "    return list(set(ngrams))\n",
    "\n",
    "def fasttext_oov_word_vector(word, model, min_n=3, max_n=6):\n",
    "    \"\"\"Construct a vector for an OOV word by averaging its n-grams that are in the model.\n",
    "       If no n-grams are found in the model, return a zero vector.\"\"\"\n",
    "    word_ngrams = ngrams(word, min_n, max_n)\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    valid_ngrams = 0\n",
    "    for ngram in word_ngrams:\n",
    "        if ngram in model:\n",
    "            vector += model[ngram]\n",
    "            valid_ngrams += 1\n",
    "    if valid_ngrams > 0:\n",
    "        vector /= valid_ngrams\n",
    "    else:\n",
    "        # Return a zero vector if no valid n-grams are found\n",
    "        vector = np.zeros(model.vector_size)\n",
    "    return vector\n",
    "\n",
    "def pt_fasttext_sentence_to_vector(sentence, model, min_n=3, max_n=6):\n",
    "    \"\"\"Get the vector representation of a sentence by averaging the vectors of its words.\n",
    "       Handles OOV words by constructing their vectors from subwords or using a zero vector if necessary.\"\"\"\n",
    "    tokens = sentence.split()  # Splitting the sentence into words; refine as needed\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    for token in tokens:\n",
    "        if token in model:\n",
    "            vector += model[token]\n",
    "        else:\n",
    "            # Attempt to construct the vector for OOV word from its n-grams or use a zero vector\n",
    "            vector += fasttext_oov_word_vector(token, model, min_n, max_n)\n",
    "    if len(tokens) > 0:\n",
    "        vector /= len(tokens)\n",
    "    return vector\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpvotesPredictorNN(nn.Module):\n",
    "    def __init__(self, word_dimensionality):\n",
    "        super(UpvotesPredictorNN, self).__init__()\n",
    "        \n",
    "        # Input layer to 1st hidden layer\n",
    "        self.fc1 = nn.Linear(word_dimensionality, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # 1st hidden layer to 2nd hidden layer\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # 2nd hidden layer to output layer\n",
    "        self.output = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        output = self.output(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.sys.platform == 'darwin':      # MacOS\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "elif os.sys.platform == 'win32':    # Windows\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:                               # other OS\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/TitlesAndScoreALL.csv'\n",
    "data_df = pd.read_csv(file_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Embedding Dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dimensionality = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(word, min_n=3, max_n=6):\n",
    "    \"\"\"\n",
    "    Generate n-grams for a word.\n",
    "    \"\"\"\n",
    "    extended_word = f\"<{word}>\"\n",
    "    ngrams = [extended_word[i:i+n] for n in range(min_n, max_n+1) for i in range(len(extended_word)-n+1)]\n",
    "    return list(set(ngrams))\n",
    "\n",
    "def fasttext_oov_word_vector(word, model, min_n=3, max_n=6):\n",
    "    \"\"\"\n",
    "    Construct a vector for an OOV word by averaging its n-grams that are in the model.\n",
    "    \"\"\"\n",
    "    word_ngrams = ngrams(word, min_n, max_n)\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    valid_ngrams = 0\n",
    "    for ngram in word_ngrams:\n",
    "        if ngram in model:\n",
    "            vector += model[ngram]\n",
    "            valid_ngrams += 1\n",
    "    if valid_ngrams > 0:\n",
    "        vector /= valid_ngrams\n",
    "    return vector\n",
    "\n",
    "def pt_fasttext_sentence_to_vector(sentence, model, min_n=3, max_n=6):\n",
    "    \"\"\"\n",
    "    Get the vector representation of a sentence by averaging the vectors of its words.\n",
    "    Handles OOV words by constructing their vectors from subwords.\n",
    "    \"\"\"\n",
    "    tokens = sentence.split()  # Splitting the sentence into words; refine as needed\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    for token in tokens:\n",
    "        if token in model:\n",
    "            vector += model[token]\n",
    "        else:\n",
    "            # Attempt to construct the vector for OOV word from its n-grams\n",
    "            vector += fasttext_oov_word_vector(token, model, min_n, max_n)\n",
    "    if len(tokens) > 0:\n",
    "        vector /= len(tokens)\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_df = data_df.copy()\n",
    "ft_df = data_df.copy()\n",
    "pt_df = data_df.copy()\n",
    "bert_df = data_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    sentence_iterator=dataframe_sentence_generator(data_df['title']), \n",
    "    vocab_size=12_828,\n",
    "    model_prefix='./data/spm_model', \n",
    "    model_type='word',\n",
    ")\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('./data/spm_model.model')\n",
    "\n",
    "tokenized_titles = [sp.encode_as_pieces(title) for title in data_df['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram tokens, word2vec\n",
    "word2vec_model = Word2Vec(sentences=tokenized_titles, vector_size=word_dimensionality, window=5, min_count=1, workers=4, sg=1)\n",
    "w2v_df['sentence_vector'] = w2v_df['title'].apply(lambda x: sentence_to_vector(x, word2vec_model, sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untrained tokens, fasttext\n",
    "fasttext_model = FastText(sentences=ft_df['title'], vector_size=word_dimensionality, window=5, min_count=1, workers=4, sg=1)\n",
    "ft_df['sentence_vector'] = ft_df['title'].apply(lambda x: fasttext_sentence_to_vector(x, fasttext_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained tokens, fasttext\n",
    "vectors = gensim.downloader.load('fasttext-wiki-news-subwords-300')\n",
    "pt_df['sentence_vector'] = pt_df['title'].apply(lambda x: pt_fasttext_sentence_to_vector(x, vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "def bert_text_preparation(text, tokenizer):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(indexed_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = torch.tensor([segments_ids])\n",
    "    return tokens_tensor, segments_tensor\n",
    "\n",
    "def get_bert_embeddings(row):\n",
    "    tokens_tensor, segments_tensor = bert_text_preparation(row['title'], tokenizer)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensor)\n",
    "        hidden_states = outputs[2]\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1, 0, 2)\n",
    "    sentence_embedding = torch.mean(token_embeddings[:, -2, :], dim=0)\n",
    "    return sentence_embedding.numpy()\n",
    "\n",
    "tqdm.pandas()\n",
    "bert_df['sentence_vector'] = bert_df.progress_apply(get_bert_embeddings, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(data_df):\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test_and_val, y_train, y_test_and_val = train_test_split(\n",
    "        data_df['sentence_vector'].tolist(), \n",
    "        data_df['score'], \n",
    "        test_size=0.3, \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    X_test, X_val, y_test, y_val = train_test_split(\n",
    "        X_test_and_val, \n",
    "        y_test_and_val, \n",
    "        test_size=0.5, \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    X_train = torch.tensor(np.vstack(X_train), dtype=torch.float32)\n",
    "    y_train = torch.tensor(np.array(y_train), dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    X_val = torch.tensor(np.vstack(X_val), dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(np.array(y_val), dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    X_test = torch.tensor(np.vstack(X_test), dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(np.array(y_test), dtype=torch.float32).view(-1, 1)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(X_train, X_test, X_val, y_train, y_test, y_val, word_dimensionality):\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    X_val = X_val.to(device)\n",
    "    y_val = y_val.to(device)\n",
    "        \n",
    "    # Define model and hyperparameters\n",
    "    model = UpvotesPredictorNN(word_dimensionality=word_dimensionality).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    num_epochs = 200\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Storage object for data vis\n",
    "    list_of_lists = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train.to(device))\n",
    "        loss = criterion(outputs, y_train.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_val)\n",
    "            val_loss = criterion(predictions, y_val)\n",
    "        \n",
    "        # Store loss for data vis\n",
    "        list_of_lists.append([epoch, loss.item(), val_loss.item()])\n",
    "        \n",
    "        # # Logging\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch+1},\\t Train Loss: {loss.item()},\\t Val Loss: {val_loss.item()}')\n",
    "            \n",
    "    return list_of_lists, model, criterion, X_test, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(results):\n",
    "    list_of_lists, model, criterion, X_test, y_test = results\n",
    "    \n",
    "    X_test =X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    \n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    epoch_list = []\n",
    "    \n",
    "    for i,j,k in list_of_lists:\n",
    "        epoch_list.append(i)\n",
    "        train_loss_list.append(j)\n",
    "        val_loss_list.append(k)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "            model.eval()\n",
    "            predictions = model(X_test)\n",
    "            test_loss = criterion(predictions, y_test) \n",
    "    \n",
    "    print(f'Test Loss: {test_loss.item()}')\n",
    "        \n",
    "    return epoch_list, train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, X_test, y_train, y_val, y_test = data_split(w2v_df)\n",
    "# w2v_results = train_loop(X_train, X_test, X_val, y_train, y_test, y_val, word_dimensionality)\n",
    "# w2v_plot = plot(w2v_results)\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test = data_split(ft_df)\n",
    "# ft_df_results = train_loop(X_train, X_test, X_val, y_train, y_test, y_val, word_dimensionality)\n",
    "# ft_plot = plot(ft_df_results)\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test = data_split(pt_df)\n",
    "# pt_df_results = train_loop(X_train, X_test, X_val, y_train, y_test, y_val, word_dimensionality)\n",
    "# pt_plot = plot(pt_df_results)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = data_split(bert_df)\n",
    "bert_results = train_loop(X_train, X_test, X_val, y_train, y_test, y_val, word_dimensionality)\n",
    "bert_plot = plot(bert_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "# Labels for each plot for clarity\n",
    "labels = ['Word2Vec', 'FastText', 'Pretrained FastText']\n",
    "linestyles = ['-', '--', '-.']\n",
    "\n",
    "for i, (epoch_list, train_loss_list, val_loss_list) in enumerate([w2v_plot, ft_plot, pt_plot, bert_plot]):\n",
    "    # Plot training and validation loss for each model\n",
    "    ax.plot(epoch_list, train_loss_list, label=f'Train Loss - {labels[i]}', linestyle=linestyles[i], color='blue')\n",
    "    ax.plot(epoch_list, val_loss_list, label=f'Val Loss - {labels[i]}', linestyle=linestyles[i], color='orange')\n",
    "\n",
    "# Set the title and labels\n",
    "ax.set_title('Loss Over Epochs')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "\n",
    "# Enable the legend\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
