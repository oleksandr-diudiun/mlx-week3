{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet('train.parquet')\n",
    "test = pd.read_parquet('test.parquet')\n",
    "validate = pd.read_parquet('validate.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect all texts to one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('AllTexts.txt', 'w') as f:\n",
    "#     pass  # This just creates the file, immediately closing it\n",
    "\n",
    "# with open('AllTexts.txt', 'a') as f:  # Open file in append mode\n",
    "#     for _, row in train.iterrows():\n",
    "#         concatenated = '\\n'.join(row['passages']['passage_text'])\n",
    "#         concatenated = '\\n'.join([concatenated, '\\n'.join(row['answers'])])\n",
    "#         concatenated = '\\n'.join([concatenated, row['query']])\n",
    "#         f.write(concatenated.lower() + '\\n')\n",
    "#     for _, row in test.iterrows():\n",
    "#         concatenated = '\\n'.join(row['passages']['passage_text'])\n",
    "#         concatenated = '\\n'.join([concatenated, '\\n'.join(row['answers'])])\n",
    "#         concatenated = '\\n'.join([concatenated, row['query']])\n",
    "#         f.write(concatenated.lower() + '\\n')\n",
    "#     for _, row in validate.iterrows():\n",
    "#         concatenated = '\\n'.join(row['passages']['passage_text'])\n",
    "#         concatenated = '\\n'.join([concatenated, '\\n'.join(row['answers'])])\n",
    "#         concatenated = '\\n'.join([concatenated, row['query']])\n",
    "#         f.write(concatenated.lower() + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train tokanizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spm.SentencePieceTrainer.train(\n",
    "#     input = 'AllTexts.txt',\n",
    "#     model_prefix='spm_AllTexts', \n",
    "#     vocab_size=30000,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokanizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('spm_AllTexts.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_file(file_path, sp_processor):\n",
    "    tokenized_sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Assuming each line in the file is a separate sentence or paragraph\n",
    "            # Tokenize the line and add the list of tokens to the tokenized_sentences list\n",
    "            tokenized_sentences.append(sp_processor.encode_as_pieces(line.strip()))\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokinize all the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokinized_sentences = tokenize_file(\"AllTexts.txt\", sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export tokens to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"Tokens_AllText.json\", 'w', encoding='utf-8') as file:\n",
    "#     json.dump(tokinized_sentences, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    min_count  =20,\n",
    "    window     =10,\n",
    "    vector_size=vector_size,\n",
    "    sample     =6e-5, \n",
    "    alpha      = 0.03, \n",
    "    min_alpha  = 0.0007, \n",
    "    negative   = 20,\n",
    "    workers    = multiprocessing.cpu_count() - 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(tokinized_sentences))\n",
    "# w2v_model.build_vocab(tokinized_sentences)\n",
    "# w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"word2vec_vocab.txt\", 'w') as vocab_file:\n",
    "#     for word in w2v_model.wv.key_to_index.keys():\n",
    "#         vocab_file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model.train(tokinized_sentences, total_examples=w2v_model.corpus_count, epochs=20, report_delay=1)\n",
    "# w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = w2v_model.wv.most_similar('▁hacker', topn=4)\n",
    "print(similar_words)\n",
    "print(w2v_model.wv.most_similar(sp.encode_as_pieces('Hacker')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_embedding(sp, text, vector_size):\n",
    "    tokens = sp.encode_as_pieces(text)\n",
    "\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if (token in w2v_model.wv): \n",
    "            embeddings.append(w2v_model.wv[token])\n",
    "\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triples for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTriplesTokens(dataframe):\n",
    "    triples = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        available_indices = list(dataframe.index)\n",
    "        available_indices.remove(index)\n",
    "        \n",
    "        for relevant in row['passages']['passage_text']:\n",
    "            random_index = np.random.choice(available_indices)\n",
    "            random_doc_index = np.random.choice(\n",
    "                list(\n",
    "                    range(\n",
    "                        len(dataframe.iloc[random_index]['passages']['passage_text'])\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "            irrelevant = dataframe.iloc[random_index]['passages']['passage_text'][random_doc_index]\n",
    "\n",
    "            triples.append([\n",
    "                row['query'],\n",
    "                relevant,\n",
    "                irrelevant,\n",
    "            ])\n",
    "\n",
    "    return triples\n",
    "\n",
    "train_triplets = prepareTriplesTokens(train)\n",
    "test_triplets = prepareTriplesTokens(test)\n",
    "validate_triplets = prepareTriplesTokens(validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what is rba', \"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\", 'This report describes the typical weather at the Double Eagle II Airport (Albuquerque, New Mexico, United States) weather station over the course of an average year. It is based on the historical records from 2001 to 2012. Earlier records are either unavailable or unreliable. The daily average low (blue) and high (red) temperature with percentile bands (inner band from 25th to 75th percentile, outer band from 10th to 90th percentile). The warm season lasts from May 23 to September 9 with an average daily high temperature above 81A°81â. f'], ['what is rba', \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\", \"1. district, community the vicar of a small parish in a West Country town. 2. community, fold, flock, church, congregation, parishioners, churchgoers The whole parish will object if he is appointed as priest. Take a lodging near a large parish church, in a remote part of London'-- (this is my friend's advice)--'go to the clerk, tell him you want to be married by banns, and say you belong to that parish.\"]]\n"
     ]
    }
   ],
   "source": [
    "print(train_triplets[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of triples to a DataFrame\n",
    "columns = ['query', 'relevant', 'irrelevant']\n",
    "train_triplets = pd.DataFrame(train_triplets, columns=columns)\n",
    "test_triplets = pd.DataFrame(test_triplets, columns=columns)\n",
    "validate_triplets = pd.DataFrame(validate_triplets, columns=columns)\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "train_triplets.to_parquet('train_triplets.parquet', engine='pyarrow') \n",
    "test_triplets.to_parquet('test_triplets.parquet', engine='pyarrow') \n",
    "validate_triplets.to_parquet('validate_triplets.parquet', engine='pyarrow') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
