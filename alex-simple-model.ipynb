{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet('train.parquet')\n",
    "test = pd.read_parquet('test.parquet')\n",
    "validate = pd.read_parquet('validate.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect all texts to one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('AllTexts.txt', 'w') as f:\n",
    "#     pass  # This just creates the file, immediately closing it\n",
    "\n",
    "# with open('AllTexts.txt', 'a') as f:  # Open file in append mode\n",
    "#     for _, row in train.iterrows():\n",
    "#         concatenated = '\\n'.join(row['passages']['passage_text'])\n",
    "#         concatenated = '\\n'.join([concatenated, '\\n'.join(row['answers'])])\n",
    "#         concatenated = '\\n'.join([concatenated, row['query']])\n",
    "#         f.write(concatenated.lower() + '\\n')\n",
    "#     for _, row in test.iterrows():\n",
    "#         concatenated = '\\n'.join(row['passages']['passage_text'])\n",
    "#         concatenated = '\\n'.join([concatenated, '\\n'.join(row['answers'])])\n",
    "#         concatenated = '\\n'.join([concatenated, row['query']])\n",
    "#         f.write(concatenated.lower() + '\\n')\n",
    "#     for _, row in validate.iterrows():\n",
    "#         concatenated = '\\n'.join(row['passages']['passage_text'])\n",
    "#         concatenated = '\\n'.join([concatenated, '\\n'.join(row['answers'])])\n",
    "#         concatenated = '\\n'.join([concatenated, row['query']])\n",
    "#         f.write(concatenated.lower() + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train tokanizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spm.SentencePieceTrainer.train(\n",
    "#     input = 'AllTexts.txt',\n",
    "#     model_prefix='spm_AllTexts', \n",
    "#     vocab_size=30000,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokanizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('spm_AllTexts.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_file(file_path, sp_processor):\n",
    "    tokenized_sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Assuming each line in the file is a separate sentence or paragraph\n",
    "            # Tokenize the line and add the list of tokens to the tokenized_sentences list\n",
    "            tokenized_sentences.append(sp_processor.encode_as_pieces(line.strip()))\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokinize all the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokinized_sentences = tokenize_file(\"AllTexts.txt\", sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export tokens to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"Tokens_AllText.json\", 'w', encoding='utf-8') as file:\n",
    "#     json.dump(tokinized_sentences, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    min_count  =20,\n",
    "    window     =10,\n",
    "    vector_size=vector_size,\n",
    "    sample     =6e-5, \n",
    "    alpha      = 0.03, \n",
    "    min_alpha  = 0.0007, \n",
    "    negative   = 20,\n",
    "    workers    = multiprocessing.cpu_count() - 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(tokinized_sentences))\n",
    "# w2v_model.build_vocab(tokinized_sentences)\n",
    "# w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"word2vec_vocab.txt\", 'w') as vocab_file:\n",
    "#     for word in w2v_model.wv.key_to_index.keys():\n",
    "#         vocab_file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model.train(tokinized_sentences, total_examples=w2v_model.corpus_count, epochs=20, report_delay=1)\n",
    "# w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = w2v_model.wv.most_similar('‚ñÅhacker', topn=4)\n",
    "print(similar_words)\n",
    "print(w2v_model.wv.most_similar(sp.encode_as_pieces('Hacker')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_embedding(sp, text, vector_size):\n",
    "    tokens = sp.encode_as_pieces(text)\n",
    "\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if (token in w2v_model.wv): \n",
    "            embeddings.append(w2v_model.wv[token])\n",
    "\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triples for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTriplesTokens(dataframe):\n",
    "    triples = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        available_indices = list(dataframe.index)\n",
    "        available_indices.remove(index)\n",
    "        \n",
    "        for relevant in row['passages']['passage_text']:\n",
    "            random_index = np.random.choice(available_indices)\n",
    "            random_doc_index = np.random.choice(\n",
    "                list(\n",
    "                    range(\n",
    "                        len(dataframe.iloc[random_index]['passages']['passage_text'])\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "            irrelevant = dataframe.iloc[random_index]['passages']['passage_text'][random_doc_index]\n",
    "\n",
    "            triples.append([\n",
    "                row['query'],\n",
    "                relevant,\n",
    "                irrelevant,\n",
    "            ])\n",
    "\n",
    "    return triples\n",
    "\n",
    "train_triplets = prepareTriplesTokens(train)\n",
    "test_triplets = prepareTriplesTokens(test)\n",
    "validate_triplets = prepareTriplesTokens(validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676193\n"
     ]
    }
   ],
   "source": [
    "print(len(train_triplets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of triples to a DataFrame\n",
    "columns = ['query', 'relevant', 'irrelevant']\n",
    "train_triplets = pd.DataFrame(train_triplets, columns=columns)\n",
    "test_triplets = pd.DataFrame(test_triplets, columns=columns)\n",
    "validate_triplets = pd.DataFrame(validate_triplets, columns=columns)\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "train_triplets.to_parquet('train_triplets.parquet', engine='pyarrow') \n",
    "test_triplets.to_parquet('test_triplets.parquet', engine='pyarrow') \n",
    "validate_triplets.to_parquet('validate_triplets.parquet', engine='pyarrow') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryDocsDataset(Dataset):\n",
    "    def __init__(self, queries, relevant_docs, irrelevant_docs):\n",
    "        self.queries = queries\n",
    "        self.relevant_docs = relevant_docs\n",
    "        self.irrelevant_docs = irrelevant_docs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'query': self.queries[idx],\n",
    "            'relevant': self.relevant_docs[idx],\n",
    "            'irrelevant': self.irrelevant_docs[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestingDataset = QueryDocsDataset(test_triplets['query'], test_triplets['relevant'], test_triplets['irrelevant'])\n",
    "TestingDataloader = DataLoader(TestingDataset, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(QueryRNNCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.weight_ih = nn.Parameter(torch.randn(input_size, hidden_size))  # Input to hidden weights\n",
    "        self.weight_hh = nn.Parameter(torch.randn(hidden_size, hidden_size))  # Hidden to hidden weights\n",
    "        self.bias_ih = nn.Parameter(torch.randn(input_size))  # Bias\n",
    "        self.bias_hh = nn.Parameter(torch.randn(hidden_size))  # Bias\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        return torch.tanh(\n",
    "            torch.mm(input, self.weight_ih) + self.bias_ih + torch.mm(hidden, self.weight_hh) + self.bias_hh\n",
    "        )\n",
    "    \n",
    "class QueryRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(QueryRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_cell = QueryRNNCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Assuming input is of shape (batch, seq_len, input_size)\n",
    "        batch_size, seq_len, _ = input.shape\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size)  # Initial hidden state\n",
    "        for i in range(seq_len):\n",
    "            hidden = self.rnn_cell(input[:, i, :], hidden)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Towers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        self.queryEncoder = QueryRNN(embedding_size, hidden_size)\n",
    "        self.docEncoder = QueryRNN(embedding_size, hidden_size)\n",
    "\n",
    "    def forward(self, query, relevant, irrelevant):\n",
    "        query_embedding = self.queryEncoder(query)\n",
    "        relevant_embedding = self.docEncoder(relevant)\n",
    "        irrelevant_embedding = self.docEncoder(irrelevant)\n",
    "        return query_embedding, relevant_embedding, irrelevant_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lose Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def distance_function(query, relevant_document):\n",
    "#     return 0\n",
    "\n",
    "# def triplet_loss_function(query, relevant_document, irrelevant_document, distance_function, margin):\n",
    "#     relevant_distance = distance_function(query, relevant_document)\n",
    "#     irrelevant_distance = distance_function(query, irrelevant_document)\n",
    "#     tripletLoss = max(0, relevant_distance - irrelevant_distance + margin)\n",
    "#     return tripletLoss\n",
    "\n",
    "def triplet_loss_function(query, relevant_doc, irrelevant_doc, margin):\n",
    "    # Assuming Euclidean distance as the distance function\n",
    "    relevant_distance = torch.norm(query - relevant_doc, p=2, dim=1)\n",
    "    irrelevant_distance = torch.norm(query - irrelevant_doc, p=2, dim=1)\n",
    "    return torch.clamp(relevant_distance - irrelevant_distance + margin, min=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 256\n",
    "batch_size = 1000\n",
    "\n",
    "# Instantiate the dataset and dataloader\n",
    "# dataset = QueryDocumentDataset(...)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate the model\n",
    "Towers = TwoTowerModel(embedding_size, hidden_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(Towers.parameters(), lr=0.001)\n",
    "\n",
    "margin = 1.0\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    Towers.train() \n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in TestingDataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model to get embeddings\n",
    "        query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings = Towers(\n",
    "            batch['query'], \n",
    "            batch['relevant'], \n",
    "            batch['irrelevant']\n",
    "        )\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = triplet_loss_function(query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings, margin)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Testing phase\n",
    "    Towers.eval()  # Set model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():  # No need to track gradients for testing\n",
    "        for batch in TestingDataloader:\n",
    "            query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings = Towers(\n",
    "                batch['query'], batch['relevant_doc'], batch['irrelevant_doc']\n",
    "            )\n",
    "            \n",
    "            loss = triplet_loss_function(query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings, margin)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "    avg_train_loss = train_loss / len(TestingDataloader)\n",
    "    avg_test_loss = test_loss / len(TestingDataloader)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
