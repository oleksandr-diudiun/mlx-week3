{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet('train.parquet')\n",
    "test = pd.read_parquet('test.parquet')\n",
    "validate = pd.read_parquet('validate.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect all texts to one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('AllTexts.txt', 'w') as f:\n",
    "#     pass  # This just creates the file, immediately closing it\n",
    "\n",
    "# with open('AllTexts.txt', 'a') as f:  # Open file in append mode\n",
    "#     for _, row in train.iterrows():\n",
    "#         concatenated = '\\n'.join(row['passages']['passage_text'])\n",
    "#         concatenated = '\\n'.join([concatenated, '\\n'.join(row['answers'])])\n",
    "#         concatenated = '\\n'.join([concatenated, row['query']])\n",
    "#         f.write(concatenated.lower() + '\\n')\n",
    "#     for _, row in test.iterrows():\n",
    "#         concatenated = '\\n'.join(row['passages']['passage_text'])\n",
    "#         concatenated = '\\n'.join([concatenated, '\\n'.join(row['answers'])])\n",
    "#         concatenated = '\\n'.join([concatenated, row['query']])\n",
    "#         f.write(concatenated.lower() + '\\n')\n",
    "#     for _, row in validate.iterrows():\n",
    "#         concatenated = '\\n'.join(row['passages']['passage_text'])\n",
    "#         concatenated = '\\n'.join([concatenated, '\\n'.join(row['answers'])])\n",
    "#         concatenated = '\\n'.join([concatenated, row['query']])\n",
    "#         f.write(concatenated.lower() + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train tokanizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spm.SentencePieceTrainer.train(\n",
    "#     input = 'AllTexts.txt',\n",
    "#     model_prefix='spm_AllTexts', \n",
    "#     vocab_size=30000,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokanizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('spm_AllTexts.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_file(file_path, sp_processor):\n",
    "    tokenized_sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Assuming each line in the file is a separate sentence or paragraph\n",
    "            # Tokenize the line and add the list of tokens to the tokenized_sentences list\n",
    "            tokenized_sentences.append(sp_processor.encode_as_pieces(line.strip()))\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokinize all the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokinized_sentences = tokenize_file(\"AllTexts.txt\", sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export tokens to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"Tokens_AllText.json\", 'w', encoding='utf-8') as file:\n",
    "#     json.dump(tokinized_sentences, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    min_count  =20,\n",
    "    window     =10,\n",
    "    vector_size=vector_size,\n",
    "    sample     =6e-5, \n",
    "    alpha      = 0.03, \n",
    "    min_alpha  = 0.0007, \n",
    "    negative   = 20,\n",
    "    workers    = multiprocessing.cpu_count() - 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(tokinized_sentences))\n",
    "# w2v_model.build_vocab(tokinized_sentences)\n",
    "# w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"word2vec_vocab.txt\", 'w') as vocab_file:\n",
    "#     for word in w2v_model.wv.key_to_index.keys():\n",
    "#         vocab_file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model.train(tokinized_sentences, total_examples=w2v_model.corpus_count, epochs=20, report_delay=1)\n",
    "# w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('▁cyber', 0.5855585336685181), ('▁hack', 0.5675662159919739), ('▁malicious', 0.5665103197097778), ('▁malware', 0.5581293106079102)]\n",
      "[('▁cyber', 0.5855585336685181), ('▁hack', 0.5675662159919739), ('▁malicious', 0.5665103197097778), ('▁malware', 0.5581293106079102), ('▁scam', 0.5384910702705383), ('▁spyware', 0.5197509527206421), ('▁legitimate', 0.5031879544258118), ('▁adware', 0.4846465289592743), ('▁pretend', 0.469952255487442), ('▁insider', 0.46461066603660583)]\n"
     ]
    }
   ],
   "source": [
    "similar_words = w2v_model.wv.most_similar('▁hacker', topn=4)\n",
    "print(similar_words)\n",
    "print(w2v_model.wv.most_similar(sp.encode_as_pieces('Hacker'.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_w2v_embedding(sp, text):\n",
    "    tokens = sp.encode_as_pieces(text.lower())\n",
    "\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if (token in w2v_model.wv): \n",
    "            embeddings.append(w2v_model.wv[token])\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triples for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTriplesTokens(dataframe):\n",
    "    triples = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        available_indices = list(dataframe.index)\n",
    "        available_indices.remove(index)\n",
    "        \n",
    "        for relevant in row['passages']['passage_text']:\n",
    "            random_index = np.random.choice(available_indices)\n",
    "            random_doc_index = np.random.choice(\n",
    "                list(\n",
    "                    range(\n",
    "                        len(dataframe.iloc[random_index]['passages']['passage_text'])\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "            irrelevant = dataframe.iloc[random_index]['passages']['passage_text'][random_doc_index]\n",
    "\n",
    "            triples.append([\n",
    "                row['query'],\n",
    "                relevant,\n",
    "                irrelevant,\n",
    "            ])\n",
    "\n",
    "    return triples\n",
    "\n",
    "# train_triplets = prepareTriplesTokens(train)\n",
    "# test_triplets = prepareTriplesTokens(test)\n",
    "# validate_triplets = prepareTriplesTokens(validate)\n",
    "\n",
    "train_triplets = pd.read_parquet('train_triplets.parquet')\n",
    "test_triplets = pd.read_parquet('test_triplets.parquet')\n",
    "validate_triplets = pd.read_parquet('validate_triplets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         query                                           relevant  \\\n",
      "0  what is rba  Since 2007, the RBA's outstanding reputation h...   \n",
      "1  what is rba  The Reserve Bank of Australia (RBA) came into ...   \n",
      "\n",
      "                                          irrelevant  \n",
      "0  This report describes the typical weather at t...  \n",
      "1  1. district, community the vicar of a small pa...  \n"
     ]
    }
   ],
   "source": [
    "print(train_triplets[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the list of triples to a DataFrame\n",
    "# columns = ['query', 'relevant', 'irrelevant']\n",
    "# train_triplets = pd.DataFrame(train_triplets, columns=columns)\n",
    "# test_triplets = pd.DataFrame(test_triplets, columns=columns)\n",
    "# validate_triplets = pd.DataFrame(validate_triplets, columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_triplets_embeddings = pd.DataFrame()\n",
    "\n",
    "# test_triplets_embeddings['query_embeddings'] = test_triplets['query'].apply(lambda x: to_w2v_embedding(sp, x))\n",
    "# test_triplets_embeddings['relevant_embeddings'] = test_triplets['relevant'].apply(lambda x: to_w2v_embedding(sp, x))\n",
    "# test_triplets_embeddings['irrelevant_embeddings'] = test_triplets['irrelevant'].apply(lambda x: to_w2v_embedding(sp, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total DataFrame size: 134.55 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the memory usage of each column in bytes, then sum them up, and convert to megabytes\n",
    "# total_memory_mb = test_triplets_embeddings.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "\n",
    "# print(f'Total DataFrame size: {total_memory_mb:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_triplets_embeddings = pd.DataFrame()\n",
    "# train_triplets_embeddings['query_embeddings'] = train_triplets['query'].apply(lambda x: to_w2v_embedding(sp, x))\n",
    "# train_triplets_embeddings['relevant_embeddings'] = train_triplets['relevant'].apply(lambda x: to_w2v_embedding(sp, x))\n",
    "# train_triplets_embeddings['irrelevant_embeddings'] = train_triplets['irrelevant'].apply(lambda x: to_w2v_embedding(sp, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total DataFrame size: 1150.24 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the memory usage of each column in bytes, then sum them up, and convert to megabytes\n",
    "# total_memory_mb = train_triplets_embeddings.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "\n",
    "# print(f'Total DataFrame size: {total_memory_mb:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [[-0.38117662, -1.751613, 0.118526, -4.4650145...\n",
      "Name: query_embeddings, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print(test_triplets_embeddings['query_embeddings'].head(1))\n",
    "\n",
    "# train_triplets_embeddings.to_parquet('train_triplets_with_embedings.parquet', engine='pyarrow') \n",
    "# test_triplets_embeddings.to_parquet('test_triplets_with_embedings.parquet', engine='pyarrow') \n",
    "# validate_triplets.to_parquet('validate_triplets_with_embedings.parquet', engine='pyarrow') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryDocsDataset(Dataset):\n",
    "    def __init__(self, sp, queries, relevant_docs, irrelevant_docs):\n",
    "        self.queries = queries\n",
    "        self.relevant_docs = relevant_docs\n",
    "        self.irrelevant_docs = irrelevant_docs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'query': torch.tensor(to_w2v_embedding(sp, self.queries[idx].lower()), dtype=torch.float),\n",
    "            'relevant': torch.tensor(to_w2v_embedding(sp, self.relevant_docs[idx].lower()), dtype=torch.float),\n",
    "            'irrelevant': torch.tensor(to_w2v_embedding(sp, self.irrelevant_docs[idx].lower()), dtype=torch.float),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom DataLoader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataLoader:\n",
    "#     def __init__(self, dataset, batch_size, shuffle=False):\n",
    "#         self.dataset = dataset\n",
    "#         self.batch_size = batch_size\n",
    "#         self.shuffle = shuffle\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         self.idx = 0\n",
    "#         if self.shuffle:\n",
    "#             np.random.shuffle(self.dataset)\n",
    "#         return self\n",
    "\n",
    "#     def __next__(self):\n",
    "#         if self.idx < len(self.dataset):\n",
    "#             batch = self.dataset[self.idx:self.idx + self.batch_size]\n",
    "#             self.idx += self.batch_size\n",
    "#             return batch\n",
    "#         else:\n",
    "#             raise StopIteration\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset) // self.batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingDataset = QueryDocsDataset(sp, train_triplets['query'], train_triplets['relevant'], train_triplets['irrelevant'])\n",
    "TestingDataset = QueryDocsDataset(sp, test_triplets['query'], test_triplets['relevant'], test_triplets['irrelevant'])\n",
    "ValidationDataset = QueryDocsDataset(sp, validate_triplets['query'], validate_triplets['relevant'], validate_triplets['irrelevant'])\n",
    "\n",
    "# TrainingDataset = QueryDocsDataset(sp, train_triplets_embeddings['query_embeddings'], train_triplets_embeddings['relevant_embeddings'], train_triplets_embeddings['irrelevant_embeddings'])\n",
    "# TestingDataset = QueryDocsDataset(sp, test_triplets_embeddings['query_embeddings'], test_triplets_embeddings['relevant_embeddings'], test_triplets_embeddings['irrelevant_embeddings'])\n",
    "# ValidationDataset = QueryDocsDataset(sp, validate_triplets_embeddings['query'], validate_triplets_embeddings['relevant'], validate_triplets_embeddings['irrelevant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(QueryRNNCell, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.weight_ih = nn.Parameter(torch.randn(input_size, hidden_size))  # Input to hidden weights\n",
    "        self.weight_hh = nn.Parameter(torch.randn(hidden_size, hidden_size))  # Hidden to hidden weights\n",
    "        \n",
    "        self.bias_hh = nn.Parameter(torch.randn(hidden_size))  # Bias\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        return torch.tanh(\n",
    "            torch.mm(input, self.weight_ih) + torch.mm(hidden, self.weight_hh) + self.bias_hh\n",
    "        )\n",
    "    \n",
    "class QueryRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(QueryRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_cell = QueryRNNCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Assuming input is of shape (batch, seq_len, input_size)\n",
    "        batch_size, seq_len, _ = input.shape\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size)  # Initial hidden state\n",
    "        for i in range(seq_len):\n",
    "            \n",
    "            mask = torch.any(input[:, i, :] != 0, dim=1).float().unsqueeze(1)  # Shape: (batch_size, 1)\n",
    "            current_input = input[:, i, :]  \n",
    "            \n",
    "            current_hidden = self.rnn_cell(current_input, hidden)\n",
    "            \n",
    "            # Apply mask: Only update hidden state for non-padded inputs\n",
    "            hidden = mask * current_hidden + (1 - mask) * hidden\n",
    "            \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Towers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        self.queryEncoder = QueryRNN(embedding_size, hidden_size)\n",
    "        self.docEncoder = QueryRNN(embedding_size, hidden_size)\n",
    "\n",
    "    def forward(self, query, relevant, irrelevant):\n",
    "        query_embedding = self.queryEncoder(query)\n",
    "        relevant_embedding = self.docEncoder(relevant)\n",
    "        irrelevant_embedding = self.docEncoder(irrelevant)\n",
    "        return query_embedding, relevant_embedding, irrelevant_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lose Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def triplet_loss_function_cosine(query, relevant_doc, irrelevant_doc, margin):\n",
    "    # Compute cosine similarity (the output ranges from -1 to 1)\n",
    "    relevant_similarity = F.cosine_similarity(query, relevant_doc)\n",
    "    irrelevant_similarity = F.cosine_similarity(query, irrelevant_doc)\n",
    "    \n",
    "    # Convert similarities to distances (ranges from 0 to 2)\n",
    "    relevant_distance = 1 - relevant_similarity\n",
    "    irrelevant_distance = 1 - irrelevant_similarity\n",
    "    \n",
    "    # Compute the triplet loss\n",
    "    triplet_loss = torch.clamp(margin + relevant_distance - irrelevant_distance, min=0)\n",
    "    return triplet_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Extract lists of tensors for 'query', 'relevant', and 'irrelevant' from the batch\n",
    "    query_tensors = [item['query'] for item in batch]\n",
    "    relevant_tensors = [item['relevant'] for item in batch]\n",
    "    irrelevant_tensors = [item['irrelevant'] for item in batch]\n",
    "    \n",
    "    # Pad sequences within each list to the same length\n",
    "    query_padded = pad_sequence(query_tensors, batch_first=True, padding_value=0)\n",
    "    relevant_padded = pad_sequence(relevant_tensors, batch_first=True, padding_value=0)\n",
    "    irrelevant_padded = pad_sequence(irrelevant_tensors, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Return a dictionary with padded sequences\n",
    "    return {\n",
    "        'query': query_padded,\n",
    "        'relevant': relevant_padded,\n",
    "        'irrelevant': irrelevant_padded\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## padding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[51, 92, 14, 71],\n",
      "         [60, 20, 82, 86],\n",
      "         [74, 74, 87, 99],\n",
      "         [23,  2, 21, 52],\n",
      "         [ 1, 87, 29, 37],\n",
      "         [ 0,  0,  0,  0]],\n",
      "\n",
      "        [[51, 92, 14, 71],\n",
      "         [60, 20, 82, 86],\n",
      "         [74, 74, 87, 99],\n",
      "         [23,  2, 21, 52],\n",
      "         [ 1, 87, 29, 37],\n",
      "         [60, 20, 82, 86]]])\n"
     ]
    }
   ],
   "source": [
    "# Convert the numpy array to a PyTorch tensor\n",
    "tensors =[ \n",
    "    torch.tensor([\n",
    "        [51, 92, 14, 71],\n",
    "        [60, 20, 82, 86],\n",
    "        [74, 74, 87, 99],\n",
    "        [23,  2, 21, 52],\n",
    "        [ 1, 87, 29, 37],\n",
    "    ]),\n",
    "    torch.tensor([\n",
    "        [51, 92, 14, 71],\n",
    "        [60, 20, 82, 86],\n",
    "        [74, 74, 87, 99],\n",
    "        [23,  2, 21, 52],\n",
    "        [ 1, 87, 29, 37],\n",
    "        [60, 20, 82, 86],\n",
    "    ]),\n",
    "]\n",
    "dump = pad_sequence(tensors, batch_first=True)\n",
    "\n",
    "print(dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 16\n",
    "margin = 1.0\n",
    "batch_size = 100\n",
    "num_epochs = 3\n",
    "\n",
    "# TrainingDataloader = DataLoader(TrainingDataset, batch_size, shuffle=False)\n",
    "# TestingDataloader = DataLoader(TestingDataset, batch_size, shuffle=False)\n",
    "\n",
    "TrainingDataloader = DataLoader(TrainingDataset, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "TestingDataloader = DataLoader(TestingDataset, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "ValidatingDataloader = DataLoader(ValidationDataset, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Instantiate the model\n",
    "Towers = TwoTowerModel(embedding_size, hidden_size)\n",
    "optimizer = torch.optim.Adam(Towers.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recheck the padding results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 7, 128])\n",
      "torch.Size([20, 134, 128])\n",
      "torch.Size([20, 154, 128])\n",
      "Element 1, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 2, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 3, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 4, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 5, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 6, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 7, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 8, last row of 7:\n",
      "[-0.8882818  -3.348267    0.36098903  0.46327683  1.2810141  -1.3186098\n",
      " -1.9863871   2.6532955   2.0709038  -0.44499442  1.1410644  -1.9977982\n",
      "  3.0292907   3.7966125  -3.3948023  -1.1889541  -3.0714736   2.3496273\n",
      " -2.6288993   3.7311935  -4.277793    3.467017    3.2347875   2.0128965\n",
      " -4.0168424  -3.3952324   0.6621953   3.2866776  -1.4068488  -0.37303498\n",
      " -1.969379   -0.02366243  2.6913583   1.9048185   1.4900229   3.6248085\n",
      "  5.1554775   4.3432117   1.122215    5.5826354   1.2623793   0.5097447\n",
      " -0.7163874   1.9075923  -0.7156864  -0.598586    4.7436876  -0.6780336\n",
      "  0.50797147 -2.3601375  -1.411537    5.2797384  -2.2775202   2.7040942\n",
      " -0.9327177   5.1660175   0.5298864  -2.871028    6.6004486   0.06043774\n",
      " -1.0087829   2.314619   -8.490975    0.65564185  0.08034915 -1.8744208\n",
      " -1.340661    3.9049559   2.6682553  -2.9101653   3.3522274   7.4489713\n",
      " -2.4132447   0.9705874   1.3886019   0.599941   -0.7818905   3.4701912\n",
      "  4.4904246  -3.8627303   5.118557   -1.2698896   2.9748638   0.08187379\n",
      "  2.0651286  -2.3976824   3.0384939   2.755078   -4.7522836   0.79728156\n",
      " -4.785485   -3.3824747  -3.2314544   5.399737   -4.068802   -1.3460119\n",
      " -1.3903389   3.4053926   1.259242    2.3484674   0.571316    3.3621442\n",
      " -3.4612641   0.74964064  5.21113     0.91659003  2.1706917  -2.8290615\n",
      "  2.4676163   1.6404735   2.9110887   0.01441479 -2.4854708  -1.136586\n",
      "  2.778274   -1.2347221   3.8130352  -2.3884017  -1.3023429   0.5237943\n",
      "  0.8718899  -1.2806287   6.1741323   3.5710938  -0.77821237 -2.396023\n",
      " -1.2373894   1.2006588 ]\n",
      "\n",
      "Element 9, last row of 7:\n",
      "[-0.8882818  -3.348267    0.36098903  0.46327683  1.2810141  -1.3186098\n",
      " -1.9863871   2.6532955   2.0709038  -0.44499442  1.1410644  -1.9977982\n",
      "  3.0292907   3.7966125  -3.3948023  -1.1889541  -3.0714736   2.3496273\n",
      " -2.6288993   3.7311935  -4.277793    3.467017    3.2347875   2.0128965\n",
      " -4.0168424  -3.3952324   0.6621953   3.2866776  -1.4068488  -0.37303498\n",
      " -1.969379   -0.02366243  2.6913583   1.9048185   1.4900229   3.6248085\n",
      "  5.1554775   4.3432117   1.122215    5.5826354   1.2623793   0.5097447\n",
      " -0.7163874   1.9075923  -0.7156864  -0.598586    4.7436876  -0.6780336\n",
      "  0.50797147 -2.3601375  -1.411537    5.2797384  -2.2775202   2.7040942\n",
      " -0.9327177   5.1660175   0.5298864  -2.871028    6.6004486   0.06043774\n",
      " -1.0087829   2.314619   -8.490975    0.65564185  0.08034915 -1.8744208\n",
      " -1.340661    3.9049559   2.6682553  -2.9101653   3.3522274   7.4489713\n",
      " -2.4132447   0.9705874   1.3886019   0.599941   -0.7818905   3.4701912\n",
      "  4.4904246  -3.8627303   5.118557   -1.2698896   2.9748638   0.08187379\n",
      "  2.0651286  -2.3976824   3.0384939   2.755078   -4.7522836   0.79728156\n",
      " -4.785485   -3.3824747  -3.2314544   5.399737   -4.068802   -1.3460119\n",
      " -1.3903389   3.4053926   1.259242    2.3484674   0.571316    3.3621442\n",
      " -3.4612641   0.74964064  5.21113     0.91659003  2.1706917  -2.8290615\n",
      "  2.4676163   1.6404735   2.9110887   0.01441479 -2.4854708  -1.136586\n",
      "  2.778274   -1.2347221   3.8130352  -2.3884017  -1.3023429   0.5237943\n",
      "  0.8718899  -1.2806287   6.1741323   3.5710938  -0.77821237 -2.396023\n",
      " -1.2373894   1.2006588 ]\n",
      "\n",
      "Element 10, last row of 7:\n",
      "[-0.8882818  -3.348267    0.36098903  0.46327683  1.2810141  -1.3186098\n",
      " -1.9863871   2.6532955   2.0709038  -0.44499442  1.1410644  -1.9977982\n",
      "  3.0292907   3.7966125  -3.3948023  -1.1889541  -3.0714736   2.3496273\n",
      " -2.6288993   3.7311935  -4.277793    3.467017    3.2347875   2.0128965\n",
      " -4.0168424  -3.3952324   0.6621953   3.2866776  -1.4068488  -0.37303498\n",
      " -1.969379   -0.02366243  2.6913583   1.9048185   1.4900229   3.6248085\n",
      "  5.1554775   4.3432117   1.122215    5.5826354   1.2623793   0.5097447\n",
      " -0.7163874   1.9075923  -0.7156864  -0.598586    4.7436876  -0.6780336\n",
      "  0.50797147 -2.3601375  -1.411537    5.2797384  -2.2775202   2.7040942\n",
      " -0.9327177   5.1660175   0.5298864  -2.871028    6.6004486   0.06043774\n",
      " -1.0087829   2.314619   -8.490975    0.65564185  0.08034915 -1.8744208\n",
      " -1.340661    3.9049559   2.6682553  -2.9101653   3.3522274   7.4489713\n",
      " -2.4132447   0.9705874   1.3886019   0.599941   -0.7818905   3.4701912\n",
      "  4.4904246  -3.8627303   5.118557   -1.2698896   2.9748638   0.08187379\n",
      "  2.0651286  -2.3976824   3.0384939   2.755078   -4.7522836   0.79728156\n",
      " -4.785485   -3.3824747  -3.2314544   5.399737   -4.068802   -1.3460119\n",
      " -1.3903389   3.4053926   1.259242    2.3484674   0.571316    3.3621442\n",
      " -3.4612641   0.74964064  5.21113     0.91659003  2.1706917  -2.8290615\n",
      "  2.4676163   1.6404735   2.9110887   0.01441479 -2.4854708  -1.136586\n",
      "  2.778274   -1.2347221   3.8130352  -2.3884017  -1.3023429   0.5237943\n",
      "  0.8718899  -1.2806287   6.1741323   3.5710938  -0.77821237 -2.396023\n",
      " -1.2373894   1.2006588 ]\n",
      "\n",
      "Element 11, last row of 7:\n",
      "[-0.8882818  -3.348267    0.36098903  0.46327683  1.2810141  -1.3186098\n",
      " -1.9863871   2.6532955   2.0709038  -0.44499442  1.1410644  -1.9977982\n",
      "  3.0292907   3.7966125  -3.3948023  -1.1889541  -3.0714736   2.3496273\n",
      " -2.6288993   3.7311935  -4.277793    3.467017    3.2347875   2.0128965\n",
      " -4.0168424  -3.3952324   0.6621953   3.2866776  -1.4068488  -0.37303498\n",
      " -1.969379   -0.02366243  2.6913583   1.9048185   1.4900229   3.6248085\n",
      "  5.1554775   4.3432117   1.122215    5.5826354   1.2623793   0.5097447\n",
      " -0.7163874   1.9075923  -0.7156864  -0.598586    4.7436876  -0.6780336\n",
      "  0.50797147 -2.3601375  -1.411537    5.2797384  -2.2775202   2.7040942\n",
      " -0.9327177   5.1660175   0.5298864  -2.871028    6.6004486   0.06043774\n",
      " -1.0087829   2.314619   -8.490975    0.65564185  0.08034915 -1.8744208\n",
      " -1.340661    3.9049559   2.6682553  -2.9101653   3.3522274   7.4489713\n",
      " -2.4132447   0.9705874   1.3886019   0.599941   -0.7818905   3.4701912\n",
      "  4.4904246  -3.8627303   5.118557   -1.2698896   2.9748638   0.08187379\n",
      "  2.0651286  -2.3976824   3.0384939   2.755078   -4.7522836   0.79728156\n",
      " -4.785485   -3.3824747  -3.2314544   5.399737   -4.068802   -1.3460119\n",
      " -1.3903389   3.4053926   1.259242    2.3484674   0.571316    3.3621442\n",
      " -3.4612641   0.74964064  5.21113     0.91659003  2.1706917  -2.8290615\n",
      "  2.4676163   1.6404735   2.9110887   0.01441479 -2.4854708  -1.136586\n",
      "  2.778274   -1.2347221   3.8130352  -2.3884017  -1.3023429   0.5237943\n",
      "  0.8718899  -1.2806287   6.1741323   3.5710938  -0.77821237 -2.396023\n",
      " -1.2373894   1.2006588 ]\n",
      "\n",
      "Element 12, last row of 7:\n",
      "[-0.8882818  -3.348267    0.36098903  0.46327683  1.2810141  -1.3186098\n",
      " -1.9863871   2.6532955   2.0709038  -0.44499442  1.1410644  -1.9977982\n",
      "  3.0292907   3.7966125  -3.3948023  -1.1889541  -3.0714736   2.3496273\n",
      " -2.6288993   3.7311935  -4.277793    3.467017    3.2347875   2.0128965\n",
      " -4.0168424  -3.3952324   0.6621953   3.2866776  -1.4068488  -0.37303498\n",
      " -1.969379   -0.02366243  2.6913583   1.9048185   1.4900229   3.6248085\n",
      "  5.1554775   4.3432117   1.122215    5.5826354   1.2623793   0.5097447\n",
      " -0.7163874   1.9075923  -0.7156864  -0.598586    4.7436876  -0.6780336\n",
      "  0.50797147 -2.3601375  -1.411537    5.2797384  -2.2775202   2.7040942\n",
      " -0.9327177   5.1660175   0.5298864  -2.871028    6.6004486   0.06043774\n",
      " -1.0087829   2.314619   -8.490975    0.65564185  0.08034915 -1.8744208\n",
      " -1.340661    3.9049559   2.6682553  -2.9101653   3.3522274   7.4489713\n",
      " -2.4132447   0.9705874   1.3886019   0.599941   -0.7818905   3.4701912\n",
      "  4.4904246  -3.8627303   5.118557   -1.2698896   2.9748638   0.08187379\n",
      "  2.0651286  -2.3976824   3.0384939   2.755078   -4.7522836   0.79728156\n",
      " -4.785485   -3.3824747  -3.2314544   5.399737   -4.068802   -1.3460119\n",
      " -1.3903389   3.4053926   1.259242    2.3484674   0.571316    3.3621442\n",
      " -3.4612641   0.74964064  5.21113     0.91659003  2.1706917  -2.8290615\n",
      "  2.4676163   1.6404735   2.9110887   0.01441479 -2.4854708  -1.136586\n",
      "  2.778274   -1.2347221   3.8130352  -2.3884017  -1.3023429   0.5237943\n",
      "  0.8718899  -1.2806287   6.1741323   3.5710938  -0.77821237 -2.396023\n",
      " -1.2373894   1.2006588 ]\n",
      "\n",
      "Element 13, last row of 7:\n",
      "[-0.8882818  -3.348267    0.36098903  0.46327683  1.2810141  -1.3186098\n",
      " -1.9863871   2.6532955   2.0709038  -0.44499442  1.1410644  -1.9977982\n",
      "  3.0292907   3.7966125  -3.3948023  -1.1889541  -3.0714736   2.3496273\n",
      " -2.6288993   3.7311935  -4.277793    3.467017    3.2347875   2.0128965\n",
      " -4.0168424  -3.3952324   0.6621953   3.2866776  -1.4068488  -0.37303498\n",
      " -1.969379   -0.02366243  2.6913583   1.9048185   1.4900229   3.6248085\n",
      "  5.1554775   4.3432117   1.122215    5.5826354   1.2623793   0.5097447\n",
      " -0.7163874   1.9075923  -0.7156864  -0.598586    4.7436876  -0.6780336\n",
      "  0.50797147 -2.3601375  -1.411537    5.2797384  -2.2775202   2.7040942\n",
      " -0.9327177   5.1660175   0.5298864  -2.871028    6.6004486   0.06043774\n",
      " -1.0087829   2.314619   -8.490975    0.65564185  0.08034915 -1.8744208\n",
      " -1.340661    3.9049559   2.6682553  -2.9101653   3.3522274   7.4489713\n",
      " -2.4132447   0.9705874   1.3886019   0.599941   -0.7818905   3.4701912\n",
      "  4.4904246  -3.8627303   5.118557   -1.2698896   2.9748638   0.08187379\n",
      "  2.0651286  -2.3976824   3.0384939   2.755078   -4.7522836   0.79728156\n",
      " -4.785485   -3.3824747  -3.2314544   5.399737   -4.068802   -1.3460119\n",
      " -1.3903389   3.4053926   1.259242    2.3484674   0.571316    3.3621442\n",
      " -3.4612641   0.74964064  5.21113     0.91659003  2.1706917  -2.8290615\n",
      "  2.4676163   1.6404735   2.9110887   0.01441479 -2.4854708  -1.136586\n",
      "  2.778274   -1.2347221   3.8130352  -2.3884017  -1.3023429   0.5237943\n",
      "  0.8718899  -1.2806287   6.1741323   3.5710938  -0.77821237 -2.396023\n",
      " -1.2373894   1.2006588 ]\n",
      "\n",
      "Element 14, last row of 7:\n",
      "[-0.8882818  -3.348267    0.36098903  0.46327683  1.2810141  -1.3186098\n",
      " -1.9863871   2.6532955   2.0709038  -0.44499442  1.1410644  -1.9977982\n",
      "  3.0292907   3.7966125  -3.3948023  -1.1889541  -3.0714736   2.3496273\n",
      " -2.6288993   3.7311935  -4.277793    3.467017    3.2347875   2.0128965\n",
      " -4.0168424  -3.3952324   0.6621953   3.2866776  -1.4068488  -0.37303498\n",
      " -1.969379   -0.02366243  2.6913583   1.9048185   1.4900229   3.6248085\n",
      "  5.1554775   4.3432117   1.122215    5.5826354   1.2623793   0.5097447\n",
      " -0.7163874   1.9075923  -0.7156864  -0.598586    4.7436876  -0.6780336\n",
      "  0.50797147 -2.3601375  -1.411537    5.2797384  -2.2775202   2.7040942\n",
      " -0.9327177   5.1660175   0.5298864  -2.871028    6.6004486   0.06043774\n",
      " -1.0087829   2.314619   -8.490975    0.65564185  0.08034915 -1.8744208\n",
      " -1.340661    3.9049559   2.6682553  -2.9101653   3.3522274   7.4489713\n",
      " -2.4132447   0.9705874   1.3886019   0.599941   -0.7818905   3.4701912\n",
      "  4.4904246  -3.8627303   5.118557   -1.2698896   2.9748638   0.08187379\n",
      "  2.0651286  -2.3976824   3.0384939   2.755078   -4.7522836   0.79728156\n",
      " -4.785485   -3.3824747  -3.2314544   5.399737   -4.068802   -1.3460119\n",
      " -1.3903389   3.4053926   1.259242    2.3484674   0.571316    3.3621442\n",
      " -3.4612641   0.74964064  5.21113     0.91659003  2.1706917  -2.8290615\n",
      "  2.4676163   1.6404735   2.9110887   0.01441479 -2.4854708  -1.136586\n",
      "  2.778274   -1.2347221   3.8130352  -2.3884017  -1.3023429   0.5237943\n",
      "  0.8718899  -1.2806287   6.1741323   3.5710938  -0.77821237 -2.396023\n",
      " -1.2373894   1.2006588 ]\n",
      "\n",
      "Element 15, last row of 7:\n",
      "[-0.8882818  -3.348267    0.36098903  0.46327683  1.2810141  -1.3186098\n",
      " -1.9863871   2.6532955   2.0709038  -0.44499442  1.1410644  -1.9977982\n",
      "  3.0292907   3.7966125  -3.3948023  -1.1889541  -3.0714736   2.3496273\n",
      " -2.6288993   3.7311935  -4.277793    3.467017    3.2347875   2.0128965\n",
      " -4.0168424  -3.3952324   0.6621953   3.2866776  -1.4068488  -0.37303498\n",
      " -1.969379   -0.02366243  2.6913583   1.9048185   1.4900229   3.6248085\n",
      "  5.1554775   4.3432117   1.122215    5.5826354   1.2623793   0.5097447\n",
      " -0.7163874   1.9075923  -0.7156864  -0.598586    4.7436876  -0.6780336\n",
      "  0.50797147 -2.3601375  -1.411537    5.2797384  -2.2775202   2.7040942\n",
      " -0.9327177   5.1660175   0.5298864  -2.871028    6.6004486   0.06043774\n",
      " -1.0087829   2.314619   -8.490975    0.65564185  0.08034915 -1.8744208\n",
      " -1.340661    3.9049559   2.6682553  -2.9101653   3.3522274   7.4489713\n",
      " -2.4132447   0.9705874   1.3886019   0.599941   -0.7818905   3.4701912\n",
      "  4.4904246  -3.8627303   5.118557   -1.2698896   2.9748638   0.08187379\n",
      "  2.0651286  -2.3976824   3.0384939   2.755078   -4.7522836   0.79728156\n",
      " -4.785485   -3.3824747  -3.2314544   5.399737   -4.068802   -1.3460119\n",
      " -1.3903389   3.4053926   1.259242    2.3484674   0.571316    3.3621442\n",
      " -3.4612641   0.74964064  5.21113     0.91659003  2.1706917  -2.8290615\n",
      "  2.4676163   1.6404735   2.9110887   0.01441479 -2.4854708  -1.136586\n",
      "  2.778274   -1.2347221   3.8130352  -2.3884017  -1.3023429   0.5237943\n",
      "  0.8718899  -1.2806287   6.1741323   3.5710938  -0.77821237 -2.396023\n",
      " -1.2373894   1.2006588 ]\n",
      "\n",
      "Element 16, last row of 7:\n",
      "[-0.8882818  -3.348267    0.36098903  0.46327683  1.2810141  -1.3186098\n",
      " -1.9863871   2.6532955   2.0709038  -0.44499442  1.1410644  -1.9977982\n",
      "  3.0292907   3.7966125  -3.3948023  -1.1889541  -3.0714736   2.3496273\n",
      " -2.6288993   3.7311935  -4.277793    3.467017    3.2347875   2.0128965\n",
      " -4.0168424  -3.3952324   0.6621953   3.2866776  -1.4068488  -0.37303498\n",
      " -1.969379   -0.02366243  2.6913583   1.9048185   1.4900229   3.6248085\n",
      "  5.1554775   4.3432117   1.122215    5.5826354   1.2623793   0.5097447\n",
      " -0.7163874   1.9075923  -0.7156864  -0.598586    4.7436876  -0.6780336\n",
      "  0.50797147 -2.3601375  -1.411537    5.2797384  -2.2775202   2.7040942\n",
      " -0.9327177   5.1660175   0.5298864  -2.871028    6.6004486   0.06043774\n",
      " -1.0087829   2.314619   -8.490975    0.65564185  0.08034915 -1.8744208\n",
      " -1.340661    3.9049559   2.6682553  -2.9101653   3.3522274   7.4489713\n",
      " -2.4132447   0.9705874   1.3886019   0.599941   -0.7818905   3.4701912\n",
      "  4.4904246  -3.8627303   5.118557   -1.2698896   2.9748638   0.08187379\n",
      "  2.0651286  -2.3976824   3.0384939   2.755078   -4.7522836   0.79728156\n",
      " -4.785485   -3.3824747  -3.2314544   5.399737   -4.068802   -1.3460119\n",
      " -1.3903389   3.4053926   1.259242    2.3484674   0.571316    3.3621442\n",
      " -3.4612641   0.74964064  5.21113     0.91659003  2.1706917  -2.8290615\n",
      "  2.4676163   1.6404735   2.9110887   0.01441479 -2.4854708  -1.136586\n",
      "  2.778274   -1.2347221   3.8130352  -2.3884017  -1.3023429   0.5237943\n",
      "  0.8718899  -1.2806287   6.1741323   3.5710938  -0.77821237 -2.396023\n",
      " -1.2373894   1.2006588 ]\n",
      "\n",
      "Element 17, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 18, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 19, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 20, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in TestingDataloader:\n",
    "    print(batch['query'].shape)        # Shape: (batch_size, max_seq_length_query, feature_dim)\n",
    "    print(batch['relevant'].shape)     # Shape: (batch_size, max_seq_length_relevant, feature_dim)\n",
    "    print(batch['irrelevant'].shape) \n",
    "\n",
    "    for i in range(batch['query'].size(0)):\n",
    "        print(f\"Element {i+1}, last row of 7:\\n{batch['query'][i, -1, :].numpy()}\\n\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Batch: 10, Train Loss: 0.9681\n",
      "Epoch [1/1], Batch: 20, Train Loss: 0.9824\n",
      "Epoch [1/1], Batch: 30, Train Loss: 1.0187\n",
      "Epoch [1/1], Batch: 40, Train Loss: 1.0107\n",
      "Epoch [1/1], Batch: 50, Train Loss: 0.9767\n",
      "Epoch [1/1], Batch: 60, Train Loss: 0.9815\n",
      "Epoch [1/1], Batch: 70, Train Loss: 1.0473\n",
      "Epoch [1/1], Batch: 80, Train Loss: 0.9927\n",
      "Epoch [1/1], Batch: 90, Train Loss: 0.9872\n",
      "Epoch [1/1], Batch: 100, Train Loss: 1.0122\n",
      "Epoch [1/1], Batch: 110, Train Loss: 0.9976\n",
      "Epoch [1/1], Batch: 120, Train Loss: 0.9866\n",
      "Epoch [1/1], Batch: 130, Train Loss: 1.0137\n",
      "Epoch [1/1], Batch: 140, Train Loss: 1.0094\n",
      "Epoch [1/1], Batch: 150, Train Loss: 1.0110\n",
      "Epoch [1/1], Batch: 160, Train Loss: 1.0023\n",
      "Epoch [1/1], Batch: 170, Train Loss: 0.9582\n",
      "Epoch [1/1], Batch: 180, Train Loss: 0.9876\n",
      "Epoch [1/1], Batch: 190, Train Loss: 0.9957\n",
      "Epoch [1/1], Batch: 200, Train Loss: 1.0244\n",
      "Epoch [1/1], Batch: 210, Train Loss: 1.0147\n",
      "Epoch [1/1], Batch: 220, Train Loss: 1.0233\n",
      "Epoch [1/1], Batch: 230, Train Loss: 1.0191\n",
      "Epoch [1/1], Batch: 240, Train Loss: 1.0129\n",
      "Epoch [1/1], Batch: 250, Train Loss: 0.9716\n",
      "Epoch [1/1], Batch: 260, Train Loss: 0.9633\n",
      "Epoch [1/1], Batch: 270, Train Loss: 1.0238\n",
      "Epoch [1/1], Batch: 280, Train Loss: 1.0020\n",
      "Epoch [1/1], Batch: 290, Train Loss: 0.9841\n",
      "Epoch [1/1], Batch: 300, Train Loss: 0.9712\n",
      "Epoch [1/1], Batch: 310, Train Loss: 1.0320\n",
      "Epoch [1/1], Batch: 320, Train Loss: 0.9934\n",
      "Epoch [1/1], Batch: 330, Train Loss: 0.9859\n",
      "Epoch [1/1], Batch: 340, Train Loss: 1.0176\n",
      "Epoch [1/1], Batch: 350, Train Loss: 1.0002\n",
      "Epoch [1/1], Batch: 360, Train Loss: 0.9928\n",
      "Epoch [1/1], Batch: 370, Train Loss: 0.9914\n",
      "Epoch [1/1], Batch: 380, Train Loss: 1.0265\n",
      "Epoch [1/1], Batch: 390, Train Loss: 0.9791\n",
      "Epoch [1/1], Batch: 400, Train Loss: 0.9978\n",
      "Epoch [1/1], Batch: 410, Train Loss: 0.9919\n",
      "Epoch [1/1], Batch: 420, Train Loss: 1.0207\n",
      "Epoch [1/1], Batch: 430, Train Loss: 0.9978\n",
      "Epoch [1/1], Batch: 440, Train Loss: 0.9805\n",
      "Epoch [1/1], Batch: 450, Train Loss: 0.9656\n",
      "Epoch [1/1], Batch: 460, Train Loss: 1.0142\n",
      "Epoch [1/1], Batch: 470, Train Loss: 1.0206\n",
      "Epoch [1/1], Batch: 480, Train Loss: 0.9591\n",
      "Epoch [1/1], Batch: 490, Train Loss: 1.0054\n",
      "Epoch [1/1], Batch: 500, Train Loss: 0.9608\n",
      "Epoch [1/1], Batch: 510, Train Loss: 1.0525\n",
      "Epoch [1/1], Batch: 520, Train Loss: 1.0119\n",
      "Epoch [1/1], Batch: 530, Train Loss: 1.0063\n",
      "Epoch [1/1], Batch: 540, Train Loss: 0.9675\n",
      "Epoch [1/1], Batch: 550, Train Loss: 0.9980\n",
      "Epoch [1/1], Batch: 560, Train Loss: 0.9815\n",
      "Epoch [1/1], Batch: 570, Train Loss: 1.0162\n",
      "Epoch [1/1], Batch: 580, Train Loss: 1.0194\n",
      "Epoch [1/1], Batch: 590, Train Loss: 0.9999\n",
      "Epoch [1/1], Batch: 600, Train Loss: 1.0118\n",
      "Epoch [1/1], Batch: 610, Train Loss: 0.9880\n",
      "Epoch [1/1], Batch: 620, Train Loss: 0.9875\n",
      "Epoch [1/1], Batch: 630, Train Loss: 1.0205\n",
      "Epoch [1/1], Batch: 640, Train Loss: 1.0052\n",
      "Epoch [1/1], Batch: 650, Train Loss: 1.0353\n",
      "Epoch [1/1], Batch: 660, Train Loss: 0.9822\n",
      "Epoch [1/1], Batch: 670, Train Loss: 1.0104\n",
      "Epoch [1/1], Batch: 680, Train Loss: 0.9605\n",
      "Epoch [1/1], Batch: 690, Train Loss: 0.9836\n",
      "Epoch [1/1], Batch: 700, Train Loss: 1.0094\n",
      "Epoch [1/1], Batch: 710, Train Loss: 0.9688\n",
      "Epoch [1/1], Batch: 720, Train Loss: 0.9842\n",
      "Epoch [1/1], Batch: 730, Train Loss: 0.9797\n",
      "Epoch [1/1], Batch: 740, Train Loss: 1.0039\n",
      "Epoch [1/1], Batch: 750, Train Loss: 0.9797\n",
      "Epoch [1/1], Batch: 760, Train Loss: 0.9974\n",
      "Epoch [1/1], Batch: 770, Train Loss: 0.9931\n",
      "Epoch [1/1], Batch: 780, Train Loss: 0.9960\n",
      "Epoch [1/1], Batch: 790, Train Loss: 0.9752\n",
      "Epoch [1/1], Batch: 800, Train Loss: 0.9977\n",
      "Epoch [1/1], Batch: 810, Train Loss: 0.9861\n",
      "Epoch [1/1], Batch: 820, Train Loss: 1.0382\n",
      "Epoch [1/1], Batch: 830, Train Loss: 1.0272\n",
      "Epoch [1/1], Batch: 840, Train Loss: 0.9854\n",
      "Epoch [1/1], Batch: 850, Train Loss: 0.9191\n",
      "Epoch [1/1], Batch: 860, Train Loss: 0.9835\n",
      "Epoch [1/1], Batch: 870, Train Loss: 0.9666\n",
      "Epoch [1/1], Batch: 880, Train Loss: 0.9711\n",
      "Epoch [1/1], Batch: 890, Train Loss: 0.9884\n",
      "Epoch [1/1], Batch: 900, Train Loss: 1.0351\n",
      "Epoch [1/1], Batch: 910, Train Loss: 0.9812\n",
      "Epoch [1/1], Batch: 920, Train Loss: 0.9497\n",
      "Epoch [1/1], Batch: 930, Train Loss: 0.9538\n",
      "Epoch [1/1], Batch: 940, Train Loss: 0.9715\n",
      "Epoch [1/1], Batch: 950, Train Loss: 0.9964\n",
      "Epoch [1/1], Batch: 960, Train Loss: 0.9470\n",
      "Epoch [1/1], Batch: 970, Train Loss: 0.9856\n",
      "Epoch [1/1], Batch: 980, Train Loss: 0.9766\n",
      "Epoch [1/1], Batch: 990, Train Loss: 0.9336\n",
      "Epoch [1/1], Batch: 1000, Train Loss: 0.9803\n",
      "Epoch [1/1], Batch: 1010, Train Loss: 0.9765\n",
      "Epoch [1/1], Batch: 1020, Train Loss: 0.9968\n",
      "Epoch [1/1], Batch: 1030, Train Loss: 0.9750\n",
      "Epoch [1/1], Batch: 1040, Train Loss: 1.0025\n",
      "Epoch [1/1], Batch: 1050, Train Loss: 0.9486\n",
      "Epoch [1/1], Batch: 1060, Train Loss: 0.9878\n",
      "Epoch [1/1], Batch: 1070, Train Loss: 0.9634\n",
      "Epoch [1/1], Batch: 1080, Train Loss: 0.9881\n",
      "Epoch [1/1], Batch: 1090, Train Loss: 0.9776\n",
      "Epoch [1/1], Batch: 1100, Train Loss: 1.0380\n",
      "Epoch [1/1], Batch: 1110, Train Loss: 0.9861\n",
      "Epoch [1/1], Batch: 1120, Train Loss: 0.9734\n",
      "Epoch [1/1], Batch: 1130, Train Loss: 0.9391\n",
      "Epoch [1/1], Batch: 1140, Train Loss: 0.9825\n",
      "Epoch [1/1], Batch: 1150, Train Loss: 0.9929\n",
      "Epoch [1/1], Batch: 1160, Train Loss: 1.0014\n",
      "Epoch [1/1], Batch: 1170, Train Loss: 1.0260\n",
      "Epoch [1/1], Batch: 1180, Train Loss: 0.9640\n",
      "Epoch [1/1], Batch: 1190, Train Loss: 1.0195\n",
      "Epoch [1/1], Batch: 1200, Train Loss: 0.9533\n",
      "Epoch [1/1], Batch: 1210, Train Loss: 1.0054\n",
      "Epoch [1/1], Batch: 1220, Train Loss: 1.0164\n",
      "Epoch [1/1], Batch: 1230, Train Loss: 0.9976\n",
      "Epoch [1/1], Batch: 1240, Train Loss: 0.9552\n",
      "Epoch [1/1], Batch: 1250, Train Loss: 0.9793\n",
      "Epoch [1/1], Batch: 1260, Train Loss: 0.9581\n",
      "Epoch [1/1], Batch: 1270, Train Loss: 1.0181\n",
      "Epoch [1/1], Batch: 1280, Train Loss: 0.9588\n",
      "Epoch [1/1], Batch: 1290, Train Loss: 0.9830\n",
      "Epoch [1/1], Batch: 1300, Train Loss: 0.9626\n",
      "Epoch [1/1], Batch: 1310, Train Loss: 1.0219\n",
      "Epoch [1/1], Batch: 1320, Train Loss: 1.0134\n",
      "Epoch [1/1], Batch: 1330, Train Loss: 0.9519\n",
      "Epoch [1/1], Batch: 1340, Train Loss: 0.9809\n",
      "Epoch [1/1], Batch: 1350, Train Loss: 1.0046\n",
      "Epoch [1/1], Batch: 1360, Train Loss: 0.9793\n",
      "Epoch [1/1], Batch: 1370, Train Loss: 0.9796\n",
      "Epoch [1/1], Batch: 1380, Train Loss: 1.0021\n",
      "Epoch [1/1], Batch: 1390, Train Loss: 0.9746\n",
      "Epoch [1/1], Batch: 1400, Train Loss: 1.0078\n",
      "Epoch [1/1], Batch: 1410, Train Loss: 1.0077\n",
      "Epoch [1/1], Batch: 1420, Train Loss: 0.9715\n",
      "Epoch [1/1], Batch: 1430, Train Loss: 0.9832\n",
      "Epoch [1/1], Batch: 1440, Train Loss: 0.9960\n",
      "Epoch [1/1], Batch: 1450, Train Loss: 0.9836\n",
      "Epoch [1/1], Batch: 1460, Train Loss: 0.9814\n",
      "Epoch [1/1], Batch: 1470, Train Loss: 0.9666\n",
      "Epoch [1/1], Batch: 1480, Train Loss: 0.9855\n",
      "Epoch [1/1], Batch: 1490, Train Loss: 0.9622\n",
      "Epoch [1/1], Batch: 1500, Train Loss: 1.0313\n",
      "Epoch [1/1], Batch: 1510, Train Loss: 0.9972\n",
      "Epoch [1/1], Batch: 1520, Train Loss: 0.9882\n",
      "Epoch [1/1], Batch: 1530, Train Loss: 0.9908\n",
      "Epoch [1/1], Batch: 1540, Train Loss: 1.0130\n",
      "Epoch [1/1], Batch: 1550, Train Loss: 0.9726\n",
      "Epoch [1/1], Batch: 1560, Train Loss: 1.0047\n",
      "Epoch [1/1], Batch: 1570, Train Loss: 0.9636\n",
      "Epoch [1/1], Batch: 1580, Train Loss: 0.9778\n",
      "Epoch [1/1], Batch: 1590, Train Loss: 0.9773\n",
      "Epoch [1/1], Batch: 1600, Train Loss: 0.9919\n",
      "Epoch [1/1], Batch: 1610, Train Loss: 1.0013\n",
      "Epoch [1/1], Batch: 1620, Train Loss: 0.9914\n",
      "Epoch [1/1], Batch: 1630, Train Loss: 1.0099\n",
      "Epoch [1/1], Batch: 1640, Train Loss: 1.0027\n",
      "Epoch [1/1], Batch: 1650, Train Loss: 0.9907\n",
      "Epoch [1/1], Batch: 1660, Train Loss: 0.9945\n",
      "Epoch [1/1], Batch: 1670, Train Loss: 0.9709\n",
      "Epoch [1/1], Batch: 1680, Train Loss: 0.9780\n",
      "Epoch [1/1], Batch: 1690, Train Loss: 1.0094\n",
      "Epoch [1/1], Batch: 1700, Train Loss: 0.9880\n",
      "Epoch [1/1], Batch: 1710, Train Loss: 0.9603\n",
      "Epoch [1/1], Batch: 1720, Train Loss: 0.9933\n",
      "Epoch [1/1], Batch: 1730, Train Loss: 0.9996\n",
      "Epoch [1/1], Batch: 1740, Train Loss: 0.9471\n",
      "Epoch [1/1], Batch: 1750, Train Loss: 0.9778\n",
      "Epoch [1/1], Batch: 1760, Train Loss: 0.9707\n",
      "Epoch [1/1], Batch: 1770, Train Loss: 0.9878\n",
      "Epoch [1/1], Batch: 1780, Train Loss: 0.9731\n",
      "Epoch [1/1], Batch: 1790, Train Loss: 1.0281\n",
      "Epoch [1/1], Batch: 1800, Train Loss: 0.9690\n",
      "Epoch [1/1], Batch: 1810, Train Loss: 1.0194\n",
      "Epoch [1/1], Batch: 1820, Train Loss: 0.9916\n",
      "Epoch [1/1], Batch: 1830, Train Loss: 0.9619\n",
      "Epoch [1/1], Batch: 1840, Train Loss: 0.9963\n",
      "Epoch [1/1], Batch: 1850, Train Loss: 0.9776\n",
      "Epoch [1/1], Batch: 1860, Train Loss: 0.9697\n",
      "Epoch [1/1], Batch: 1870, Train Loss: 1.0017\n",
      "Epoch [1/1], Batch: 1880, Train Loss: 0.9975\n",
      "Epoch [1/1], Batch: 1890, Train Loss: 0.9504\n",
      "Epoch [1/1], Batch: 1900, Train Loss: 0.9898\n",
      "Epoch [1/1], Batch: 1910, Train Loss: 1.0035\n",
      "Epoch [1/1], Batch: 1920, Train Loss: 0.9901\n",
      "Epoch [1/1], Batch: 1930, Train Loss: 1.0052\n",
      "Epoch [1/1], Batch: 1940, Train Loss: 0.9837\n",
      "Epoch [1/1], Batch: 1950, Train Loss: 0.9988\n",
      "Epoch [1/1], Batch: 1960, Train Loss: 0.9571\n",
      "Epoch [1/1], Batch: 1970, Train Loss: 0.9231\n",
      "Epoch [1/1], Batch: 1980, Train Loss: 0.9516\n",
      "Epoch [1/1], Batch: 1990, Train Loss: 1.0282\n",
      "Epoch [1/1], Batch: 2000, Train Loss: 0.9455\n",
      "Epoch [1/1], Batch: 2010, Train Loss: 0.9312\n",
      "Epoch [1/1], Batch: 2020, Train Loss: 0.9347\n",
      "Epoch [1/1], Batch: 2030, Train Loss: 0.9665\n",
      "Epoch [1/1], Batch: 2040, Train Loss: 1.0355\n",
      "Epoch [1/1], Batch: 2050, Train Loss: 0.9554\n",
      "Epoch [1/1], Batch: 2060, Train Loss: 0.9846\n",
      "Epoch [1/1], Batch: 2070, Train Loss: 0.9919\n",
      "Epoch [1/1], Batch: 2080, Train Loss: 0.9657\n",
      "Epoch [1/1], Batch: 2090, Train Loss: 0.9558\n",
      "Epoch [1/1], Batch: 2100, Train Loss: 0.9744\n",
      "Epoch [1/1], Batch: 2110, Train Loss: 0.9909\n",
      "Epoch [1/1], Batch: 2120, Train Loss: 0.9735\n",
      "Epoch [1/1], Batch: 2130, Train Loss: 0.9645\n",
      "Epoch [1/1], Batch: 2140, Train Loss: 0.9406\n",
      "Epoch [1/1], Batch: 2150, Train Loss: 0.9818\n",
      "Epoch [1/1], Batch: 2160, Train Loss: 0.9653\n",
      "Epoch [1/1], Batch: 2170, Train Loss: 0.9885\n",
      "Epoch [1/1], Batch: 2180, Train Loss: 0.9551\n",
      "Epoch [1/1], Batch: 2190, Train Loss: 0.9901\n",
      "Epoch [1/1], Batch: 2200, Train Loss: 0.9285\n",
      "Epoch [1/1], Batch: 2210, Train Loss: 0.9682\n",
      "Epoch [1/1], Batch: 2220, Train Loss: 0.9414\n",
      "Epoch [1/1], Batch: 2230, Train Loss: 0.9917\n",
      "Epoch [1/1], Batch: 2240, Train Loss: 0.9682\n",
      "Epoch [1/1], Batch: 2250, Train Loss: 1.0010\n",
      "Epoch [1/1], Batch: 2260, Train Loss: 0.9935\n",
      "Epoch [1/1], Batch: 2270, Train Loss: 0.9916\n",
      "Epoch [1/1], Batch: 2280, Train Loss: 1.0031\n",
      "Epoch [1/1], Batch: 2290, Train Loss: 0.9602\n",
      "Epoch [1/1], Batch: 2300, Train Loss: 0.9527\n",
      "Epoch [1/1], Batch: 2310, Train Loss: 0.9888\n",
      "Epoch [1/1], Batch: 2320, Train Loss: 0.9564\n",
      "Epoch [1/1], Batch: 2330, Train Loss: 1.0034\n",
      "Epoch [1/1], Batch: 2340, Train Loss: 0.9633\n",
      "Epoch [1/1], Batch: 2350, Train Loss: 0.9806\n",
      "Epoch [1/1], Batch: 2360, Train Loss: 0.9821\n",
      "Epoch [1/1], Batch: 2370, Train Loss: 0.9904\n",
      "Epoch [1/1], Batch: 2380, Train Loss: 0.9896\n",
      "Epoch [1/1], Batch: 2390, Train Loss: 1.0421\n",
      "Epoch [1/1], Batch: 2400, Train Loss: 0.9941\n",
      "Epoch [1/1], Batch: 2410, Train Loss: 0.9622\n",
      "Epoch [1/1], Batch: 2420, Train Loss: 0.9879\n",
      "Epoch [1/1], Batch: 2430, Train Loss: 0.9683\n",
      "Epoch [1/1], Batch: 2440, Train Loss: 0.9481\n",
      "Epoch [1/1], Batch: 2450, Train Loss: 0.9739\n",
      "Epoch [1/1], Batch: 2460, Train Loss: 0.9919\n",
      "Epoch [1/1], Batch: 2470, Train Loss: 0.9810\n",
      "Epoch [1/1], Batch: 2480, Train Loss: 0.9732\n",
      "Epoch [1/1], Batch: 2490, Train Loss: 0.9877\n",
      "Epoch [1/1], Batch: 2500, Train Loss: 0.9847\n",
      "Epoch [1/1], Batch: 2510, Train Loss: 0.9952\n",
      "Epoch [1/1], Batch: 2520, Train Loss: 0.9570\n",
      "Epoch [1/1], Batch: 2530, Train Loss: 0.9814\n",
      "Epoch [1/1], Batch: 2540, Train Loss: 0.9922\n",
      "Epoch [1/1], Batch: 2550, Train Loss: 1.0081\n",
      "Epoch [1/1], Batch: 2560, Train Loss: 0.9660\n",
      "Epoch [1/1], Batch: 2570, Train Loss: 0.9974\n",
      "Epoch [1/1], Batch: 2580, Train Loss: 0.9801\n",
      "Epoch [1/1], Batch: 2590, Train Loss: 0.9968\n",
      "Epoch [1/1], Batch: 2600, Train Loss: 1.0048\n",
      "Epoch [1/1], Batch: 2610, Train Loss: 0.9595\n",
      "Epoch [1/1], Batch: 2620, Train Loss: 0.9705\n",
      "Epoch [1/1], Batch: 2630, Train Loss: 0.9504\n",
      "Epoch [1/1], Batch: 2640, Train Loss: 0.9709\n",
      "Epoch [1/1], Batch: 2650, Train Loss: 0.9416\n",
      "Epoch [1/1], Batch: 2660, Train Loss: 0.9806\n",
      "Epoch [1/1], Batch: 2670, Train Loss: 1.0078\n",
      "Epoch [1/1], Batch: 2680, Train Loss: 0.9844\n",
      "Epoch [1/1], Batch: 2690, Train Loss: 0.9865\n",
      "Epoch [1/1], Batch: 2700, Train Loss: 0.9663\n",
      "Epoch [1/1], Batch: 2710, Train Loss: 0.9574\n",
      "Epoch [1/1], Batch: 2720, Train Loss: 0.9741\n",
      "Epoch [1/1], Batch: 2730, Train Loss: 0.9858\n",
      "Epoch [1/1], Batch: 2740, Train Loss: 0.9798\n",
      "Epoch [1/1], Batch: 2750, Train Loss: 0.9955\n",
      "Epoch [1/1], Batch: 2760, Train Loss: 0.9803\n",
      "Epoch [1/1], Batch: 2770, Train Loss: 0.9945\n",
      "Epoch [1/1], Batch: 2780, Train Loss: 0.9474\n",
      "Epoch [1/1], Batch: 2790, Train Loss: 1.0039\n",
      "Epoch [1/1], Batch: 2800, Train Loss: 0.9242\n",
      "Epoch [1/1], Batch: 2810, Train Loss: 1.0273\n",
      "Epoch [1/1], Batch: 2820, Train Loss: 0.9669\n",
      "Epoch [1/1], Batch: 2830, Train Loss: 0.9520\n",
      "Epoch [1/1], Batch: 2840, Train Loss: 0.9213\n",
      "Epoch [1/1], Batch: 2850, Train Loss: 0.9506\n",
      "Epoch [1/1], Batch: 2860, Train Loss: 0.9693\n",
      "Epoch [1/1], Batch: 2870, Train Loss: 0.9700\n",
      "Epoch [1/1], Batch: 2880, Train Loss: 0.9966\n",
      "Epoch [1/1], Batch: 2890, Train Loss: 0.9867\n",
      "Epoch [1/1], Batch: 2900, Train Loss: 1.0259\n",
      "Epoch [1/1], Batch: 2910, Train Loss: 0.9891\n",
      "Epoch [1/1], Batch: 2920, Train Loss: 0.9615\n",
      "Epoch [1/1], Batch: 2930, Train Loss: 1.0104\n",
      "Epoch [1/1], Batch: 2940, Train Loss: 0.9739\n",
      "Epoch [1/1], Batch: 2950, Train Loss: 0.9784\n",
      "Epoch [1/1], Batch: 2960, Train Loss: 0.9218\n",
      "Epoch [1/1], Batch: 2970, Train Loss: 0.9495\n",
      "Epoch [1/1], Batch: 2980, Train Loss: 0.9594\n",
      "Epoch [1/1], Batch: 2990, Train Loss: 0.9942\n",
      "Epoch [1/1], Batch: 3000, Train Loss: 0.9678\n",
      "Epoch [1/1], Batch: 3010, Train Loss: 0.9866\n",
      "Epoch [1/1], Batch: 3020, Train Loss: 0.9671\n",
      "Epoch [1/1], Batch: 3030, Train Loss: 1.0092\n",
      "Epoch [1/1], Batch: 3040, Train Loss: 0.9694\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[272], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      5\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mTrainingDataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[175], line 14\u001b[0m, in \u001b[0;36mQueryDocsDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(to_w2v_embedding(sp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueries[idx]\u001b[38;5;241m.\u001b[39mlower()), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat),\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelevant\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(to_w2v_embedding(sp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelevant_docs[idx]\u001b[38;5;241m.\u001b[39mlower()), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat),\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mirrelevant\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mto_w2v_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mirrelevant_docs\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat),\n\u001b[1;32m     15\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m, in \u001b[0;36mto_w2v_embedding\u001b[0;34m(sp, text)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (token \u001b[38;5;129;01min\u001b[39;00m w2v_model\u001b[38;5;241m.\u001b[39mwv): \n\u001b[0;32m----> 7\u001b[0m         embeddings\u001b[38;5;241m.\u001b[39mappend(\u001b[43mw2v_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gensim/models/keyedvectors.py:453\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors[index]\n\u001b[0;32m--> 453\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetflags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# disallow direct tampering that would invalidate `norms` etc\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    Towers.train() \n",
    "    train_loss = 0.0\n",
    "\n",
    "    i = 0\n",
    "    for batch in TrainingDataloader:\n",
    "        i +=1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model to get embeddings\n",
    "        query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings = Towers(\n",
    "            batch['query'], \n",
    "            batch['relevant'], \n",
    "            batch['irrelevant']\n",
    "        )\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = triplet_loss_function_cosine(query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings, margin)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        if (i) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch: {i}, Train Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # break\n",
    "    \n",
    "    # Testing phase\n",
    "    Towers.eval()  # Set model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():  # No need to track gradients for testing\n",
    "        for batch in TestingDataloader:\n",
    "            query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings = Towers(\n",
    "                batch['query'], batch['relevant_doc'], batch['irrelevant_doc']\n",
    "            )\n",
    "            \n",
    "            loss = triplet_loss_function_cosine(query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings, margin)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "    avg_train_loss = train_loss / len(TestingDataloader)\n",
    "    avg_test_loss = test_loss / len(TestingDataloader)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
