{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect all texts to one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_parquet('train.parquet')\n",
    "# test = pd.read_parquet('test.parquet')\n",
    "# validate = pd.read_parquet('validate.parquet')\n",
    "\n",
    "# with open('AllTexts.txt', 'w') as f:\n",
    "#     pass  # This just creates the file, immediately closing it\n",
    "\n",
    "# with open('AllTexts.txt', 'a') as f:  # Open file in append mode\n",
    "#     for _, row in train.iterrows():\n",
    "#         concatenated = '\\n'.join(row['passages']['passage_text'])\n",
    "#         concatenated = '\\n'.join([concatenated, '\\n'.join(row['answers'])])\n",
    "#         concatenated = '\\n'.join([concatenated, row['query']])\n",
    "#         f.write(concatenated + '\\n')\n",
    "#     for _, row in test.iterrows():\n",
    "#         concatenated = '\\n'.join(row['passages']['passage_text'])\n",
    "#         concatenated = '\\n'.join([concatenated, '\\n'.join(row['answers'])])\n",
    "#         concatenated = '\\n'.join([concatenated, row['query']])\n",
    "#         f.write(concatenated + '\\n')\n",
    "#     for _, row in validate.iterrows():\n",
    "#         concatenated = '\\n'.join(row['passages']['passage_text'])\n",
    "#         concatenated = '\\n'.join([concatenated, '\\n'.join(row['answers'])])\n",
    "#         concatenated = '\\n'.join([concatenated, row['query']])\n",
    "#         f.write(concatenated + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train tokanizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: AllTexts.txt\n",
      "  input_format: \n",
      "  model_prefix: spm_AllTexts\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 30000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: AllTexts.txt\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(124) LOG(WARNING) Too many sentences are loaded! (1052541), which may slow down training.\n",
      "trainer_interface.cc(126) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(129) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1052541 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=366839418\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9521% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=96\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999521\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1052541 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=229628222\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 1000096 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1052541\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 1198039\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 1198039 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=428208 obj=11.5506 num_tokens=2983019 num_tokens/piece=6.96629\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=368644 obj=9.05448 num_tokens=2998063 num_tokens/piece=8.13268\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=276468 obj=9.02049 num_tokens=3083928 num_tokens/piece=11.1547\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=276372 obj=9.01674 num_tokens=3100856 num_tokens/piece=11.2199\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=207276 obj=9.03854 num_tokens=3235954 num_tokens/piece=15.6118\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=207270 obj=9.03602 num_tokens=3236109 num_tokens/piece=15.613\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=155450 obj=9.06985 num_tokens=3384706 num_tokens/piece=21.7736\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=155450 obj=9.06317 num_tokens=3384679 num_tokens/piece=21.7734\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=116586 obj=9.11177 num_tokens=3542014 num_tokens/piece=30.3811\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=116586 obj=9.10398 num_tokens=3541839 num_tokens/piece=30.3796\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=87439 obj=9.17114 num_tokens=3704656 num_tokens/piece=42.3685\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=87439 obj=9.15931 num_tokens=3704520 num_tokens/piece=42.3669\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=65579 obj=9.2438 num_tokens=3869611 num_tokens/piece=59.0069\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=65579 obj=9.22879 num_tokens=3869593 num_tokens/piece=59.0066\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=49184 obj=9.3379 num_tokens=4045243 num_tokens/piece=82.2471\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=49184 obj=9.31905 num_tokens=4045485 num_tokens/piece=82.2521\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=36888 obj=9.45084 num_tokens=4224041 num_tokens/piece=114.51\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=36888 obj=9.42716 num_tokens=4224229 num_tokens/piece=114.515\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=33000 obj=9.48178 num_tokens=4291364 num_tokens/piece=130.041\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=33000 obj=9.47216 num_tokens=4291537 num_tokens/piece=130.047\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: spm_AllTexts.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: spm_AllTexts.vocab\n"
     ]
    }
   ],
   "source": [
    "# spm.SentencePieceTrainer.train(\n",
    "#     input = 'AllTexts.txt',\n",
    "#     model_prefix='spm_AllTexts', \n",
    "#     vocab_size=30000,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('spm_AllTexts.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
