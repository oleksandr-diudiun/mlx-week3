{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet('train.parquet')\n",
    "test = pd.read_parquet('test.parquet')\n",
    "validate = pd.read_parquet('validate.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect all texts to one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('AllTexts.txt', 'w') as f:\n",
    "#     pass  # This just creates the file, immediately closing it\n",
    "\n",
    "# with open('AllTexts.txt', 'a') as f:  # Open file in append mode\n",
    "#     for _, row in train.iterrows():\n",
    "#         concatenated = '\\n'.join(row['passages']['passage_text'])\n",
    "#         concatenated = '\\n'.join([concatenated, '\\n'.join(row['answers'])])\n",
    "#         concatenated = '\\n'.join([concatenated, row['query']])\n",
    "#         f.write(concatenated.lower() + '\\n')\n",
    "#     for _, row in test.iterrows():\n",
    "#         concatenated = '\\n'.join(row['passages']['passage_text'])\n",
    "#         concatenated = '\\n'.join([concatenated, '\\n'.join(row['answers'])])\n",
    "#         concatenated = '\\n'.join([concatenated, row['query']])\n",
    "#         f.write(concatenated.lower() + '\\n')\n",
    "#     for _, row in validate.iterrows():\n",
    "#         concatenated = '\\n'.join(row['passages']['passage_text'])\n",
    "#         concatenated = '\\n'.join([concatenated, '\\n'.join(row['answers'])])\n",
    "#         concatenated = '\\n'.join([concatenated, row['query']])\n",
    "#         f.write(concatenated.lower() + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train tokanizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spm.SentencePieceTrainer.train(\n",
    "#     input = 'AllTexts.txt',\n",
    "#     model_prefix='spm_AllTexts', \n",
    "#     vocab_size=30000,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokanizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('spm_AllTexts.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_file(file_path, sp_processor):\n",
    "    tokenized_sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Assuming each line in the file is a separate sentence or paragraph\n",
    "            # Tokenize the line and add the list of tokens to the tokenized_sentences list\n",
    "            tokenized_sentences.append(sp_processor.encode_as_pieces(line.strip()))\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokinize all the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokinized_sentences = tokenize_file(\"AllTexts.txt\", sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export tokens to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"Tokens_AllText.json\", 'w', encoding='utf-8') as file:\n",
    "#     json.dump(tokinized_sentences, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    min_count  =20,\n",
    "    window     =10,\n",
    "    vector_size=vector_size,\n",
    "    sample     =6e-5, \n",
    "    alpha      = 0.03, \n",
    "    min_alpha  = 0.0007, \n",
    "    negative   = 20,\n",
    "    workers    = multiprocessing.cpu_count() - 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(tokinized_sentences))\n",
    "# w2v_model.build_vocab(tokinized_sentences)\n",
    "# w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"word2vec_vocab.txt\", 'w') as vocab_file:\n",
    "#     for word in w2v_model.wv.key_to_index.keys():\n",
    "#         vocab_file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model.train(tokinized_sentences, total_examples=w2v_model.corpus_count, epochs=20, report_delay=1)\n",
    "# w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('▁cyber', 0.5855585336685181), ('▁hack', 0.5675662159919739), ('▁malicious', 0.5665103197097778), ('▁malware', 0.5581293106079102)]\n",
      "[('▁cyber', 0.5855585336685181), ('▁hack', 0.5675662159919739), ('▁malicious', 0.5665103197097778), ('▁malware', 0.5581293106079102), ('▁scam', 0.5384910702705383), ('▁spyware', 0.5197509527206421), ('▁legitimate', 0.5031879544258118), ('▁adware', 0.4846465289592743), ('▁pretend', 0.469952255487442), ('▁insider', 0.46461066603660583)]\n"
     ]
    }
   ],
   "source": [
    "similar_words = w2v_model.wv.most_similar('▁hacker', topn=4)\n",
    "print(similar_words)\n",
    "print(w2v_model.wv.most_similar(sp.encode_as_pieces('Hacker'.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_w2v_embedding(sp, text):\n",
    "    tokens = sp.encode_as_pieces(text.lower())\n",
    "\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if (token in w2v_model.wv): \n",
    "            embeddings.append(w2v_model.wv[token])\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triples for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTriplesTokens(dataframe):\n",
    "    triples = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        available_indices = list(dataframe.index)\n",
    "        available_indices.remove(index)\n",
    "        \n",
    "        for relevant in row['passages']['passage_text']:\n",
    "            random_index = np.random.choice(available_indices)\n",
    "            random_doc_index = np.random.choice(\n",
    "                list(\n",
    "                    range(\n",
    "                        len(dataframe.iloc[random_index]['passages']['passage_text'])\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "            irrelevant = dataframe.iloc[random_index]['passages']['passage_text'][random_doc_index]\n",
    "\n",
    "            triples.append([\n",
    "                row['query'],\n",
    "                relevant,\n",
    "                irrelevant,\n",
    "            ])\n",
    "\n",
    "    return triples\n",
    "\n",
    "# train_triplets = prepareTriplesTokens(train)\n",
    "# test_triplets = prepareTriplesTokens(test)\n",
    "# validate_triplets = prepareTriplesTokens(validate)\n",
    "\n",
    "train_triplets = pd.read_parquet('train_triplets.parquet')\n",
    "test_triplets = pd.read_parquet('test_triplets.parquet')\n",
    "validate_triplets = pd.read_parquet('validate_triplets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         query                                           relevant  \\\n",
      "0  what is rba  Since 2007, the RBA's outstanding reputation h...   \n",
      "1  what is rba  The Reserve Bank of Australia (RBA) came into ...   \n",
      "\n",
      "                                          irrelevant  \n",
      "0  This report describes the typical weather at t...  \n",
      "1  1. district, community the vicar of a small pa...  \n"
     ]
    }
   ],
   "source": [
    "print(train_triplets[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the list of triples to a DataFrame\n",
    "# columns = ['query', 'relevant', 'irrelevant']\n",
    "# train_triplets = pd.DataFrame(train_triplets, columns=columns)\n",
    "# test_triplets = pd.DataFrame(test_triplets, columns=columns)\n",
    "# validate_triplets = pd.DataFrame(validate_triplets, columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_triplets_embeddings = pd.DataFrame()\n",
    "\n",
    "# test_triplets_embeddings['query_embeddings'] = test_triplets['query'].apply(lambda x: to_w2v_embedding(sp, x))\n",
    "# test_triplets_embeddings['relevant_embeddings'] = test_triplets['relevant'].apply(lambda x: to_w2v_embedding(sp, x))\n",
    "# test_triplets_embeddings['irrelevant_embeddings'] = test_triplets['irrelevant'].apply(lambda x: to_w2v_embedding(sp, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total DataFrame size: 134.55 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the memory usage of each column in bytes, then sum them up, and convert to megabytes\n",
    "# total_memory_mb = test_triplets_embeddings.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "\n",
    "# print(f'Total DataFrame size: {total_memory_mb:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_triplets_embeddings = pd.DataFrame()\n",
    "# train_triplets_embeddings['query_embeddings'] = train_triplets['query'].apply(lambda x: to_w2v_embedding(sp, x))\n",
    "# train_triplets_embeddings['relevant_embeddings'] = train_triplets['relevant'].apply(lambda x: to_w2v_embedding(sp, x))\n",
    "# train_triplets_embeddings['irrelevant_embeddings'] = train_triplets['irrelevant'].apply(lambda x: to_w2v_embedding(sp, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total DataFrame size: 1150.24 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the memory usage of each column in bytes, then sum them up, and convert to megabytes\n",
    "# total_memory_mb = train_triplets_embeddings.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "\n",
    "# print(f'Total DataFrame size: {total_memory_mb:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [[-0.38117662, -1.751613, 0.118526, -4.4650145...\n",
      "Name: query_embeddings, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print(test_triplets_embeddings['query_embeddings'].head(1))\n",
    "\n",
    "# train_triplets_embeddings.to_parquet('train_triplets_with_embedings.parquet', engine='pyarrow') \n",
    "# test_triplets_embeddings.to_parquet('test_triplets_with_embedings.parquet', engine='pyarrow') \n",
    "# validate_triplets.to_parquet('validate_triplets_with_embedings.parquet', engine='pyarrow') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryDocsDataset(Dataset):\n",
    "    def __init__(self, sp, queries, relevant_docs, irrelevant_docs):\n",
    "        self.queries = queries\n",
    "        self.relevant_docs = relevant_docs\n",
    "        self.irrelevant_docs = irrelevant_docs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'query': torch.tensor(to_w2v_embedding(sp, self.queries[idx].lower()), dtype=torch.float),\n",
    "            'relevant': torch.tensor(to_w2v_embedding(sp, self.relevant_docs[idx].lower()), dtype=torch.float),\n",
    "            'irrelevant': torch.tensor(to_w2v_embedding(sp, self.irrelevant_docs[idx].lower()), dtype=torch.float),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom DataLoader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataLoader:\n",
    "#     def __init__(self, dataset, batch_size, shuffle=False):\n",
    "#         self.dataset = dataset\n",
    "#         self.batch_size = batch_size\n",
    "#         self.shuffle = shuffle\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         self.idx = 0\n",
    "#         if self.shuffle:\n",
    "#             np.random.shuffle(self.dataset)\n",
    "#         return self\n",
    "\n",
    "#     def __next__(self):\n",
    "#         if self.idx < len(self.dataset):\n",
    "#             batch = self.dataset[self.idx:self.idx + self.batch_size]\n",
    "#             self.idx += self.batch_size\n",
    "#             return batch\n",
    "#         else:\n",
    "#             raise StopIteration\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset) // self.batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingDataset = QueryDocsDataset(sp, train_triplets['query'], train_triplets['relevant'], train_triplets['irrelevant'])\n",
    "TestingDataset = QueryDocsDataset(sp, test_triplets['query'], test_triplets['relevant'], test_triplets['irrelevant'])\n",
    "ValidationDataset = QueryDocsDataset(sp, validate_triplets['query'], validate_triplets['relevant'], validate_triplets['irrelevant'])\n",
    "\n",
    "# TrainingDataset = QueryDocsDataset(sp, train_triplets_embeddings['query_embeddings'], train_triplets_embeddings['relevant_embeddings'], train_triplets_embeddings['irrelevant_embeddings'])\n",
    "# TestingDataset = QueryDocsDataset(sp, test_triplets_embeddings['query_embeddings'], test_triplets_embeddings['relevant_embeddings'], test_triplets_embeddings['irrelevant_embeddings'])\n",
    "# ValidationDataset = QueryDocsDataset(sp, validate_triplets_embeddings['query'], validate_triplets_embeddings['relevant'], validate_triplets_embeddings['irrelevant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(QueryRNNCell, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.weight_ih = nn.Parameter(torch.randn(input_size, hidden_size))  # Input to hidden weights\n",
    "        self.weight_hh = nn.Parameter(torch.randn(hidden_size, hidden_size))  # Hidden to hidden weights\n",
    "        \n",
    "        self.bias_hh = nn.Parameter(torch.randn(hidden_size))  # Bias\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        return torch.tanh(\n",
    "            torch.mm(input, self.weight_ih) + torch.mm(hidden, self.weight_hh) + self.bias_hh\n",
    "        )\n",
    "    \n",
    "class QueryRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(QueryRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_cell = QueryRNNCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Assuming input is of shape (batch, seq_len, input_size)\n",
    "        batch_size, seq_len, _ = input.shape\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size)  # Initial hidden state\n",
    "        for i in range(seq_len):\n",
    "            \n",
    "            mask = torch.any(input[:, i, :] != 0, dim=1).float().unsqueeze(1)  # Shape: (batch_size, 1)\n",
    "            current_input = input[:, i, :]  \n",
    "            \n",
    "            current_hidden = self.rnn_cell(current_input, hidden)\n",
    "            \n",
    "            # Apply mask: Only update hidden state for non-padded inputs\n",
    "            hidden = mask * current_hidden + (1 - mask) * hidden\n",
    "            \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Towers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        self.queryEncoder = QueryRNN(embedding_size, hidden_size)\n",
    "        self.docEncoder = QueryRNN(embedding_size, hidden_size)\n",
    "\n",
    "    def forward(self, query, relevant, irrelevant):\n",
    "        query_embedding = self.queryEncoder(query)\n",
    "        relevant_embedding = self.docEncoder(relevant)\n",
    "        irrelevant_embedding = self.docEncoder(irrelevant)\n",
    "        return query_embedding, relevant_embedding, irrelevant_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lose Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def triplet_loss_function_cosine(query, relevant_doc, irrelevant_doc, margin):\n",
    "    # Compute cosine similarity (the output ranges from -1 to 1)\n",
    "    relevant_similarity = F.cosine_similarity(query, relevant_doc)\n",
    "    irrelevant_similarity = F.cosine_similarity(query, irrelevant_doc)\n",
    "    \n",
    "    # Convert similarities to distances (ranges from 0 to 2)\n",
    "    relevant_distance = 1 - relevant_similarity\n",
    "    irrelevant_distance = 1 - irrelevant_similarity\n",
    "    \n",
    "    # Compute the triplet loss\n",
    "    triplet_loss = torch.clamp(margin + relevant_distance - irrelevant_distance, min=0)\n",
    "    return triplet_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Extract lists of tensors for 'query', 'relevant', and 'irrelevant' from the batch\n",
    "    query_tensors = [item['query'] for item in batch]\n",
    "    relevant_tensors = [item['relevant'] for item in batch]\n",
    "    irrelevant_tensors = [item['irrelevant'] for item in batch]\n",
    "    \n",
    "    # Pad sequences within each list to the same length\n",
    "    query_padded = pad_sequence(query_tensors, batch_first=True, padding_value=0)\n",
    "    relevant_padded = pad_sequence(relevant_tensors, batch_first=True, padding_value=0)\n",
    "    irrelevant_padded = pad_sequence(irrelevant_tensors, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Return a dictionary with padded sequences\n",
    "    return {\n",
    "        'query': query_padded,\n",
    "        'relevant': relevant_padded,\n",
    "        'irrelevant': irrelevant_padded\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## padding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[51, 92, 14, 71],\n",
      "         [60, 20, 82, 86],\n",
      "         [74, 74, 87, 99],\n",
      "         [23,  2, 21, 52],\n",
      "         [ 1, 87, 29, 37],\n",
      "         [ 0,  0,  0,  0]],\n",
      "\n",
      "        [[51, 92, 14, 71],\n",
      "         [60, 20, 82, 86],\n",
      "         [74, 74, 87, 99],\n",
      "         [23,  2, 21, 52],\n",
      "         [ 1, 87, 29, 37],\n",
      "         [60, 20, 82, 86]]])\n"
     ]
    }
   ],
   "source": [
    "# Convert the numpy array to a PyTorch tensor\n",
    "tensors =[ \n",
    "    torch.tensor([\n",
    "        [51, 92, 14, 71],\n",
    "        [60, 20, 82, 86],\n",
    "        [74, 74, 87, 99],\n",
    "        [23,  2, 21, 52],\n",
    "        [ 1, 87, 29, 37],\n",
    "    ]),\n",
    "    torch.tensor([\n",
    "        [51, 92, 14, 71],\n",
    "        [60, 20, 82, 86],\n",
    "        [74, 74, 87, 99],\n",
    "        [23,  2, 21, 52],\n",
    "        [ 1, 87, 29, 37],\n",
    "        [60, 20, 82, 86],\n",
    "    ]),\n",
    "]\n",
    "dump = pad_sequence(tensors, batch_first=True)\n",
    "\n",
    "print(dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 16\n",
    "margin = 1.0\n",
    "batch_size = 200\n",
    "num_epochs = 1\n",
    "\n",
    "# TrainingDataloader = DataLoader(TrainingDataset, batch_size, shuffle=False)\n",
    "# TestingDataloader = DataLoader(TestingDataset, batch_size, shuffle=False)\n",
    "\n",
    "TrainingDataloader = DataLoader(TrainingDataset, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "TestingDataloader = DataLoader(TestingDataset, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "ValidatingDataloader = DataLoader(ValidationDataset, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Instantiate the model\n",
    "Towers = TwoTowerModel(embedding_size, hidden_size)\n",
    "optimizer = torch.optim.Adam(Towers.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recheck the padding results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 7, 128])\n",
      "torch.Size([20, 134, 128])\n",
      "torch.Size([20, 154, 128])\n",
      "Element 1, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 2, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 3, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 4, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 5, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 6, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 7, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 8, last row of 7:\n",
      "[-0.8882818  -3.348267    0.36098903  0.46327683  1.2810141  -1.3186098\n",
      " -1.9863871   2.6532955   2.0709038  -0.44499442  1.1410644  -1.9977982\n",
      "  3.0292907   3.7966125  -3.3948023  -1.1889541  -3.0714736   2.3496273\n",
      " -2.6288993   3.7311935  -4.277793    3.467017    3.2347875   2.0128965\n",
      " -4.0168424  -3.3952324   0.6621953   3.2866776  -1.4068488  -0.37303498\n",
      " -1.969379   -0.02366243  2.6913583   1.9048185   1.4900229   3.6248085\n",
      "  5.1554775   4.3432117   1.122215    5.5826354   1.2623793   0.5097447\n",
      " -0.7163874   1.9075923  -0.7156864  -0.598586    4.7436876  -0.6780336\n",
      "  0.50797147 -2.3601375  -1.411537    5.2797384  -2.2775202   2.7040942\n",
      " -0.9327177   5.1660175   0.5298864  -2.871028    6.6004486   0.06043774\n",
      " -1.0087829   2.314619   -8.490975    0.65564185  0.08034915 -1.8744208\n",
      " -1.340661    3.9049559   2.6682553  -2.9101653   3.3522274   7.4489713\n",
      " -2.4132447   0.9705874   1.3886019   0.599941   -0.7818905   3.4701912\n",
      "  4.4904246  -3.8627303   5.118557   -1.2698896   2.9748638   0.08187379\n",
      "  2.0651286  -2.3976824   3.0384939   2.755078   -4.7522836   0.79728156\n",
      " -4.785485   -3.3824747  -3.2314544   5.399737   -4.068802   -1.3460119\n",
      " -1.3903389   3.4053926   1.259242    2.3484674   0.571316    3.3621442\n",
      " -3.4612641   0.74964064  5.21113     0.91659003  2.1706917  -2.8290615\n",
      "  2.4676163   1.6404735   2.9110887   0.01441479 -2.4854708  -1.136586\n",
      "  2.778274   -1.2347221   3.8130352  -2.3884017  -1.3023429   0.5237943\n",
      "  0.8718899  -1.2806287   6.1741323   3.5710938  -0.77821237 -2.396023\n",
      " -1.2373894   1.2006588 ]\n",
      "\n",
      "Element 9, last row of 7:\n",
      "[-0.8882818  -3.348267    0.36098903  0.46327683  1.2810141  -1.3186098\n",
      " -1.9863871   2.6532955   2.0709038  -0.44499442  1.1410644  -1.9977982\n",
      "  3.0292907   3.7966125  -3.3948023  -1.1889541  -3.0714736   2.3496273\n",
      " -2.6288993   3.7311935  -4.277793    3.467017    3.2347875   2.0128965\n",
      " -4.0168424  -3.3952324   0.6621953   3.2866776  -1.4068488  -0.37303498\n",
      " -1.969379   -0.02366243  2.6913583   1.9048185   1.4900229   3.6248085\n",
      "  5.1554775   4.3432117   1.122215    5.5826354   1.2623793   0.5097447\n",
      " -0.7163874   1.9075923  -0.7156864  -0.598586    4.7436876  -0.6780336\n",
      "  0.50797147 -2.3601375  -1.411537    5.2797384  -2.2775202   2.7040942\n",
      " -0.9327177   5.1660175   0.5298864  -2.871028    6.6004486   0.06043774\n",
      " -1.0087829   2.314619   -8.490975    0.65564185  0.08034915 -1.8744208\n",
      " -1.340661    3.9049559   2.6682553  -2.9101653   3.3522274   7.4489713\n",
      " -2.4132447   0.9705874   1.3886019   0.599941   -0.7818905   3.4701912\n",
      "  4.4904246  -3.8627303   5.118557   -1.2698896   2.9748638   0.08187379\n",
      "  2.0651286  -2.3976824   3.0384939   2.755078   -4.7522836   0.79728156\n",
      " -4.785485   -3.3824747  -3.2314544   5.399737   -4.068802   -1.3460119\n",
      " -1.3903389   3.4053926   1.259242    2.3484674   0.571316    3.3621442\n",
      " -3.4612641   0.74964064  5.21113     0.91659003  2.1706917  -2.8290615\n",
      "  2.4676163   1.6404735   2.9110887   0.01441479 -2.4854708  -1.136586\n",
      "  2.778274   -1.2347221   3.8130352  -2.3884017  -1.3023429   0.5237943\n",
      "  0.8718899  -1.2806287   6.1741323   3.5710938  -0.77821237 -2.396023\n",
      " -1.2373894   1.2006588 ]\n",
      "\n",
      "Element 10, last row of 7:\n",
      "[-0.8882818  -3.348267    0.36098903  0.46327683  1.2810141  -1.3186098\n",
      " -1.9863871   2.6532955   2.0709038  -0.44499442  1.1410644  -1.9977982\n",
      "  3.0292907   3.7966125  -3.3948023  -1.1889541  -3.0714736   2.3496273\n",
      " -2.6288993   3.7311935  -4.277793    3.467017    3.2347875   2.0128965\n",
      " -4.0168424  -3.3952324   0.6621953   3.2866776  -1.4068488  -0.37303498\n",
      " -1.969379   -0.02366243  2.6913583   1.9048185   1.4900229   3.6248085\n",
      "  5.1554775   4.3432117   1.122215    5.5826354   1.2623793   0.5097447\n",
      " -0.7163874   1.9075923  -0.7156864  -0.598586    4.7436876  -0.6780336\n",
      "  0.50797147 -2.3601375  -1.411537    5.2797384  -2.2775202   2.7040942\n",
      " -0.9327177   5.1660175   0.5298864  -2.871028    6.6004486   0.06043774\n",
      " -1.0087829   2.314619   -8.490975    0.65564185  0.08034915 -1.8744208\n",
      " -1.340661    3.9049559   2.6682553  -2.9101653   3.3522274   7.4489713\n",
      " -2.4132447   0.9705874   1.3886019   0.599941   -0.7818905   3.4701912\n",
      "  4.4904246  -3.8627303   5.118557   -1.2698896   2.9748638   0.08187379\n",
      "  2.0651286  -2.3976824   3.0384939   2.755078   -4.7522836   0.79728156\n",
      " -4.785485   -3.3824747  -3.2314544   5.399737   -4.068802   -1.3460119\n",
      " -1.3903389   3.4053926   1.259242    2.3484674   0.571316    3.3621442\n",
      " -3.4612641   0.74964064  5.21113     0.91659003  2.1706917  -2.8290615\n",
      "  2.4676163   1.6404735   2.9110887   0.01441479 -2.4854708  -1.136586\n",
      "  2.778274   -1.2347221   3.8130352  -2.3884017  -1.3023429   0.5237943\n",
      "  0.8718899  -1.2806287   6.1741323   3.5710938  -0.77821237 -2.396023\n",
      " -1.2373894   1.2006588 ]\n",
      "\n",
      "Element 11, last row of 7:\n",
      "[-0.8882818  -3.348267    0.36098903  0.46327683  1.2810141  -1.3186098\n",
      " -1.9863871   2.6532955   2.0709038  -0.44499442  1.1410644  -1.9977982\n",
      "  3.0292907   3.7966125  -3.3948023  -1.1889541  -3.0714736   2.3496273\n",
      " -2.6288993   3.7311935  -4.277793    3.467017    3.2347875   2.0128965\n",
      " -4.0168424  -3.3952324   0.6621953   3.2866776  -1.4068488  -0.37303498\n",
      " -1.969379   -0.02366243  2.6913583   1.9048185   1.4900229   3.6248085\n",
      "  5.1554775   4.3432117   1.122215    5.5826354   1.2623793   0.5097447\n",
      " -0.7163874   1.9075923  -0.7156864  -0.598586    4.7436876  -0.6780336\n",
      "  0.50797147 -2.3601375  -1.411537    5.2797384  -2.2775202   2.7040942\n",
      " -0.9327177   5.1660175   0.5298864  -2.871028    6.6004486   0.06043774\n",
      " -1.0087829   2.314619   -8.490975    0.65564185  0.08034915 -1.8744208\n",
      " -1.340661    3.9049559   2.6682553  -2.9101653   3.3522274   7.4489713\n",
      " -2.4132447   0.9705874   1.3886019   0.599941   -0.7818905   3.4701912\n",
      "  4.4904246  -3.8627303   5.118557   -1.2698896   2.9748638   0.08187379\n",
      "  2.0651286  -2.3976824   3.0384939   2.755078   -4.7522836   0.79728156\n",
      " -4.785485   -3.3824747  -3.2314544   5.399737   -4.068802   -1.3460119\n",
      " -1.3903389   3.4053926   1.259242    2.3484674   0.571316    3.3621442\n",
      " -3.4612641   0.74964064  5.21113     0.91659003  2.1706917  -2.8290615\n",
      "  2.4676163   1.6404735   2.9110887   0.01441479 -2.4854708  -1.136586\n",
      "  2.778274   -1.2347221   3.8130352  -2.3884017  -1.3023429   0.5237943\n",
      "  0.8718899  -1.2806287   6.1741323   3.5710938  -0.77821237 -2.396023\n",
      " -1.2373894   1.2006588 ]\n",
      "\n",
      "Element 12, last row of 7:\n",
      "[-0.8882818  -3.348267    0.36098903  0.46327683  1.2810141  -1.3186098\n",
      " -1.9863871   2.6532955   2.0709038  -0.44499442  1.1410644  -1.9977982\n",
      "  3.0292907   3.7966125  -3.3948023  -1.1889541  -3.0714736   2.3496273\n",
      " -2.6288993   3.7311935  -4.277793    3.467017    3.2347875   2.0128965\n",
      " -4.0168424  -3.3952324   0.6621953   3.2866776  -1.4068488  -0.37303498\n",
      " -1.969379   -0.02366243  2.6913583   1.9048185   1.4900229   3.6248085\n",
      "  5.1554775   4.3432117   1.122215    5.5826354   1.2623793   0.5097447\n",
      " -0.7163874   1.9075923  -0.7156864  -0.598586    4.7436876  -0.6780336\n",
      "  0.50797147 -2.3601375  -1.411537    5.2797384  -2.2775202   2.7040942\n",
      " -0.9327177   5.1660175   0.5298864  -2.871028    6.6004486   0.06043774\n",
      " -1.0087829   2.314619   -8.490975    0.65564185  0.08034915 -1.8744208\n",
      " -1.340661    3.9049559   2.6682553  -2.9101653   3.3522274   7.4489713\n",
      " -2.4132447   0.9705874   1.3886019   0.599941   -0.7818905   3.4701912\n",
      "  4.4904246  -3.8627303   5.118557   -1.2698896   2.9748638   0.08187379\n",
      "  2.0651286  -2.3976824   3.0384939   2.755078   -4.7522836   0.79728156\n",
      " -4.785485   -3.3824747  -3.2314544   5.399737   -4.068802   -1.3460119\n",
      " -1.3903389   3.4053926   1.259242    2.3484674   0.571316    3.3621442\n",
      " -3.4612641   0.74964064  5.21113     0.91659003  2.1706917  -2.8290615\n",
      "  2.4676163   1.6404735   2.9110887   0.01441479 -2.4854708  -1.136586\n",
      "  2.778274   -1.2347221   3.8130352  -2.3884017  -1.3023429   0.5237943\n",
      "  0.8718899  -1.2806287   6.1741323   3.5710938  -0.77821237 -2.396023\n",
      " -1.2373894   1.2006588 ]\n",
      "\n",
      "Element 13, last row of 7:\n",
      "[-0.8882818  -3.348267    0.36098903  0.46327683  1.2810141  -1.3186098\n",
      " -1.9863871   2.6532955   2.0709038  -0.44499442  1.1410644  -1.9977982\n",
      "  3.0292907   3.7966125  -3.3948023  -1.1889541  -3.0714736   2.3496273\n",
      " -2.6288993   3.7311935  -4.277793    3.467017    3.2347875   2.0128965\n",
      " -4.0168424  -3.3952324   0.6621953   3.2866776  -1.4068488  -0.37303498\n",
      " -1.969379   -0.02366243  2.6913583   1.9048185   1.4900229   3.6248085\n",
      "  5.1554775   4.3432117   1.122215    5.5826354   1.2623793   0.5097447\n",
      " -0.7163874   1.9075923  -0.7156864  -0.598586    4.7436876  -0.6780336\n",
      "  0.50797147 -2.3601375  -1.411537    5.2797384  -2.2775202   2.7040942\n",
      " -0.9327177   5.1660175   0.5298864  -2.871028    6.6004486   0.06043774\n",
      " -1.0087829   2.314619   -8.490975    0.65564185  0.08034915 -1.8744208\n",
      " -1.340661    3.9049559   2.6682553  -2.9101653   3.3522274   7.4489713\n",
      " -2.4132447   0.9705874   1.3886019   0.599941   -0.7818905   3.4701912\n",
      "  4.4904246  -3.8627303   5.118557   -1.2698896   2.9748638   0.08187379\n",
      "  2.0651286  -2.3976824   3.0384939   2.755078   -4.7522836   0.79728156\n",
      " -4.785485   -3.3824747  -3.2314544   5.399737   -4.068802   -1.3460119\n",
      " -1.3903389   3.4053926   1.259242    2.3484674   0.571316    3.3621442\n",
      " -3.4612641   0.74964064  5.21113     0.91659003  2.1706917  -2.8290615\n",
      "  2.4676163   1.6404735   2.9110887   0.01441479 -2.4854708  -1.136586\n",
      "  2.778274   -1.2347221   3.8130352  -2.3884017  -1.3023429   0.5237943\n",
      "  0.8718899  -1.2806287   6.1741323   3.5710938  -0.77821237 -2.396023\n",
      " -1.2373894   1.2006588 ]\n",
      "\n",
      "Element 14, last row of 7:\n",
      "[-0.8882818  -3.348267    0.36098903  0.46327683  1.2810141  -1.3186098\n",
      " -1.9863871   2.6532955   2.0709038  -0.44499442  1.1410644  -1.9977982\n",
      "  3.0292907   3.7966125  -3.3948023  -1.1889541  -3.0714736   2.3496273\n",
      " -2.6288993   3.7311935  -4.277793    3.467017    3.2347875   2.0128965\n",
      " -4.0168424  -3.3952324   0.6621953   3.2866776  -1.4068488  -0.37303498\n",
      " -1.969379   -0.02366243  2.6913583   1.9048185   1.4900229   3.6248085\n",
      "  5.1554775   4.3432117   1.122215    5.5826354   1.2623793   0.5097447\n",
      " -0.7163874   1.9075923  -0.7156864  -0.598586    4.7436876  -0.6780336\n",
      "  0.50797147 -2.3601375  -1.411537    5.2797384  -2.2775202   2.7040942\n",
      " -0.9327177   5.1660175   0.5298864  -2.871028    6.6004486   0.06043774\n",
      " -1.0087829   2.314619   -8.490975    0.65564185  0.08034915 -1.8744208\n",
      " -1.340661    3.9049559   2.6682553  -2.9101653   3.3522274   7.4489713\n",
      " -2.4132447   0.9705874   1.3886019   0.599941   -0.7818905   3.4701912\n",
      "  4.4904246  -3.8627303   5.118557   -1.2698896   2.9748638   0.08187379\n",
      "  2.0651286  -2.3976824   3.0384939   2.755078   -4.7522836   0.79728156\n",
      " -4.785485   -3.3824747  -3.2314544   5.399737   -4.068802   -1.3460119\n",
      " -1.3903389   3.4053926   1.259242    2.3484674   0.571316    3.3621442\n",
      " -3.4612641   0.74964064  5.21113     0.91659003  2.1706917  -2.8290615\n",
      "  2.4676163   1.6404735   2.9110887   0.01441479 -2.4854708  -1.136586\n",
      "  2.778274   -1.2347221   3.8130352  -2.3884017  -1.3023429   0.5237943\n",
      "  0.8718899  -1.2806287   6.1741323   3.5710938  -0.77821237 -2.396023\n",
      " -1.2373894   1.2006588 ]\n",
      "\n",
      "Element 15, last row of 7:\n",
      "[-0.8882818  -3.348267    0.36098903  0.46327683  1.2810141  -1.3186098\n",
      " -1.9863871   2.6532955   2.0709038  -0.44499442  1.1410644  -1.9977982\n",
      "  3.0292907   3.7966125  -3.3948023  -1.1889541  -3.0714736   2.3496273\n",
      " -2.6288993   3.7311935  -4.277793    3.467017    3.2347875   2.0128965\n",
      " -4.0168424  -3.3952324   0.6621953   3.2866776  -1.4068488  -0.37303498\n",
      " -1.969379   -0.02366243  2.6913583   1.9048185   1.4900229   3.6248085\n",
      "  5.1554775   4.3432117   1.122215    5.5826354   1.2623793   0.5097447\n",
      " -0.7163874   1.9075923  -0.7156864  -0.598586    4.7436876  -0.6780336\n",
      "  0.50797147 -2.3601375  -1.411537    5.2797384  -2.2775202   2.7040942\n",
      " -0.9327177   5.1660175   0.5298864  -2.871028    6.6004486   0.06043774\n",
      " -1.0087829   2.314619   -8.490975    0.65564185  0.08034915 -1.8744208\n",
      " -1.340661    3.9049559   2.6682553  -2.9101653   3.3522274   7.4489713\n",
      " -2.4132447   0.9705874   1.3886019   0.599941   -0.7818905   3.4701912\n",
      "  4.4904246  -3.8627303   5.118557   -1.2698896   2.9748638   0.08187379\n",
      "  2.0651286  -2.3976824   3.0384939   2.755078   -4.7522836   0.79728156\n",
      " -4.785485   -3.3824747  -3.2314544   5.399737   -4.068802   -1.3460119\n",
      " -1.3903389   3.4053926   1.259242    2.3484674   0.571316    3.3621442\n",
      " -3.4612641   0.74964064  5.21113     0.91659003  2.1706917  -2.8290615\n",
      "  2.4676163   1.6404735   2.9110887   0.01441479 -2.4854708  -1.136586\n",
      "  2.778274   -1.2347221   3.8130352  -2.3884017  -1.3023429   0.5237943\n",
      "  0.8718899  -1.2806287   6.1741323   3.5710938  -0.77821237 -2.396023\n",
      " -1.2373894   1.2006588 ]\n",
      "\n",
      "Element 16, last row of 7:\n",
      "[-0.8882818  -3.348267    0.36098903  0.46327683  1.2810141  -1.3186098\n",
      " -1.9863871   2.6532955   2.0709038  -0.44499442  1.1410644  -1.9977982\n",
      "  3.0292907   3.7966125  -3.3948023  -1.1889541  -3.0714736   2.3496273\n",
      " -2.6288993   3.7311935  -4.277793    3.467017    3.2347875   2.0128965\n",
      " -4.0168424  -3.3952324   0.6621953   3.2866776  -1.4068488  -0.37303498\n",
      " -1.969379   -0.02366243  2.6913583   1.9048185   1.4900229   3.6248085\n",
      "  5.1554775   4.3432117   1.122215    5.5826354   1.2623793   0.5097447\n",
      " -0.7163874   1.9075923  -0.7156864  -0.598586    4.7436876  -0.6780336\n",
      "  0.50797147 -2.3601375  -1.411537    5.2797384  -2.2775202   2.7040942\n",
      " -0.9327177   5.1660175   0.5298864  -2.871028    6.6004486   0.06043774\n",
      " -1.0087829   2.314619   -8.490975    0.65564185  0.08034915 -1.8744208\n",
      " -1.340661    3.9049559   2.6682553  -2.9101653   3.3522274   7.4489713\n",
      " -2.4132447   0.9705874   1.3886019   0.599941   -0.7818905   3.4701912\n",
      "  4.4904246  -3.8627303   5.118557   -1.2698896   2.9748638   0.08187379\n",
      "  2.0651286  -2.3976824   3.0384939   2.755078   -4.7522836   0.79728156\n",
      " -4.785485   -3.3824747  -3.2314544   5.399737   -4.068802   -1.3460119\n",
      " -1.3903389   3.4053926   1.259242    2.3484674   0.571316    3.3621442\n",
      " -3.4612641   0.74964064  5.21113     0.91659003  2.1706917  -2.8290615\n",
      "  2.4676163   1.6404735   2.9110887   0.01441479 -2.4854708  -1.136586\n",
      "  2.778274   -1.2347221   3.8130352  -2.3884017  -1.3023429   0.5237943\n",
      "  0.8718899  -1.2806287   6.1741323   3.5710938  -0.77821237 -2.396023\n",
      " -1.2373894   1.2006588 ]\n",
      "\n",
      "Element 17, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 18, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 19, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Element 20, last row of 7:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in TestingDataloader:\n",
    "    print(batch['query'].shape)        # Shape: (batch_size, max_seq_length_query, feature_dim)\n",
    "    print(batch['relevant'].shape)     # Shape: (batch_size, max_seq_length_relevant, feature_dim)\n",
    "    print(batch['irrelevant'].shape) \n",
    "\n",
    "    for i in range(batch['query'].size(0)):\n",
    "        print(f\"Element {i+1}, last row of 7:\\n{batch['query'][i, -1, :].numpy()}\\n\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Batch: 1, Train Loss: 1.0460\n",
      "Epoch [1/1], Batch: 2, Train Loss: 0.9949\n",
      "Epoch [1/1], Batch: 3, Train Loss: 1.0174\n",
      "Epoch [1/1], Batch: 4, Train Loss: 1.0061\n",
      "Epoch [1/1], Batch: 5, Train Loss: 1.0147\n",
      "Epoch [1/1], Batch: 6, Train Loss: 1.0037\n",
      "Epoch [1/1], Batch: 7, Train Loss: 1.0045\n",
      "Epoch [1/1], Batch: 8, Train Loss: 1.0030\n",
      "Epoch [1/1], Batch: 9, Train Loss: 0.9893\n",
      "Epoch [1/1], Batch: 10, Train Loss: 1.0435\n",
      "Epoch [1/1], Batch: 11, Train Loss: 0.9585\n",
      "Epoch [1/1], Batch: 12, Train Loss: 0.9795\n",
      "Epoch [1/1], Batch: 13, Train Loss: 1.0259\n",
      "Epoch [1/1], Batch: 14, Train Loss: 1.0026\n",
      "Epoch [1/1], Batch: 15, Train Loss: 1.0158\n",
      "Epoch [1/1], Batch: 16, Train Loss: 1.0381\n",
      "Epoch [1/1], Batch: 17, Train Loss: 0.9921\n",
      "Epoch [1/1], Batch: 18, Train Loss: 0.9759\n",
      "Epoch [1/1], Batch: 19, Train Loss: 1.0445\n",
      "Epoch [1/1], Batch: 20, Train Loss: 0.9955\n",
      "Epoch [1/1], Batch: 21, Train Loss: 0.9629\n",
      "Epoch [1/1], Batch: 22, Train Loss: 0.9904\n",
      "Epoch [1/1], Batch: 23, Train Loss: 0.9831\n",
      "Epoch [1/1], Batch: 24, Train Loss: 1.0019\n",
      "Epoch [1/1], Batch: 25, Train Loss: 1.0353\n",
      "Epoch [1/1], Batch: 26, Train Loss: 1.0264\n",
      "Epoch [1/1], Batch: 27, Train Loss: 0.9836\n",
      "Epoch [1/1], Batch: 28, Train Loss: 0.9691\n",
      "Epoch [1/1], Batch: 29, Train Loss: 1.0352\n",
      "Epoch [1/1], Batch: 30, Train Loss: 1.0165\n",
      "Epoch [1/1], Batch: 31, Train Loss: 0.9939\n",
      "Epoch [1/1], Batch: 32, Train Loss: 1.0104\n",
      "Epoch [1/1], Batch: 33, Train Loss: 1.0005\n",
      "Epoch [1/1], Batch: 34, Train Loss: 0.9889\n",
      "Epoch [1/1], Batch: 35, Train Loss: 0.9881\n",
      "Epoch [1/1], Batch: 36, Train Loss: 1.0185\n",
      "Epoch [1/1], Batch: 37, Train Loss: 1.0172\n",
      "Epoch [1/1], Batch: 38, Train Loss: 0.9878\n",
      "Epoch [1/1], Batch: 39, Train Loss: 0.9816\n",
      "Epoch [1/1], Batch: 40, Train Loss: 0.9899\n",
      "Epoch [1/1], Batch: 41, Train Loss: 0.9815\n",
      "Epoch [1/1], Batch: 42, Train Loss: 1.0143\n",
      "Epoch [1/1], Batch: 43, Train Loss: 1.0464\n",
      "Epoch [1/1], Batch: 44, Train Loss: 0.9928\n",
      "Epoch [1/1], Batch: 45, Train Loss: 0.9877\n",
      "Epoch [1/1], Batch: 46, Train Loss: 1.0271\n",
      "Epoch [1/1], Batch: 47, Train Loss: 1.0318\n",
      "Epoch [1/1], Batch: 48, Train Loss: 0.9767\n",
      "Epoch [1/1], Batch: 49, Train Loss: 0.9996\n",
      "Epoch [1/1], Batch: 50, Train Loss: 1.0327\n",
      "Epoch [1/1], Batch: 51, Train Loss: 1.0581\n",
      "Epoch [1/1], Batch: 52, Train Loss: 1.0001\n",
      "Epoch [1/1], Batch: 53, Train Loss: 0.9869\n",
      "Epoch [1/1], Batch: 54, Train Loss: 0.9914\n",
      "Epoch [1/1], Batch: 55, Train Loss: 0.9847\n",
      "Epoch [1/1], Batch: 56, Train Loss: 0.9954\n",
      "Epoch [1/1], Batch: 57, Train Loss: 0.9973\n",
      "Epoch [1/1], Batch: 58, Train Loss: 1.0217\n",
      "Epoch [1/1], Batch: 59, Train Loss: 1.0000\n",
      "Epoch [1/1], Batch: 60, Train Loss: 1.0262\n",
      "Epoch [1/1], Batch: 61, Train Loss: 1.0186\n",
      "Epoch [1/1], Batch: 62, Train Loss: 0.9986\n",
      "Epoch [1/1], Batch: 63, Train Loss: 1.0312\n",
      "Epoch [1/1], Batch: 64, Train Loss: 1.0083\n",
      "Epoch [1/1], Batch: 65, Train Loss: 0.9802\n",
      "Epoch [1/1], Batch: 66, Train Loss: 1.0002\n",
      "Epoch [1/1], Batch: 67, Train Loss: 0.9337\n",
      "Epoch [1/1], Batch: 68, Train Loss: 0.9667\n",
      "Epoch [1/1], Batch: 69, Train Loss: 0.9914\n",
      "Epoch [1/1], Batch: 70, Train Loss: 0.9766\n",
      "Epoch [1/1], Batch: 71, Train Loss: 0.9968\n",
      "Epoch [1/1], Batch: 72, Train Loss: 1.0158\n",
      "Epoch [1/1], Batch: 73, Train Loss: 0.9846\n",
      "Epoch [1/1], Batch: 74, Train Loss: 0.9963\n",
      "Epoch [1/1], Batch: 75, Train Loss: 0.9755\n",
      "Epoch [1/1], Batch: 76, Train Loss: 1.0031\n",
      "Epoch [1/1], Batch: 77, Train Loss: 0.9850\n",
      "Epoch [1/1], Batch: 78, Train Loss: 1.0027\n",
      "Epoch [1/1], Batch: 79, Train Loss: 1.0144\n",
      "Epoch [1/1], Batch: 80, Train Loss: 0.9685\n",
      "Epoch [1/1], Batch: 81, Train Loss: 1.0048\n",
      "Epoch [1/1], Batch: 82, Train Loss: 1.0288\n",
      "Epoch [1/1], Batch: 83, Train Loss: 1.0471\n",
      "Epoch [1/1], Batch: 84, Train Loss: 1.0214\n",
      "Epoch [1/1], Batch: 85, Train Loss: 0.9758\n",
      "Epoch [1/1], Batch: 86, Train Loss: 1.0027\n",
      "Epoch [1/1], Batch: 87, Train Loss: 1.0490\n",
      "Epoch [1/1], Batch: 88, Train Loss: 0.9716\n",
      "Epoch [1/1], Batch: 89, Train Loss: 1.0658\n",
      "Epoch [1/1], Batch: 90, Train Loss: 1.0181\n",
      "Epoch [1/1], Batch: 91, Train Loss: 1.0097\n",
      "Epoch [1/1], Batch: 92, Train Loss: 1.0079\n",
      "Epoch [1/1], Batch: 93, Train Loss: 1.0307\n",
      "Epoch [1/1], Batch: 94, Train Loss: 0.9794\n",
      "Epoch [1/1], Batch: 95, Train Loss: 1.0331\n",
      "Epoch [1/1], Batch: 96, Train Loss: 1.0000\n",
      "Epoch [1/1], Batch: 97, Train Loss: 0.9800\n",
      "Epoch [1/1], Batch: 98, Train Loss: 0.9990\n",
      "Epoch [1/1], Batch: 99, Train Loss: 1.0384\n",
      "Epoch [1/1], Batch: 100, Train Loss: 1.0305\n",
      "Epoch [1/1], Batch: 101, Train Loss: 1.0150\n",
      "Epoch [1/1], Batch: 102, Train Loss: 1.0210\n",
      "Epoch [1/1], Batch: 103, Train Loss: 0.9636\n",
      "Epoch [1/1], Batch: 104, Train Loss: 0.9914\n",
      "Epoch [1/1], Batch: 105, Train Loss: 1.0285\n",
      "Epoch [1/1], Batch: 106, Train Loss: 1.0255\n",
      "Epoch [1/1], Batch: 107, Train Loss: 0.9890\n",
      "Epoch [1/1], Batch: 108, Train Loss: 0.9874\n",
      "Epoch [1/1], Batch: 109, Train Loss: 0.9726\n",
      "Epoch [1/1], Batch: 110, Train Loss: 1.0094\n",
      "Epoch [1/1], Batch: 111, Train Loss: 0.9881\n",
      "Epoch [1/1], Batch: 112, Train Loss: 1.0296\n",
      "Epoch [1/1], Batch: 113, Train Loss: 0.9668\n",
      "Epoch [1/1], Batch: 114, Train Loss: 0.9902\n",
      "Epoch [1/1], Batch: 115, Train Loss: 0.9971\n",
      "Epoch [1/1], Batch: 116, Train Loss: 1.0067\n",
      "Epoch [1/1], Batch: 117, Train Loss: 1.0155\n",
      "Epoch [1/1], Batch: 118, Train Loss: 1.0065\n",
      "Epoch [1/1], Batch: 119, Train Loss: 1.0380\n",
      "Epoch [1/1], Batch: 120, Train Loss: 1.0465\n",
      "Epoch [1/1], Batch: 121, Train Loss: 1.0126\n",
      "Epoch [1/1], Batch: 122, Train Loss: 1.0149\n",
      "Epoch [1/1], Batch: 123, Train Loss: 0.9833\n",
      "Epoch [1/1], Batch: 124, Train Loss: 1.0262\n",
      "Epoch [1/1], Batch: 125, Train Loss: 1.0074\n",
      "Epoch [1/1], Batch: 126, Train Loss: 0.9880\n",
      "Epoch [1/1], Batch: 127, Train Loss: 1.0175\n",
      "Epoch [1/1], Batch: 128, Train Loss: 0.9811\n",
      "Epoch [1/1], Batch: 129, Train Loss: 0.9872\n",
      "Epoch [1/1], Batch: 130, Train Loss: 1.0222\n",
      "Epoch [1/1], Batch: 131, Train Loss: 0.9446\n",
      "Epoch [1/1], Batch: 132, Train Loss: 1.0363\n",
      "Epoch [1/1], Batch: 133, Train Loss: 0.9691\n",
      "Epoch [1/1], Batch: 134, Train Loss: 0.9607\n",
      "Epoch [1/1], Batch: 135, Train Loss: 1.0222\n",
      "Epoch [1/1], Batch: 136, Train Loss: 0.9577\n",
      "Epoch [1/1], Batch: 137, Train Loss: 0.9898\n",
      "Epoch [1/1], Batch: 138, Train Loss: 0.9898\n",
      "Epoch [1/1], Batch: 139, Train Loss: 0.9799\n",
      "Epoch [1/1], Batch: 140, Train Loss: 0.9825\n",
      "Epoch [1/1], Batch: 141, Train Loss: 0.9767\n",
      "Epoch [1/1], Batch: 142, Train Loss: 1.0112\n",
      "Epoch [1/1], Batch: 143, Train Loss: 0.9932\n",
      "Epoch [1/1], Batch: 144, Train Loss: 1.0120\n",
      "Epoch [1/1], Batch: 145, Train Loss: 0.9889\n",
      "Epoch [1/1], Batch: 146, Train Loss: 1.0000\n",
      "Epoch [1/1], Batch: 147, Train Loss: 1.0188\n",
      "Epoch [1/1], Batch: 148, Train Loss: 1.0288\n",
      "Epoch [1/1], Batch: 149, Train Loss: 0.9984\n",
      "Epoch [1/1], Batch: 150, Train Loss: 1.0121\n",
      "Epoch [1/1], Batch: 151, Train Loss: 1.0061\n",
      "Epoch [1/1], Batch: 152, Train Loss: 0.9971\n",
      "Epoch [1/1], Batch: 153, Train Loss: 1.0356\n",
      "Epoch [1/1], Batch: 154, Train Loss: 1.0056\n",
      "Epoch [1/1], Batch: 155, Train Loss: 0.9803\n",
      "Epoch [1/1], Batch: 156, Train Loss: 0.9976\n",
      "Epoch [1/1], Batch: 157, Train Loss: 1.0247\n",
      "Epoch [1/1], Batch: 158, Train Loss: 0.9945\n",
      "Epoch [1/1], Batch: 159, Train Loss: 0.9912\n",
      "Epoch [1/1], Batch: 160, Train Loss: 0.9727\n",
      "Epoch [1/1], Batch: 161, Train Loss: 0.9893\n",
      "Epoch [1/1], Batch: 162, Train Loss: 0.9873\n",
      "Epoch [1/1], Batch: 163, Train Loss: 1.0022\n",
      "Epoch [1/1], Batch: 164, Train Loss: 1.0269\n",
      "Epoch [1/1], Batch: 165, Train Loss: 1.0172\n",
      "Epoch [1/1], Batch: 166, Train Loss: 1.0031\n",
      "Epoch [1/1], Batch: 167, Train Loss: 0.9505\n",
      "Epoch [1/1], Batch: 168, Train Loss: 1.0179\n",
      "Epoch [1/1], Batch: 169, Train Loss: 0.9951\n",
      "Epoch [1/1], Batch: 170, Train Loss: 0.9866\n",
      "Epoch [1/1], Batch: 171, Train Loss: 1.0189\n",
      "Epoch [1/1], Batch: 172, Train Loss: 1.0441\n",
      "Epoch [1/1], Batch: 173, Train Loss: 1.0098\n",
      "Epoch [1/1], Batch: 174, Train Loss: 0.9774\n",
      "Epoch [1/1], Batch: 175, Train Loss: 1.0204\n",
      "Epoch [1/1], Batch: 176, Train Loss: 0.9864\n",
      "Epoch [1/1], Batch: 177, Train Loss: 0.9707\n",
      "Epoch [1/1], Batch: 178, Train Loss: 0.9538\n",
      "Epoch [1/1], Batch: 179, Train Loss: 0.9833\n",
      "Epoch [1/1], Batch: 180, Train Loss: 1.0029\n",
      "Epoch [1/1], Batch: 181, Train Loss: 0.9896\n",
      "Epoch [1/1], Batch: 182, Train Loss: 0.9661\n",
      "Epoch [1/1], Batch: 183, Train Loss: 1.0130\n",
      "Epoch [1/1], Batch: 184, Train Loss: 0.9675\n",
      "Epoch [1/1], Batch: 185, Train Loss: 1.0204\n",
      "Epoch [1/1], Batch: 186, Train Loss: 1.0191\n",
      "Epoch [1/1], Batch: 187, Train Loss: 1.0165\n",
      "Epoch [1/1], Batch: 188, Train Loss: 0.9970\n",
      "Epoch [1/1], Batch: 189, Train Loss: 0.9851\n",
      "Epoch [1/1], Batch: 190, Train Loss: 0.9968\n",
      "Epoch [1/1], Batch: 191, Train Loss: 1.0449\n",
      "Epoch [1/1], Batch: 192, Train Loss: 0.9843\n",
      "Epoch [1/1], Batch: 193, Train Loss: 0.9996\n",
      "Epoch [1/1], Batch: 194, Train Loss: 0.9804\n",
      "Epoch [1/1], Batch: 195, Train Loss: 0.9652\n",
      "Epoch [1/1], Batch: 196, Train Loss: 0.9866\n",
      "Epoch [1/1], Batch: 197, Train Loss: 1.0063\n",
      "Epoch [1/1], Batch: 198, Train Loss: 0.9799\n",
      "Epoch [1/1], Batch: 199, Train Loss: 0.9983\n",
      "Epoch [1/1], Batch: 200, Train Loss: 0.9950\n",
      "Epoch [1/1], Batch: 201, Train Loss: 1.0031\n",
      "Epoch [1/1], Batch: 202, Train Loss: 0.9764\n",
      "Epoch [1/1], Batch: 203, Train Loss: 1.0217\n",
      "Epoch [1/1], Batch: 204, Train Loss: 0.9742\n",
      "Epoch [1/1], Batch: 205, Train Loss: 1.0276\n",
      "Epoch [1/1], Batch: 206, Train Loss: 0.9911\n",
      "Epoch [1/1], Batch: 207, Train Loss: 0.9794\n",
      "Epoch [1/1], Batch: 208, Train Loss: 0.9906\n",
      "Epoch [1/1], Batch: 209, Train Loss: 0.9568\n",
      "Epoch [1/1], Batch: 210, Train Loss: 0.9999\n",
      "Epoch [1/1], Batch: 211, Train Loss: 0.9884\n",
      "Epoch [1/1], Batch: 212, Train Loss: 0.9808\n",
      "Epoch [1/1], Batch: 213, Train Loss: 1.0031\n",
      "Epoch [1/1], Batch: 214, Train Loss: 1.0304\n",
      "Epoch [1/1], Batch: 215, Train Loss: 1.0186\n",
      "Epoch [1/1], Batch: 216, Train Loss: 1.0344\n",
      "Epoch [1/1], Batch: 217, Train Loss: 1.0221\n",
      "Epoch [1/1], Batch: 218, Train Loss: 0.9806\n",
      "Epoch [1/1], Batch: 219, Train Loss: 0.9860\n",
      "Epoch [1/1], Batch: 220, Train Loss: 0.9905\n",
      "Epoch [1/1], Batch: 221, Train Loss: 0.9295\n",
      "Epoch [1/1], Batch: 222, Train Loss: 0.9906\n",
      "Epoch [1/1], Batch: 223, Train Loss: 1.0123\n",
      "Epoch [1/1], Batch: 224, Train Loss: 0.9701\n",
      "Epoch [1/1], Batch: 225, Train Loss: 1.0010\n",
      "Epoch [1/1], Batch: 226, Train Loss: 0.9957\n",
      "Epoch [1/1], Batch: 227, Train Loss: 1.0030\n",
      "Epoch [1/1], Batch: 228, Train Loss: 0.9877\n",
      "Epoch [1/1], Batch: 229, Train Loss: 0.9952\n",
      "Epoch [1/1], Batch: 230, Train Loss: 1.0235\n",
      "Epoch [1/1], Batch: 231, Train Loss: 1.0017\n",
      "Epoch [1/1], Batch: 232, Train Loss: 1.0500\n",
      "Epoch [1/1], Batch: 233, Train Loss: 0.9460\n",
      "Epoch [1/1], Batch: 234, Train Loss: 0.9992\n",
      "Epoch [1/1], Batch: 235, Train Loss: 0.9708\n",
      "Epoch [1/1], Batch: 236, Train Loss: 0.9930\n",
      "Epoch [1/1], Batch: 237, Train Loss: 1.0053\n",
      "Epoch [1/1], Batch: 238, Train Loss: 0.9976\n",
      "Epoch [1/1], Batch: 239, Train Loss: 0.9803\n",
      "Epoch [1/1], Batch: 240, Train Loss: 1.0344\n",
      "Epoch [1/1], Batch: 241, Train Loss: 0.9704\n",
      "Epoch [1/1], Batch: 242, Train Loss: 1.0170\n",
      "Epoch [1/1], Batch: 243, Train Loss: 1.0105\n",
      "Epoch [1/1], Batch: 244, Train Loss: 0.9708\n",
      "Epoch [1/1], Batch: 245, Train Loss: 1.0238\n",
      "Epoch [1/1], Batch: 246, Train Loss: 0.9943\n",
      "Epoch [1/1], Batch: 247, Train Loss: 0.9598\n",
      "Epoch [1/1], Batch: 248, Train Loss: 0.9803\n",
      "Epoch [1/1], Batch: 249, Train Loss: 0.9458\n",
      "Epoch [1/1], Batch: 250, Train Loss: 0.9877\n",
      "Epoch [1/1], Batch: 251, Train Loss: 0.9755\n",
      "Epoch [1/1], Batch: 252, Train Loss: 0.9901\n",
      "Epoch [1/1], Batch: 253, Train Loss: 0.9370\n",
      "Epoch [1/1], Batch: 254, Train Loss: 0.9485\n",
      "Epoch [1/1], Batch: 255, Train Loss: 1.0665\n",
      "Epoch [1/1], Batch: 256, Train Loss: 0.9843\n",
      "Epoch [1/1], Batch: 257, Train Loss: 1.0132\n",
      "Epoch [1/1], Batch: 258, Train Loss: 0.9414\n",
      "Epoch [1/1], Batch: 259, Train Loss: 1.0024\n",
      "Epoch [1/1], Batch: 260, Train Loss: 0.9741\n",
      "Epoch [1/1], Batch: 261, Train Loss: 0.9630\n",
      "Epoch [1/1], Batch: 262, Train Loss: 1.0028\n",
      "Epoch [1/1], Batch: 263, Train Loss: 0.9891\n",
      "Epoch [1/1], Batch: 264, Train Loss: 1.0283\n",
      "Epoch [1/1], Batch: 265, Train Loss: 0.9943\n",
      "Epoch [1/1], Batch: 266, Train Loss: 1.0173\n",
      "Epoch [1/1], Batch: 267, Train Loss: 0.9944\n",
      "Epoch [1/1], Batch: 268, Train Loss: 0.9867\n",
      "Epoch [1/1], Batch: 269, Train Loss: 0.9664\n",
      "Epoch [1/1], Batch: 270, Train Loss: 1.0143\n",
      "Epoch [1/1], Batch: 271, Train Loss: 0.9935\n",
      "Epoch [1/1], Batch: 272, Train Loss: 1.0225\n",
      "Epoch [1/1], Batch: 273, Train Loss: 0.9942\n",
      "Epoch [1/1], Batch: 274, Train Loss: 0.9729\n",
      "Epoch [1/1], Batch: 275, Train Loss: 0.9989\n",
      "Epoch [1/1], Batch: 276, Train Loss: 1.0000\n",
      "Epoch [1/1], Batch: 277, Train Loss: 0.9849\n",
      "Epoch [1/1], Batch: 278, Train Loss: 0.9744\n",
      "Epoch [1/1], Batch: 279, Train Loss: 1.0308\n",
      "Epoch [1/1], Batch: 280, Train Loss: 0.9920\n",
      "Epoch [1/1], Batch: 281, Train Loss: 0.9915\n",
      "Epoch [1/1], Batch: 282, Train Loss: 1.0001\n",
      "Epoch [1/1], Batch: 283, Train Loss: 1.0134\n",
      "Epoch [1/1], Batch: 284, Train Loss: 0.9701\n",
      "Epoch [1/1], Batch: 285, Train Loss: 1.0159\n",
      "Epoch [1/1], Batch: 286, Train Loss: 0.9798\n",
      "Epoch [1/1], Batch: 287, Train Loss: 0.9993\n",
      "Epoch [1/1], Batch: 288, Train Loss: 1.0356\n",
      "Epoch [1/1], Batch: 289, Train Loss: 1.0164\n",
      "Epoch [1/1], Batch: 290, Train Loss: 0.9790\n",
      "Epoch [1/1], Batch: 291, Train Loss: 1.0124\n",
      "Epoch [1/1], Batch: 292, Train Loss: 1.0070\n",
      "Epoch [1/1], Batch: 293, Train Loss: 0.9748\n",
      "Epoch [1/1], Batch: 294, Train Loss: 1.0264\n",
      "Epoch [1/1], Batch: 295, Train Loss: 1.0061\n",
      "Epoch [1/1], Batch: 296, Train Loss: 0.9992\n",
      "Epoch [1/1], Batch: 297, Train Loss: 1.0021\n",
      "Epoch [1/1], Batch: 298, Train Loss: 1.0112\n",
      "Epoch [1/1], Batch: 299, Train Loss: 0.9994\n",
      "Epoch [1/1], Batch: 300, Train Loss: 0.9918\n",
      "Epoch [1/1], Batch: 301, Train Loss: 1.0193\n",
      "Epoch [1/1], Batch: 302, Train Loss: 1.0007\n",
      "Epoch [1/1], Batch: 303, Train Loss: 1.0122\n",
      "Epoch [1/1], Batch: 304, Train Loss: 1.0220\n",
      "Epoch [1/1], Batch: 305, Train Loss: 1.0039\n",
      "Epoch [1/1], Batch: 306, Train Loss: 0.9870\n",
      "Epoch [1/1], Batch: 307, Train Loss: 1.0090\n",
      "Epoch [1/1], Batch: 308, Train Loss: 1.0226\n",
      "Epoch [1/1], Batch: 309, Train Loss: 1.0177\n",
      "Epoch [1/1], Batch: 310, Train Loss: 1.0150\n",
      "Epoch [1/1], Batch: 311, Train Loss: 0.9959\n",
      "Epoch [1/1], Batch: 312, Train Loss: 1.0292\n",
      "Epoch [1/1], Batch: 313, Train Loss: 1.0301\n",
      "Epoch [1/1], Batch: 314, Train Loss: 1.0134\n",
      "Epoch [1/1], Batch: 315, Train Loss: 0.9724\n",
      "Epoch [1/1], Batch: 316, Train Loss: 0.9818\n",
      "Epoch [1/1], Batch: 317, Train Loss: 0.9888\n",
      "Epoch [1/1], Batch: 318, Train Loss: 1.0184\n",
      "Epoch [1/1], Batch: 319, Train Loss: 0.9792\n",
      "Epoch [1/1], Batch: 320, Train Loss: 1.0214\n",
      "Epoch [1/1], Batch: 321, Train Loss: 1.0083\n",
      "Epoch [1/1], Batch: 322, Train Loss: 0.9938\n",
      "Epoch [1/1], Batch: 323, Train Loss: 1.0170\n",
      "Epoch [1/1], Batch: 324, Train Loss: 0.9878\n",
      "Epoch [1/1], Batch: 325, Train Loss: 0.9793\n",
      "Epoch [1/1], Batch: 326, Train Loss: 0.9746\n",
      "Epoch [1/1], Batch: 327, Train Loss: 1.0012\n",
      "Epoch [1/1], Batch: 328, Train Loss: 1.0072\n",
      "Epoch [1/1], Batch: 329, Train Loss: 1.0258\n",
      "Epoch [1/1], Batch: 330, Train Loss: 0.9888\n",
      "Epoch [1/1], Batch: 331, Train Loss: 0.9847\n",
      "Epoch [1/1], Batch: 332, Train Loss: 0.9979\n",
      "Epoch [1/1], Batch: 333, Train Loss: 0.9939\n",
      "Epoch [1/1], Batch: 334, Train Loss: 0.9786\n",
      "Epoch [1/1], Batch: 335, Train Loss: 0.9740\n",
      "Epoch [1/1], Batch: 336, Train Loss: 0.9655\n",
      "Epoch [1/1], Batch: 337, Train Loss: 0.9362\n",
      "Epoch [1/1], Batch: 338, Train Loss: 0.9726\n",
      "Epoch [1/1], Batch: 339, Train Loss: 0.9633\n",
      "Epoch [1/1], Batch: 340, Train Loss: 0.9755\n",
      "Epoch [1/1], Batch: 341, Train Loss: 1.0127\n",
      "Epoch [1/1], Batch: 342, Train Loss: 0.9648\n",
      "Epoch [1/1], Batch: 343, Train Loss: 1.0169\n",
      "Epoch [1/1], Batch: 344, Train Loss: 1.0197\n",
      "Epoch [1/1], Batch: 345, Train Loss: 0.9795\n",
      "Epoch [1/1], Batch: 346, Train Loss: 0.9883\n",
      "Epoch [1/1], Batch: 347, Train Loss: 1.0008\n",
      "Epoch [1/1], Batch: 348, Train Loss: 1.0087\n",
      "Epoch [1/1], Batch: 349, Train Loss: 0.9852\n",
      "Epoch [1/1], Batch: 350, Train Loss: 1.0154\n",
      "Epoch [1/1], Batch: 351, Train Loss: 0.9996\n",
      "Epoch [1/1], Batch: 352, Train Loss: 1.0065\n",
      "Epoch [1/1], Batch: 353, Train Loss: 1.0394\n",
      "Epoch [1/1], Batch: 354, Train Loss: 1.0230\n",
      "Epoch [1/1], Batch: 355, Train Loss: 1.0198\n",
      "Epoch [1/1], Batch: 356, Train Loss: 0.9690\n",
      "Epoch [1/1], Batch: 357, Train Loss: 1.0092\n",
      "Epoch [1/1], Batch: 358, Train Loss: 1.0241\n",
      "Epoch [1/1], Batch: 359, Train Loss: 0.9754\n",
      "Epoch [1/1], Batch: 360, Train Loss: 1.0151\n",
      "Epoch [1/1], Batch: 361, Train Loss: 1.0346\n",
      "Epoch [1/1], Batch: 362, Train Loss: 1.0156\n",
      "Epoch [1/1], Batch: 363, Train Loss: 0.9611\n",
      "Epoch [1/1], Batch: 364, Train Loss: 0.9686\n",
      "Epoch [1/1], Batch: 365, Train Loss: 0.9895\n",
      "Epoch [1/1], Batch: 366, Train Loss: 1.0047\n",
      "Epoch [1/1], Batch: 367, Train Loss: 0.9803\n",
      "Epoch [1/1], Batch: 368, Train Loss: 0.9955\n",
      "Epoch [1/1], Batch: 369, Train Loss: 0.9811\n",
      "Epoch [1/1], Batch: 370, Train Loss: 0.9749\n",
      "Epoch [1/1], Batch: 371, Train Loss: 0.9293\n",
      "Epoch [1/1], Batch: 372, Train Loss: 0.9988\n",
      "Epoch [1/1], Batch: 373, Train Loss: 0.9970\n",
      "Epoch [1/1], Batch: 374, Train Loss: 1.0059\n",
      "Epoch [1/1], Batch: 375, Train Loss: 0.9558\n",
      "Epoch [1/1], Batch: 376, Train Loss: 1.0139\n",
      "Epoch [1/1], Batch: 377, Train Loss: 0.9988\n",
      "Epoch [1/1], Batch: 378, Train Loss: 0.9928\n",
      "Epoch [1/1], Batch: 379, Train Loss: 0.9822\n",
      "Epoch [1/1], Batch: 380, Train Loss: 1.0207\n",
      "Epoch [1/1], Batch: 381, Train Loss: 0.9720\n",
      "Epoch [1/1], Batch: 382, Train Loss: 0.9873\n",
      "Epoch [1/1], Batch: 383, Train Loss: 0.9963\n",
      "Epoch [1/1], Batch: 384, Train Loss: 0.9792\n",
      "Epoch [1/1], Batch: 385, Train Loss: 0.9938\n",
      "Epoch [1/1], Batch: 386, Train Loss: 0.9861\n",
      "Epoch [1/1], Batch: 387, Train Loss: 1.0244\n",
      "Epoch [1/1], Batch: 388, Train Loss: 1.0066\n",
      "Epoch [1/1], Batch: 389, Train Loss: 1.0213\n",
      "Epoch [1/1], Batch: 390, Train Loss: 0.9873\n",
      "Epoch [1/1], Batch: 391, Train Loss: 0.9955\n",
      "Epoch [1/1], Batch: 392, Train Loss: 0.9747\n",
      "Epoch [1/1], Batch: 393, Train Loss: 0.9738\n",
      "Epoch [1/1], Batch: 394, Train Loss: 1.0142\n",
      "Epoch [1/1], Batch: 395, Train Loss: 1.0630\n",
      "Epoch [1/1], Batch: 396, Train Loss: 1.0341\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    Towers.train() \n",
    "    train_loss = 0.0\n",
    "\n",
    "    i = 0\n",
    "    for batch in TestingDataloader:\n",
    "        i +=1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model to get embeddings\n",
    "        query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings = Towers(\n",
    "            batch['query'], \n",
    "            batch['relevant'], \n",
    "            batch['irrelevant']\n",
    "        )\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = triplet_loss_function_cosine(query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings, margin)\n",
    "\n",
    "        # print(query_embeddings)\n",
    "        # print(relevant_doc_embeddings)\n",
    "        # print(irrelevant_doc_embeddings)\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Batch: {i}, Train Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # break\n",
    "    \n",
    "    # # Testing phase\n",
    "    # Towers.eval()  # Set model to evaluation mode\n",
    "    # test_loss = 0.0\n",
    "    # with torch.no_grad():  # No need to track gradients for testing\n",
    "    #     for batch in TestingDataloader:\n",
    "    #         query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings = Towers(\n",
    "    #             batch['query'], batch['relevant_doc'], batch['irrelevant_doc']\n",
    "    #         )\n",
    "            \n",
    "    #         loss = triplet_loss_function_cosine(query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings, margin)\n",
    "    #         test_loss += loss.item()\n",
    "            \n",
    "    # avg_train_loss = train_loss / len(TestingDataloader)\n",
    "    # avg_test_loss = test_loss / len(TestingDataloader)\n",
    "    \n",
    "    # print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
