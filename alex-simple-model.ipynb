{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet('train.parquet')\n",
    "test = pd.read_parquet('test.parquet')\n",
    "validate = pd.read_parquet('validate.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect all texts to one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('AllTexts.txt', 'w') as f:\n",
    "#     pass  # This just creates the file, immediately closing it\n",
    "\n",
    "# with open('AllTexts.txt', 'a') as f:  # Open file in append mode\n",
    "#     for _, row in train.iterrows():\n",
    "#         concatenated = '\\n'.join(row['passages']['passage_text'])\n",
    "#         concatenated = '\\n'.join([concatenated, '\\n'.join(row['answers'])])\n",
    "#         concatenated = '\\n'.join([concatenated, row['query']])\n",
    "#         f.write(concatenated.lower() + '\\n')\n",
    "#     for _, row in test.iterrows():\n",
    "#         concatenated = '\\n'.join(row['passages']['passage_text'])\n",
    "#         concatenated = '\\n'.join([concatenated, '\\n'.join(row['answers'])])\n",
    "#         concatenated = '\\n'.join([concatenated, row['query']])\n",
    "#         f.write(concatenated.lower() + '\\n')\n",
    "#     for _, row in validate.iterrows():\n",
    "#         concatenated = '\\n'.join(row['passages']['passage_text'])\n",
    "#         concatenated = '\\n'.join([concatenated, '\\n'.join(row['answers'])])\n",
    "#         concatenated = '\\n'.join([concatenated, row['query']])\n",
    "#         f.write(concatenated.lower() + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train tokanizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spm.SentencePieceTrainer.train(\n",
    "#     input = 'AllTexts.txt',\n",
    "#     model_prefix='spm_AllTexts', \n",
    "#     vocab_size=30000,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokanizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('spm_AllTexts.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_file(file_path, sp_processor):\n",
    "    tokenized_sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Assuming each line in the file is a separate sentence or paragraph\n",
    "            # Tokenize the line and add the list of tokens to the tokenized_sentences list\n",
    "            tokenized_sentences.append(sp_processor.encode_as_pieces(line.strip()))\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokinize all the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokinized_sentences = tokenize_file(\"AllTexts.txt\", sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export tokens to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"Tokens_AllText.json\", 'w', encoding='utf-8') as file:\n",
    "#     json.dump(tokinized_sentences, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    min_count  =20,\n",
    "    window     =10,\n",
    "    vector_size=vector_size,\n",
    "    sample     =6e-5, \n",
    "    alpha      = 0.03, \n",
    "    min_alpha  = 0.0007, \n",
    "    negative   = 20,\n",
    "    workers    = multiprocessing.cpu_count() - 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(tokinized_sentences))\n",
    "# w2v_model.build_vocab(tokinized_sentences)\n",
    "# w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"word2vec_vocab.txt\", 'w') as vocab_file:\n",
    "#     for word in w2v_model.wv.key_to_index.keys():\n",
    "#         vocab_file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model.train(tokinized_sentences, total_examples=w2v_model.corpus_count, epochs=20, report_delay=1)\n",
    "# w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('▁cyber', 0.5855585336685181), ('▁hack', 0.5675662159919739), ('▁malicious', 0.5665103197097778), ('▁malware', 0.5581293106079102)]\n",
      "[('▁cyber', 0.5855585336685181), ('▁hack', 0.5675662159919739), ('▁malicious', 0.5665103197097778), ('▁malware', 0.5581293106079102), ('▁scam', 0.5384910702705383), ('▁spyware', 0.5197509527206421), ('▁legitimate', 0.5031879544258118), ('▁adware', 0.4846465289592743), ('▁pretend', 0.469952255487442), ('▁insider', 0.46461066603660583)]\n"
     ]
    }
   ],
   "source": [
    "similar_words = w2v_model.wv.most_similar('▁hacker', topn=4)\n",
    "print(similar_words)\n",
    "print(w2v_model.wv.most_similar(sp.encode_as_pieces('Hacker'.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_w2v_embedding(sp, text):\n",
    "#     tokens = sp.encode_as_pieces(text.lower())\n",
    "\n",
    "#     embeddings = []\n",
    "#     for token in tokens:\n",
    "#         if (token in w2v_model.wv): \n",
    "#             embeddings.append(w2v_model.wv[token])\n",
    "\n",
    "#     return np.stack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triples for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTriplesTokens(dataframe):\n",
    "    triples = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        available_indices = list(dataframe.index)\n",
    "        available_indices.remove(index)\n",
    "        \n",
    "        for relevant in row['passages']['passage_text']:\n",
    "            random_index = np.random.choice(available_indices)\n",
    "            random_doc_index = np.random.choice(\n",
    "                list(\n",
    "                    range(\n",
    "                        len(dataframe.iloc[random_index]['passages']['passage_text'])\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "            irrelevant = dataframe.iloc[random_index]['passages']['passage_text'][random_doc_index]\n",
    "\n",
    "            triples.append([\n",
    "                row['query'],\n",
    "                relevant,\n",
    "                irrelevant,\n",
    "            ])\n",
    "\n",
    "    return triples\n",
    "\n",
    "# train_triplets = prepareTriplesTokens(train)\n",
    "# test_triplets = prepareTriplesTokens(test)\n",
    "# validate_triplets = prepareTriplesTokens(validate)\n",
    "\n",
    "train_triplets = pd.read_parquet('train_triplets.parquet')\n",
    "test_triplets = pd.read_parquet('test_triplets.parquet')\n",
    "validate_triplets = pd.read_parquet('validate_triplets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         query                                           relevant  \\\n",
      "0  what is rba  Since 2007, the RBA's outstanding reputation h...   \n",
      "1  what is rba  The Reserve Bank of Australia (RBA) came into ...   \n",
      "2  what is rba  RBA Recognized with the 2014 Microsoft US Regi...   \n",
      "\n",
      "                                          irrelevant  \n",
      "0  This report describes the typical weather at t...  \n",
      "1  1. district, community the vicar of a small pa...  \n",
      "2  They have tried to make Panda Express prices c...  \n"
     ]
    }
   ],
   "source": [
    "print(train_triplets[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the list of triples to a DataFrame\n",
    "# columns = ['query', 'relevant', 'irrelevant']\n",
    "# train_triplets = pd.DataFrame(train_triplets, columns=columns)\n",
    "# test_triplets = pd.DataFrame(test_triplets, columns=columns)\n",
    "# validate_triplets = pd.DataFrame(validate_triplets, columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_triplets_embeddings = pd.DataFrame()\n",
    "\n",
    "# test_triplets_embeddings['query_embeddings'] = test_triplets['query'].apply(lambda x: to_w2v_embedding(sp, x))\n",
    "# test_triplets_embeddings['relevant_embeddings'] = test_triplets['relevant'].apply(lambda x: to_w2v_embedding(sp, x))\n",
    "# test_triplets_embeddings['irrelevant_embeddings'] = test_triplets['irrelevant'].apply(lambda x: to_w2v_embedding(sp, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total DataFrame size: 134.55 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the memory usage of each column in bytes, then sum them up, and convert to megabytes\n",
    "# total_memory_mb = test_triplets_embeddings.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "\n",
    "# print(f'Total DataFrame size: {total_memory_mb:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_triplets_embeddings = pd.DataFrame()\n",
    "# train_triplets_embeddings['query_embeddings'] = train_triplets['query'].apply(lambda x: to_w2v_embedding(sp, x))\n",
    "# train_triplets_embeddings['relevant_embeddings'] = train_triplets['relevant'].apply(lambda x: to_w2v_embedding(sp, x))\n",
    "# train_triplets_embeddings['irrelevant_embeddings'] = train_triplets['irrelevant'].apply(lambda x: to_w2v_embedding(sp, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total DataFrame size: 1150.24 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the memory usage of each column in bytes, then sum them up, and convert to megabytes\n",
    "# total_memory_mb = train_triplets_embeddings.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "\n",
    "# print(f'Total DataFrame size: {total_memory_mb:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [[-0.38117662, -1.751613, 0.118526, -4.4650145...\n",
      "Name: query_embeddings, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print(test_triplets_embeddings['query_embeddings'].head(1))\n",
    "\n",
    "# train_triplets_embeddings.to_parquet('train_triplets_with_embedings.parquet', engine='pyarrow') \n",
    "# test_triplets_embeddings.to_parquet('test_triplets_with_embedings.parquet', engine='pyarrow') \n",
    "# validate_triplets.to_parquet('validate_triplets_with_embedings.parquet', engine='pyarrow') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryDocsDataset(Dataset):\n",
    "    def __init__(self, sp, queries, relevant_docs, irrelevant_docs, device):\n",
    "        self.queries = queries\n",
    "        self.relevant_docs = relevant_docs\n",
    "        self.irrelevant_docs = irrelevant_docs\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "    \n",
    "    def to_w2v_embedding(self, sp, text):\n",
    "        tokens = sp.encode_as_pieces(text.lower())\n",
    "\n",
    "        embeddings = []\n",
    "        for token in tokens:\n",
    "            if (token in w2v_model.wv): \n",
    "                embeddings.append(w2v_model.wv[token])\n",
    "\n",
    "        return np.stack(embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'query': torch.tensor(self.to_w2v_embedding(sp, self.queries[idx].lower()), dtype=torch.float, device=self.device),\n",
    "            'relevant': torch.tensor(self.to_w2v_embedding(sp, self.relevant_docs[idx].lower()), dtype=torch.float, device=self.device),\n",
    "            'irrelevant': torch.tensor(self.to_w2v_embedding(sp, self.irrelevant_docs[idx].lower()), dtype=torch.float, device=self.device),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingDataset = QueryDocsDataset(sp, train_triplets['query'], train_triplets['relevant'], train_triplets['irrelevant'], device)\n",
    "TestingDataset = QueryDocsDataset(sp, test_triplets['query'], test_triplets['relevant'], test_triplets['irrelevant'], device)\n",
    "ValidationDataset = QueryDocsDataset(sp, validate_triplets['query'], validate_triplets['relevant'], validate_triplets['irrelevant'], device)\n",
    "\n",
    "# TrainingDataset = QueryDocsDataset(sp, train_triplets_embeddings['query_embeddings'], train_triplets_embeddings['relevant_embeddings'], train_triplets_embeddings['irrelevant_embeddings'])\n",
    "# TestingDataset = QueryDocsDataset(sp, test_triplets_embeddings['query_embeddings'], test_triplets_embeddings['relevant_embeddings'], test_triplets_embeddings['irrelevant_embeddings'])\n",
    "# ValidationDataset = QueryDocsDataset(sp, validate_triplets_embeddings['query'], validate_triplets_embeddings['relevant'], validate_triplets_embeddings['irrelevant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, device):\n",
    "        super(QueryRNNCell, self).__init__()\n",
    "        self.device = device\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.weight_ih = nn.Parameter(torch.randn(input_size, hidden_size, device=self.device))  # Input to hidden weights\n",
    "        self.weight_hh = nn.Parameter(torch.randn(hidden_size, hidden_size, device=self.device))  # Hidden to hidden weights\n",
    "        \n",
    "        self.bias_hh = nn.Parameter(torch.randn(hidden_size, device=self.device))  # Bias\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        return torch.tanh(\n",
    "            torch.mm(input, self.weight_ih) + torch.mm(hidden, self.weight_hh) + self.bias_hh\n",
    "        )\n",
    "    \n",
    "class QueryRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, device):\n",
    "        super(QueryRNN, self).__init__()\n",
    "        self.device = device\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_cell = QueryRNNCell(input_size, hidden_size, device)\n",
    "        self.rnn_cell.to(device)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Assuming input is of shape (batch, seq_len, input_size)\n",
    "        batch_size, seq_len, _ = input.shape\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size, device=self.device)  # Initial hidden state\n",
    "        for i in range(seq_len):\n",
    "            \n",
    "            mask = torch.any(input[:, i, :] != 0, dim=1).float().unsqueeze(1)  # Shape: (batch_size, 1)\n",
    "            current_input = input[:, i, :]  \n",
    "            \n",
    "            current_hidden = self.rnn_cell(current_input, hidden)\n",
    "            \n",
    "            # Apply mask: Only update hidden state for non-padded inputs\n",
    "            hidden = mask * current_hidden + (1 - mask) * hidden\n",
    "            \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Towers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, device):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.queryEncoder = QueryRNN(embedding_size, hidden_size, device)\n",
    "        self.queryEncoder.to(device)\n",
    "        self.docEncoder = QueryRNN(embedding_size, hidden_size, device)\n",
    "        self.docEncoder.to(device)\n",
    "\n",
    "    def forward(self, query, relevant, irrelevant):\n",
    "        query_embedding = self.queryEncoder(query)\n",
    "        relevant_embedding = self.docEncoder(relevant)\n",
    "        irrelevant_embedding = self.docEncoder(irrelevant)\n",
    "        return query_embedding, relevant_embedding, irrelevant_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lose Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def triplet_loss_function_cosine(query, relevant_doc, irrelevant_doc, margin):\n",
    "    # Compute cosine similarity (the output ranges from -1 to 1)\n",
    "    relevant_similarity = F.cosine_similarity(query, relevant_doc)\n",
    "    irrelevant_similarity = F.cosine_similarity(query, irrelevant_doc)\n",
    "    \n",
    "    # Convert similarities to distances (ranges from 0 to 2)\n",
    "    relevant_distance = 1 - relevant_similarity\n",
    "    irrelevant_distance = 1 - irrelevant_similarity\n",
    "    \n",
    "    # Compute the triplet loss\n",
    "    triplet_loss = torch.clamp(margin + relevant_distance - irrelevant_distance, min=0)\n",
    "    return triplet_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Extract lists of tensors for 'query', 'relevant', and 'irrelevant' from the batch\n",
    "    query_tensors = [item['query'] for item in batch]\n",
    "    relevant_tensors = [item['relevant'] for item in batch]\n",
    "    irrelevant_tensors = [item['irrelevant'] for item in batch]\n",
    "    \n",
    "    # Pad sequences within each list to the same length\n",
    "    query_padded = pad_sequence(query_tensors, batch_first=True, padding_value=0)\n",
    "    relevant_padded = pad_sequence(relevant_tensors, batch_first=True, padding_value=0)\n",
    "    irrelevant_padded = pad_sequence(irrelevant_tensors, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Return a dictionary with padded sequences\n",
    "    return {\n",
    "        'query': query_padded,\n",
    "        'relevant': relevant_padded,\n",
    "        'irrelevant': irrelevant_padded\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## padding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[51, 92, 14, 71],\n",
      "         [60, 20, 82, 86],\n",
      "         [74, 74, 87, 99],\n",
      "         [23,  2, 21, 52],\n",
      "         [ 1, 87, 29, 37],\n",
      "         [ 0,  0,  0,  0]],\n",
      "\n",
      "        [[51, 92, 14, 71],\n",
      "         [60, 20, 82, 86],\n",
      "         [74, 74, 87, 99],\n",
      "         [23,  2, 21, 52],\n",
      "         [ 1, 87, 29, 37],\n",
      "         [60, 20, 82, 86]]])\n"
     ]
    }
   ],
   "source": [
    "# Convert the numpy array to a PyTorch tensor\n",
    "tensors =[ \n",
    "    torch.tensor([\n",
    "        [51, 92, 14, 71],\n",
    "        [60, 20, 82, 86],\n",
    "        [74, 74, 87, 99],\n",
    "        [23,  2, 21, 52],\n",
    "        [ 1, 87, 29, 37],\n",
    "    ]),\n",
    "    torch.tensor([\n",
    "        [51, 92, 14, 71],\n",
    "        [60, 20, 82, 86],\n",
    "        [74, 74, 87, 99],\n",
    "        [23,  2, 21, 52],\n",
    "        [ 1, 87, 29, 37],\n",
    "        [60, 20, 82, 86],\n",
    "    ]),\n",
    "]\n",
    "dump = pad_sequence(tensors, batch_first=True)\n",
    "\n",
    "print(dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 16\n",
    "margin = 1.0\n",
    "batch_size = 100\n",
    "num_epochs = 3\n",
    "\n",
    "# TrainingDataloader = DataLoader(TrainingDataset, batch_size, shuffle=False)\n",
    "# TestingDataloader = DataLoader(TestingDataset, batch_size, shuffle=False)\n",
    "\n",
    "TrainingDataloader = DataLoader(TrainingDataset, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "TestingDataloader = DataLoader(TestingDataset, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "ValidatingDataloader = DataLoader(ValidationDataset, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Instantiate the model\n",
    "Towers = TwoTowerModel(embedding_size, hidden_size, device)\n",
    "Towers.to(device)\n",
    "optimizer = torch.optim.Adam(Towers.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recheck the padding results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10, 128])\n",
      "torch.Size([100, 137, 128])\n",
      "torch.Size([100, 183, 128])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mirrelevant\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape) \n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElement \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, last row of 7:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "for batch in TestingDataloader:\n",
    "    print(batch['query'].shape)        # Shape: (batch_size, max_seq_length_query, feature_dim)\n",
    "    print(batch['relevant'].shape)     # Shape: (batch_size, max_seq_length_relevant, feature_dim)\n",
    "    print(batch['irrelevant'].shape) \n",
    "\n",
    "    for i in range(batch['query'].size(0)):\n",
    "        print(f\"Element {i+1}, last row of 7:\\n{batch['query'][i, -1, :].numpy()}\\n\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Batch: 10, Train Loss: 0.9865\n",
      "Epoch [1/3], Batch: 20, Train Loss: 0.9749\n",
      "Epoch [1/3], Batch: 30, Train Loss: 1.0429\n",
      "Epoch [1/3], Batch: 40, Train Loss: 0.9906\n",
      "Epoch [1/3], Batch: 50, Train Loss: 0.9730\n",
      "Epoch [1/3], Batch: 60, Train Loss: 0.9829\n",
      "Epoch [1/3], Batch: 70, Train Loss: 0.9972\n",
      "Epoch [1/3], Batch: 80, Train Loss: 0.9318\n",
      "Epoch [1/3], Batch: 90, Train Loss: 0.9828\n",
      "Epoch [1/3], Batch: 100, Train Loss: 0.9839\n",
      "Epoch [1/3], Batch: 110, Train Loss: 0.9489\n",
      "Epoch [1/3], Batch: 120, Train Loss: 0.9902\n",
      "Epoch [1/3], Batch: 130, Train Loss: 0.9769\n",
      "Epoch [1/3], Batch: 140, Train Loss: 0.9868\n",
      "Epoch [1/3], Batch: 150, Train Loss: 0.9703\n",
      "Epoch [1/3], Batch: 160, Train Loss: 1.0066\n",
      "Epoch [1/3], Batch: 170, Train Loss: 0.9691\n",
      "Epoch [1/3], Batch: 180, Train Loss: 1.0342\n",
      "Epoch [1/3], Batch: 190, Train Loss: 1.0023\n",
      "Epoch [1/3], Batch: 200, Train Loss: 0.9425\n",
      "Epoch [1/3], Batch: 210, Train Loss: 0.9934\n",
      "Epoch [1/3], Batch: 220, Train Loss: 0.9595\n",
      "Epoch [1/3], Batch: 230, Train Loss: 0.9695\n",
      "Epoch [1/3], Batch: 240, Train Loss: 1.0559\n",
      "Epoch [1/3], Batch: 250, Train Loss: 1.0345\n",
      "Epoch [1/3], Batch: 260, Train Loss: 0.9788\n",
      "Epoch [1/3], Batch: 270, Train Loss: 0.9901\n",
      "Epoch [1/3], Batch: 280, Train Loss: 0.9974\n",
      "Epoch [1/3], Batch: 290, Train Loss: 0.9774\n",
      "Epoch [1/3], Batch: 300, Train Loss: 1.0188\n",
      "Epoch [1/3], Batch: 310, Train Loss: 0.9697\n",
      "Epoch [1/3], Batch: 320, Train Loss: 0.9736\n",
      "Epoch [1/3], Batch: 330, Train Loss: 1.0125\n",
      "Epoch [1/3], Batch: 340, Train Loss: 1.0093\n",
      "Epoch [1/3], Batch: 350, Train Loss: 1.0334\n",
      "Epoch [1/3], Batch: 360, Train Loss: 1.0513\n",
      "Epoch [1/3], Batch: 370, Train Loss: 0.9731\n",
      "Epoch [1/3], Batch: 380, Train Loss: 1.0006\n",
      "Epoch [1/3], Batch: 390, Train Loss: 0.9342\n",
      "Epoch [1/3], Batch: 400, Train Loss: 0.9704\n",
      "Epoch [1/3], Batch: 410, Train Loss: 1.0121\n",
      "Epoch [1/3], Batch: 420, Train Loss: 1.0607\n",
      "Epoch [1/3], Batch: 430, Train Loss: 0.9546\n",
      "Epoch [1/3], Batch: 440, Train Loss: 1.0026\n",
      "Epoch [1/3], Batch: 450, Train Loss: 1.0457\n",
      "Epoch [1/3], Batch: 460, Train Loss: 1.0082\n",
      "Epoch [1/3], Batch: 470, Train Loss: 1.0385\n",
      "Epoch [1/3], Batch: 480, Train Loss: 0.9974\n",
      "Epoch [1/3], Batch: 490, Train Loss: 0.9804\n",
      "Epoch [1/3], Batch: 500, Train Loss: 0.9749\n",
      "Epoch [1/3], Batch: 510, Train Loss: 1.0442\n",
      "Epoch [1/3], Batch: 520, Train Loss: 1.0173\n",
      "Epoch [1/3], Batch: 530, Train Loss: 1.0181\n",
      "Epoch [1/3], Batch: 540, Train Loss: 0.9996\n",
      "Epoch [1/3], Batch: 550, Train Loss: 1.0191\n",
      "Epoch [1/3], Batch: 560, Train Loss: 1.0048\n",
      "Epoch [1/3], Batch: 570, Train Loss: 0.9531\n",
      "Epoch [1/3], Batch: 580, Train Loss: 1.0074\n",
      "Epoch [1/3], Batch: 590, Train Loss: 1.0030\n",
      "Epoch [1/3], Batch: 600, Train Loss: 1.0212\n",
      "Epoch [1/3], Batch: 610, Train Loss: 0.9918\n",
      "Epoch [1/3], Batch: 620, Train Loss: 1.0471\n",
      "Epoch [1/3], Batch: 630, Train Loss: 1.0175\n",
      "Epoch [1/3], Batch: 640, Train Loss: 0.9661\n",
      "Epoch [1/3], Batch: 650, Train Loss: 1.0476\n",
      "Epoch [1/3], Batch: 660, Train Loss: 0.9940\n",
      "Epoch [1/3], Batch: 670, Train Loss: 1.0131\n",
      "Epoch [1/3], Batch: 680, Train Loss: 1.0098\n",
      "Epoch [1/3], Batch: 690, Train Loss: 0.9637\n",
      "Epoch [1/3], Batch: 700, Train Loss: 1.0128\n",
      "Epoch [1/3], Batch: 710, Train Loss: 0.9831\n",
      "Epoch [1/3], Batch: 720, Train Loss: 1.0109\n",
      "Epoch [1/3], Batch: 730, Train Loss: 0.9621\n",
      "Epoch [1/3], Batch: 740, Train Loss: 0.9941\n",
      "Epoch [1/3], Batch: 750, Train Loss: 1.0102\n",
      "Epoch [1/3], Batch: 760, Train Loss: 0.9968\n",
      "Epoch [1/3], Batch: 770, Train Loss: 1.0425\n",
      "Epoch [1/3], Batch: 780, Train Loss: 1.0195\n",
      "Epoch [1/3], Batch: 790, Train Loss: 0.9853\n",
      "Epoch [1/3], Batch: 800, Train Loss: 1.0110\n",
      "Epoch [1/3], Batch: 810, Train Loss: 1.0509\n",
      "Epoch [1/3], Batch: 820, Train Loss: 0.9538\n",
      "Epoch [1/3], Batch: 830, Train Loss: 1.0274\n",
      "Epoch [1/3], Batch: 840, Train Loss: 1.0037\n",
      "Epoch [1/3], Batch: 850, Train Loss: 1.0327\n",
      "Epoch [1/3], Batch: 860, Train Loss: 0.9730\n",
      "Epoch [1/3], Batch: 870, Train Loss: 0.9754\n",
      "Epoch [1/3], Batch: 880, Train Loss: 1.0182\n",
      "Epoch [1/3], Batch: 890, Train Loss: 1.0119\n",
      "Epoch [1/3], Batch: 900, Train Loss: 0.9669\n",
      "Epoch [1/3], Batch: 910, Train Loss: 1.0080\n",
      "Epoch [1/3], Batch: 920, Train Loss: 1.0186\n",
      "Epoch [1/3], Batch: 930, Train Loss: 1.0120\n",
      "Epoch [1/3], Batch: 940, Train Loss: 0.9494\n",
      "Epoch [1/3], Batch: 950, Train Loss: 1.0515\n",
      "Epoch [1/3], Batch: 960, Train Loss: 0.9686\n",
      "Epoch [1/3], Batch: 970, Train Loss: 1.0003\n",
      "Epoch [1/3], Batch: 980, Train Loss: 0.9747\n",
      "Epoch [1/3], Batch: 990, Train Loss: 0.9299\n",
      "Epoch [1/3], Batch: 1000, Train Loss: 1.0738\n",
      "Epoch [1/3], Batch: 1010, Train Loss: 0.9952\n",
      "Epoch [1/3], Batch: 1020, Train Loss: 1.0170\n",
      "Epoch [1/3], Batch: 1030, Train Loss: 0.9478\n",
      "Epoch [1/3], Batch: 1040, Train Loss: 0.9415\n",
      "Epoch [1/3], Batch: 1050, Train Loss: 0.9867\n",
      "Epoch [1/3], Batch: 1060, Train Loss: 0.9788\n",
      "Epoch [1/3], Batch: 1070, Train Loss: 0.9573\n",
      "Epoch [1/3], Batch: 1080, Train Loss: 0.9776\n",
      "Epoch [1/3], Batch: 1090, Train Loss: 0.9602\n",
      "Epoch [1/3], Batch: 1100, Train Loss: 1.0710\n",
      "Epoch [1/3], Batch: 1110, Train Loss: 1.0411\n",
      "Epoch [1/3], Batch: 1120, Train Loss: 0.9952\n",
      "Epoch [1/3], Batch: 1130, Train Loss: 0.8915\n",
      "Epoch [1/3], Batch: 1140, Train Loss: 0.9516\n",
      "Epoch [1/3], Batch: 1150, Train Loss: 1.0349\n",
      "Epoch [1/3], Batch: 1160, Train Loss: 0.9659\n",
      "Epoch [1/3], Batch: 1170, Train Loss: 1.0373\n",
      "Epoch [1/3], Batch: 1180, Train Loss: 0.9976\n",
      "Epoch [1/3], Batch: 1190, Train Loss: 0.9325\n",
      "Epoch [1/3], Batch: 1200, Train Loss: 0.9704\n",
      "Epoch [1/3], Batch: 1210, Train Loss: 1.0132\n",
      "Epoch [1/3], Batch: 1220, Train Loss: 0.9689\n",
      "Epoch [1/3], Batch: 1230, Train Loss: 0.9786\n",
      "Epoch [1/3], Batch: 1240, Train Loss: 0.9837\n",
      "Epoch [1/3], Batch: 1250, Train Loss: 0.9966\n",
      "Epoch [1/3], Batch: 1260, Train Loss: 1.0119\n",
      "Epoch [1/3], Batch: 1270, Train Loss: 1.0146\n",
      "Epoch [1/3], Batch: 1280, Train Loss: 1.0172\n",
      "Epoch [1/3], Batch: 1290, Train Loss: 0.9404\n",
      "Epoch [1/3], Batch: 1300, Train Loss: 1.0854\n",
      "Epoch [1/3], Batch: 1310, Train Loss: 0.9862\n",
      "Epoch [1/3], Batch: 1320, Train Loss: 0.9572\n",
      "Epoch [1/3], Batch: 1330, Train Loss: 1.0036\n",
      "Epoch [1/3], Batch: 1340, Train Loss: 0.9778\n",
      "Epoch [1/3], Batch: 1350, Train Loss: 0.9681\n",
      "Epoch [1/3], Batch: 1360, Train Loss: 1.0083\n",
      "Epoch [1/3], Batch: 1370, Train Loss: 1.0183\n",
      "Epoch [1/3], Batch: 1380, Train Loss: 0.9891\n",
      "Epoch [1/3], Batch: 1390, Train Loss: 0.9773\n",
      "Epoch [1/3], Batch: 1400, Train Loss: 0.9797\n",
      "Epoch [1/3], Batch: 1410, Train Loss: 0.9826\n",
      "Epoch [1/3], Batch: 1420, Train Loss: 0.9623\n",
      "Epoch [1/3], Batch: 1430, Train Loss: 0.9997\n",
      "Epoch [1/3], Batch: 1440, Train Loss: 0.9681\n",
      "Epoch [1/3], Batch: 1450, Train Loss: 1.0092\n",
      "Epoch [1/3], Batch: 1460, Train Loss: 1.0284\n",
      "Epoch [1/3], Batch: 1470, Train Loss: 0.9814\n",
      "Epoch [1/3], Batch: 1480, Train Loss: 1.0246\n",
      "Epoch [1/3], Batch: 1490, Train Loss: 0.9975\n",
      "Epoch [1/3], Batch: 1500, Train Loss: 0.9968\n",
      "Epoch [1/3], Batch: 1510, Train Loss: 0.9512\n",
      "Epoch [1/3], Batch: 1520, Train Loss: 1.0439\n",
      "Epoch [1/3], Batch: 1530, Train Loss: 0.9842\n",
      "Epoch [1/3], Batch: 1540, Train Loss: 0.9623\n",
      "Epoch [1/3], Batch: 1550, Train Loss: 1.0090\n",
      "Epoch [1/3], Batch: 1560, Train Loss: 1.0006\n",
      "Epoch [1/3], Batch: 1570, Train Loss: 1.0431\n",
      "Epoch [1/3], Batch: 1580, Train Loss: 0.9974\n",
      "Epoch [1/3], Batch: 1590, Train Loss: 1.0002\n",
      "Epoch [1/3], Batch: 1600, Train Loss: 1.0246\n",
      "Epoch [1/3], Batch: 1610, Train Loss: 0.9677\n",
      "Epoch [1/3], Batch: 1620, Train Loss: 0.9625\n",
      "Epoch [1/3], Batch: 1630, Train Loss: 1.0010\n",
      "Epoch [1/3], Batch: 1640, Train Loss: 0.9344\n",
      "Epoch [1/3], Batch: 1650, Train Loss: 1.0317\n",
      "Epoch [1/3], Batch: 1660, Train Loss: 0.9362\n",
      "Epoch [1/3], Batch: 1670, Train Loss: 1.0349\n",
      "Epoch [1/3], Batch: 1680, Train Loss: 0.9666\n",
      "Epoch [1/3], Batch: 1690, Train Loss: 0.8963\n",
      "Epoch [1/3], Batch: 1700, Train Loss: 0.9943\n",
      "Epoch [1/3], Batch: 1710, Train Loss: 0.9838\n",
      "Epoch [1/3], Batch: 1720, Train Loss: 0.9954\n",
      "Epoch [1/3], Batch: 1730, Train Loss: 0.9607\n",
      "Epoch [1/3], Batch: 1740, Train Loss: 0.9994\n",
      "Epoch [1/3], Batch: 1750, Train Loss: 0.9962\n",
      "Epoch [1/3], Batch: 1760, Train Loss: 0.9652\n",
      "Epoch [1/3], Batch: 1770, Train Loss: 0.9778\n",
      "Epoch [1/3], Batch: 1780, Train Loss: 1.0211\n",
      "Epoch [1/3], Batch: 1790, Train Loss: 0.9856\n",
      "Epoch [1/3], Batch: 1800, Train Loss: 0.9552\n",
      "Epoch [1/3], Batch: 1810, Train Loss: 0.9965\n",
      "Epoch [1/3], Batch: 1820, Train Loss: 0.9723\n",
      "Epoch [1/3], Batch: 1830, Train Loss: 0.9778\n",
      "Epoch [1/3], Batch: 1840, Train Loss: 0.9388\n",
      "Epoch [1/3], Batch: 1850, Train Loss: 1.0590\n",
      "Epoch [1/3], Batch: 1860, Train Loss: 0.9676\n",
      "Epoch [1/3], Batch: 1870, Train Loss: 0.9279\n",
      "Epoch [1/3], Batch: 1880, Train Loss: 1.0441\n",
      "Epoch [1/3], Batch: 1890, Train Loss: 0.9596\n",
      "Epoch [1/3], Batch: 1900, Train Loss: 1.0058\n",
      "Epoch [1/3], Batch: 1910, Train Loss: 0.9565\n",
      "Epoch [1/3], Batch: 1920, Train Loss: 0.9545\n",
      "Epoch [1/3], Batch: 1930, Train Loss: 0.9609\n",
      "Epoch [1/3], Batch: 1940, Train Loss: 1.0541\n",
      "Epoch [1/3], Batch: 1950, Train Loss: 1.0110\n",
      "Epoch [1/3], Batch: 1960, Train Loss: 1.0111\n",
      "Epoch [1/3], Batch: 1970, Train Loss: 1.0318\n",
      "Epoch [1/3], Batch: 1980, Train Loss: 0.9995\n",
      "Epoch [1/3], Batch: 1990, Train Loss: 0.9718\n",
      "Epoch [1/3], Batch: 2000, Train Loss: 0.9903\n",
      "Epoch [1/3], Batch: 2010, Train Loss: 0.9879\n",
      "Epoch [1/3], Batch: 2020, Train Loss: 0.9976\n",
      "Epoch [1/3], Batch: 2030, Train Loss: 0.9525\n",
      "Epoch [1/3], Batch: 2040, Train Loss: 0.9836\n",
      "Epoch [1/3], Batch: 2050, Train Loss: 1.0151\n",
      "Epoch [1/3], Batch: 2060, Train Loss: 0.9216\n",
      "Epoch [1/3], Batch: 2070, Train Loss: 1.0237\n",
      "Epoch [1/3], Batch: 2080, Train Loss: 0.9767\n",
      "Epoch [1/3], Batch: 2090, Train Loss: 1.0524\n",
      "Epoch [1/3], Batch: 2100, Train Loss: 0.9546\n",
      "Epoch [1/3], Batch: 2110, Train Loss: 0.9582\n",
      "Epoch [1/3], Batch: 2120, Train Loss: 0.9512\n",
      "Epoch [1/3], Batch: 2130, Train Loss: 1.0140\n",
      "Epoch [1/3], Batch: 2140, Train Loss: 0.9284\n",
      "Epoch [1/3], Batch: 2150, Train Loss: 1.0360\n",
      "Epoch [1/3], Batch: 2160, Train Loss: 0.9855\n",
      "Epoch [1/3], Batch: 2170, Train Loss: 1.0199\n",
      "Epoch [1/3], Batch: 2180, Train Loss: 1.0040\n",
      "Epoch [1/3], Batch: 2190, Train Loss: 0.9596\n",
      "Epoch [1/3], Batch: 2200, Train Loss: 0.9623\n",
      "Epoch [1/3], Batch: 2210, Train Loss: 1.0349\n",
      "Epoch [1/3], Batch: 2220, Train Loss: 0.9932\n",
      "Epoch [1/3], Batch: 2230, Train Loss: 1.0460\n",
      "Epoch [1/3], Batch: 2240, Train Loss: 0.9708\n",
      "Epoch [1/3], Batch: 2250, Train Loss: 0.8899\n",
      "Epoch [1/3], Batch: 2260, Train Loss: 0.9857\n",
      "Epoch [1/3], Batch: 2270, Train Loss: 0.9677\n",
      "Epoch [1/3], Batch: 2280, Train Loss: 0.9034\n",
      "Epoch [1/3], Batch: 2290, Train Loss: 1.0501\n",
      "Epoch [1/3], Batch: 2300, Train Loss: 0.9646\n",
      "Epoch [1/3], Batch: 2310, Train Loss: 1.1015\n",
      "Epoch [1/3], Batch: 2320, Train Loss: 1.0302\n",
      "Epoch [1/3], Batch: 2330, Train Loss: 0.9432\n",
      "Epoch [1/3], Batch: 2340, Train Loss: 1.0000\n",
      "Epoch [1/3], Batch: 2350, Train Loss: 0.9628\n",
      "Epoch [1/3], Batch: 2360, Train Loss: 0.9670\n",
      "Epoch [1/3], Batch: 2370, Train Loss: 0.9953\n",
      "Epoch [1/3], Batch: 2380, Train Loss: 1.0132\n",
      "Epoch [1/3], Batch: 2390, Train Loss: 0.9200\n",
      "Epoch [1/3], Batch: 2400, Train Loss: 0.9270\n",
      "Epoch [1/3], Batch: 2410, Train Loss: 0.9757\n",
      "Epoch [1/3], Batch: 2420, Train Loss: 0.9615\n",
      "Epoch [1/3], Batch: 2430, Train Loss: 0.9725\n",
      "Epoch [1/3], Batch: 2440, Train Loss: 0.9987\n",
      "Epoch [1/3], Batch: 2450, Train Loss: 0.9928\n",
      "Epoch [1/3], Batch: 2460, Train Loss: 1.0158\n",
      "Epoch [1/3], Batch: 2470, Train Loss: 0.9968\n",
      "Epoch [1/3], Batch: 2480, Train Loss: 0.9609\n",
      "Epoch [1/3], Batch: 2490, Train Loss: 0.9737\n",
      "Epoch [1/3], Batch: 2500, Train Loss: 1.0015\n",
      "Epoch [1/3], Batch: 2510, Train Loss: 1.0108\n",
      "Epoch [1/3], Batch: 2520, Train Loss: 0.9544\n",
      "Epoch [1/3], Batch: 2530, Train Loss: 0.9638\n",
      "Epoch [1/3], Batch: 2540, Train Loss: 1.0223\n",
      "Epoch [1/3], Batch: 2550, Train Loss: 0.9198\n",
      "Epoch [1/3], Batch: 2560, Train Loss: 0.9788\n",
      "Epoch [1/3], Batch: 2570, Train Loss: 0.9566\n",
      "Epoch [1/3], Batch: 2580, Train Loss: 0.9699\n",
      "Epoch [1/3], Batch: 2590, Train Loss: 0.9431\n",
      "Epoch [1/3], Batch: 2600, Train Loss: 0.9790\n",
      "Epoch [1/3], Batch: 2610, Train Loss: 0.9483\n",
      "Epoch [1/3], Batch: 2620, Train Loss: 1.0051\n",
      "Epoch [1/3], Batch: 2630, Train Loss: 1.0664\n",
      "Epoch [1/3], Batch: 2640, Train Loss: 0.9432\n",
      "Epoch [1/3], Batch: 2650, Train Loss: 0.9806\n",
      "Epoch [1/3], Batch: 2660, Train Loss: 0.9527\n",
      "Epoch [1/3], Batch: 2670, Train Loss: 0.9420\n",
      "Epoch [1/3], Batch: 2680, Train Loss: 0.9312\n",
      "Epoch [1/3], Batch: 2690, Train Loss: 0.9993\n",
      "Epoch [1/3], Batch: 2700, Train Loss: 0.9594\n",
      "Epoch [1/3], Batch: 2710, Train Loss: 0.9960\n",
      "Epoch [1/3], Batch: 2720, Train Loss: 0.9703\n",
      "Epoch [1/3], Batch: 2730, Train Loss: 1.0100\n",
      "Epoch [1/3], Batch: 2740, Train Loss: 0.9924\n",
      "Epoch [1/3], Batch: 2750, Train Loss: 0.9420\n",
      "Epoch [1/3], Batch: 2760, Train Loss: 0.9415\n",
      "Epoch [1/3], Batch: 2770, Train Loss: 0.9755\n",
      "Epoch [1/3], Batch: 2780, Train Loss: 0.9601\n",
      "Epoch [1/3], Batch: 2790, Train Loss: 0.9728\n",
      "Epoch [1/3], Batch: 2800, Train Loss: 0.9736\n",
      "Epoch [1/3], Batch: 2810, Train Loss: 0.9907\n",
      "Epoch [1/3], Batch: 2820, Train Loss: 0.9919\n",
      "Epoch [1/3], Batch: 2830, Train Loss: 0.9788\n",
      "Epoch [1/3], Batch: 2840, Train Loss: 1.0121\n",
      "Epoch [1/3], Batch: 2850, Train Loss: 0.9796\n",
      "Epoch [1/3], Batch: 2860, Train Loss: 1.0523\n",
      "Epoch [1/3], Batch: 2870, Train Loss: 0.9884\n",
      "Epoch [1/3], Batch: 2880, Train Loss: 0.9319\n",
      "Epoch [1/3], Batch: 2890, Train Loss: 0.9484\n",
      "Epoch [1/3], Batch: 2900, Train Loss: 1.0172\n",
      "Epoch [1/3], Batch: 2910, Train Loss: 1.0282\n",
      "Epoch [1/3], Batch: 2920, Train Loss: 1.0020\n",
      "Epoch [1/3], Batch: 2930, Train Loss: 0.9596\n",
      "Epoch [1/3], Batch: 2940, Train Loss: 0.9878\n",
      "Epoch [1/3], Batch: 2950, Train Loss: 0.9388\n",
      "Epoch [1/3], Batch: 2960, Train Loss: 1.0013\n",
      "Epoch [1/3], Batch: 2970, Train Loss: 1.0286\n",
      "Epoch [1/3], Batch: 2980, Train Loss: 1.0112\n",
      "Epoch [1/3], Batch: 2990, Train Loss: 1.0055\n",
      "Epoch [1/3], Batch: 3000, Train Loss: 0.9885\n",
      "Epoch [1/3], Batch: 3010, Train Loss: 0.9577\n",
      "Epoch [1/3], Batch: 3020, Train Loss: 0.9105\n",
      "Epoch [1/3], Batch: 3030, Train Loss: 0.9418\n",
      "Epoch [1/3], Batch: 3040, Train Loss: 0.9762\n",
      "Epoch [1/3], Batch: 3050, Train Loss: 0.9836\n",
      "Epoch [1/3], Batch: 3060, Train Loss: 1.0271\n",
      "Epoch [1/3], Batch: 3070, Train Loss: 0.9833\n",
      "Epoch [1/3], Batch: 3080, Train Loss: 0.9577\n",
      "Epoch [1/3], Batch: 3090, Train Loss: 0.9808\n",
      "Epoch [1/3], Batch: 3100, Train Loss: 0.9466\n",
      "Epoch [1/3], Batch: 3110, Train Loss: 0.9886\n",
      "Epoch [1/3], Batch: 3120, Train Loss: 0.9316\n",
      "Epoch [1/3], Batch: 3130, Train Loss: 1.0027\n",
      "Epoch [1/3], Batch: 3140, Train Loss: 1.0066\n",
      "Epoch [1/3], Batch: 3150, Train Loss: 0.9269\n",
      "Epoch [1/3], Batch: 3160, Train Loss: 0.9998\n",
      "Epoch [1/3], Batch: 3170, Train Loss: 0.9032\n",
      "Epoch [1/3], Batch: 3180, Train Loss: 1.0178\n",
      "Epoch [1/3], Batch: 3190, Train Loss: 0.9957\n",
      "Epoch [1/3], Batch: 3200, Train Loss: 1.0297\n",
      "Epoch [1/3], Batch: 3210, Train Loss: 0.9140\n",
      "Epoch [1/3], Batch: 3220, Train Loss: 0.9821\n",
      "Epoch [1/3], Batch: 3230, Train Loss: 0.9691\n",
      "Epoch [1/3], Batch: 3240, Train Loss: 0.9269\n",
      "Epoch [1/3], Batch: 3250, Train Loss: 1.0023\n",
      "Epoch [1/3], Batch: 3260, Train Loss: 0.9864\n",
      "Epoch [1/3], Batch: 3270, Train Loss: 0.9404\n",
      "Epoch [1/3], Batch: 3280, Train Loss: 0.9779\n",
      "Epoch [1/3], Batch: 3290, Train Loss: 0.9704\n",
      "Epoch [1/3], Batch: 3300, Train Loss: 0.9165\n",
      "Epoch [1/3], Batch: 3310, Train Loss: 1.0406\n",
      "Epoch [1/3], Batch: 3320, Train Loss: 0.9463\n",
      "Epoch [1/3], Batch: 3330, Train Loss: 0.9603\n",
      "Epoch [1/3], Batch: 3340, Train Loss: 0.9599\n",
      "Epoch [1/3], Batch: 3350, Train Loss: 0.9077\n",
      "Epoch [1/3], Batch: 3360, Train Loss: 1.0008\n",
      "Epoch [1/3], Batch: 3370, Train Loss: 0.9710\n",
      "Epoch [1/3], Batch: 3380, Train Loss: 1.0237\n",
      "Epoch [1/3], Batch: 3390, Train Loss: 1.0039\n",
      "Epoch [1/3], Batch: 3400, Train Loss: 0.9448\n",
      "Epoch [1/3], Batch: 3410, Train Loss: 0.9905\n",
      "Epoch [1/3], Batch: 3420, Train Loss: 0.9577\n",
      "Epoch [1/3], Batch: 3430, Train Loss: 1.0478\n",
      "Epoch [1/3], Batch: 3440, Train Loss: 0.9598\n",
      "Epoch [1/3], Batch: 3450, Train Loss: 0.9927\n",
      "Epoch [1/3], Batch: 3460, Train Loss: 0.9279\n",
      "Epoch [1/3], Batch: 3470, Train Loss: 1.0075\n",
      "Epoch [1/3], Batch: 3480, Train Loss: 0.9916\n",
      "Epoch [1/3], Batch: 3490, Train Loss: 1.0076\n",
      "Epoch [1/3], Batch: 3500, Train Loss: 0.9498\n",
      "Epoch [1/3], Batch: 3510, Train Loss: 1.0215\n",
      "Epoch [1/3], Batch: 3520, Train Loss: 0.9527\n",
      "Epoch [1/3], Batch: 3530, Train Loss: 0.9873\n",
      "Epoch [1/3], Batch: 3540, Train Loss: 0.9853\n",
      "Epoch [1/3], Batch: 3550, Train Loss: 0.9913\n",
      "Epoch [1/3], Batch: 3560, Train Loss: 1.0394\n",
      "Epoch [1/3], Batch: 3570, Train Loss: 0.9635\n",
      "Epoch [1/3], Batch: 3580, Train Loss: 0.9331\n",
      "Epoch [1/3], Batch: 3590, Train Loss: 1.0571\n",
      "Epoch [1/3], Batch: 3600, Train Loss: 1.0141\n",
      "Epoch [1/3], Batch: 3610, Train Loss: 0.9588\n",
      "Epoch [1/3], Batch: 3620, Train Loss: 0.9136\n",
      "Epoch [1/3], Batch: 3630, Train Loss: 0.9899\n",
      "Epoch [1/3], Batch: 3640, Train Loss: 0.9732\n",
      "Epoch [1/3], Batch: 3650, Train Loss: 1.0179\n",
      "Epoch [1/3], Batch: 3660, Train Loss: 1.0465\n",
      "Epoch [1/3], Batch: 3670, Train Loss: 1.0013\n",
      "Epoch [1/3], Batch: 3680, Train Loss: 0.9508\n",
      "Epoch [1/3], Batch: 3690, Train Loss: 0.9394\n",
      "Epoch [1/3], Batch: 3700, Train Loss: 0.9638\n",
      "Epoch [1/3], Batch: 3710, Train Loss: 0.9766\n",
      "Epoch [1/3], Batch: 3720, Train Loss: 0.9460\n",
      "Epoch [1/3], Batch: 3730, Train Loss: 0.9532\n",
      "Epoch [1/3], Batch: 3740, Train Loss: 0.9640\n",
      "Epoch [1/3], Batch: 3750, Train Loss: 0.9476\n",
      "Epoch [1/3], Batch: 3760, Train Loss: 0.9391\n",
      "Epoch [1/3], Batch: 3770, Train Loss: 0.9594\n",
      "Epoch [1/3], Batch: 3780, Train Loss: 0.9534\n",
      "Epoch [1/3], Batch: 3790, Train Loss: 0.9354\n",
      "Epoch [1/3], Batch: 3800, Train Loss: 0.9451\n",
      "Epoch [1/3], Batch: 3810, Train Loss: 0.9929\n",
      "Epoch [1/3], Batch: 3820, Train Loss: 1.0146\n",
      "Epoch [1/3], Batch: 3830, Train Loss: 0.9773\n",
      "Epoch [1/3], Batch: 3840, Train Loss: 0.9699\n",
      "Epoch [1/3], Batch: 3850, Train Loss: 0.9496\n",
      "Epoch [1/3], Batch: 3860, Train Loss: 0.9730\n",
      "Epoch [1/3], Batch: 3870, Train Loss: 0.9807\n",
      "Epoch [1/3], Batch: 3880, Train Loss: 1.0288\n",
      "Epoch [1/3], Batch: 3890, Train Loss: 1.0157\n",
      "Epoch [1/3], Batch: 3900, Train Loss: 0.9380\n",
      "Epoch [1/3], Batch: 3910, Train Loss: 0.9691\n",
      "Epoch [1/3], Batch: 3920, Train Loss: 0.9902\n",
      "Epoch [1/3], Batch: 3930, Train Loss: 0.9512\n",
      "Epoch [1/3], Batch: 3940, Train Loss: 0.9372\n",
      "Epoch [1/3], Batch: 3950, Train Loss: 0.9919\n",
      "Epoch [1/3], Batch: 3960, Train Loss: 0.9494\n",
      "Epoch [1/3], Batch: 3970, Train Loss: 0.9724\n",
      "Epoch [1/3], Batch: 3980, Train Loss: 1.0305\n",
      "Epoch [1/3], Batch: 3990, Train Loss: 0.9710\n",
      "Epoch [1/3], Batch: 4000, Train Loss: 0.9888\n",
      "Epoch [1/3], Batch: 4010, Train Loss: 0.9354\n",
      "Epoch [1/3], Batch: 4020, Train Loss: 0.9999\n",
      "Epoch [1/3], Batch: 4030, Train Loss: 0.9315\n",
      "Epoch [1/3], Batch: 4040, Train Loss: 0.9821\n",
      "Epoch [1/3], Batch: 4050, Train Loss: 1.0115\n",
      "Epoch [1/3], Batch: 4060, Train Loss: 0.8938\n",
      "Epoch [1/3], Batch: 4070, Train Loss: 0.8580\n",
      "Epoch [1/3], Batch: 4080, Train Loss: 0.9309\n",
      "Epoch [1/3], Batch: 4090, Train Loss: 0.9507\n",
      "Epoch [1/3], Batch: 4100, Train Loss: 0.9830\n",
      "Epoch [1/3], Batch: 4110, Train Loss: 0.9434\n",
      "Epoch [1/3], Batch: 4120, Train Loss: 0.9715\n",
      "Epoch [1/3], Batch: 4130, Train Loss: 0.9939\n",
      "Epoch [1/3], Batch: 4140, Train Loss: 0.9840\n",
      "Epoch [1/3], Batch: 4150, Train Loss: 0.9217\n",
      "Epoch [1/3], Batch: 4160, Train Loss: 0.9374\n",
      "Epoch [1/3], Batch: 4170, Train Loss: 1.0672\n",
      "Epoch [1/3], Batch: 4180, Train Loss: 1.0352\n",
      "Epoch [1/3], Batch: 4190, Train Loss: 1.0089\n",
      "Epoch [1/3], Batch: 4200, Train Loss: 0.9136\n",
      "Epoch [1/3], Batch: 4210, Train Loss: 1.0078\n",
      "Epoch [1/3], Batch: 4220, Train Loss: 0.9757\n",
      "Epoch [1/3], Batch: 4230, Train Loss: 0.9816\n",
      "Epoch [1/3], Batch: 4240, Train Loss: 0.9830\n",
      "Epoch [1/3], Batch: 4250, Train Loss: 0.9918\n",
      "Epoch [1/3], Batch: 4260, Train Loss: 0.9210\n",
      "Epoch [1/3], Batch: 4270, Train Loss: 0.9822\n",
      "Epoch [1/3], Batch: 4280, Train Loss: 0.9185\n",
      "Epoch [1/3], Batch: 4290, Train Loss: 1.0030\n",
      "Epoch [1/3], Batch: 4300, Train Loss: 0.8963\n",
      "Epoch [1/3], Batch: 4310, Train Loss: 1.0016\n",
      "Epoch [1/3], Batch: 4320, Train Loss: 0.9549\n",
      "Epoch [1/3], Batch: 4330, Train Loss: 0.9940\n",
      "Epoch [1/3], Batch: 4340, Train Loss: 0.9362\n",
      "Epoch [1/3], Batch: 4350, Train Loss: 0.9178\n",
      "Epoch [1/3], Batch: 4360, Train Loss: 0.9152\n",
      "Epoch [1/3], Batch: 4370, Train Loss: 1.0668\n",
      "Epoch [1/3], Batch: 4380, Train Loss: 0.9640\n",
      "Epoch [1/3], Batch: 4390, Train Loss: 0.9938\n",
      "Epoch [1/3], Batch: 4400, Train Loss: 0.9239\n",
      "Epoch [1/3], Batch: 4410, Train Loss: 0.9820\n",
      "Epoch [1/3], Batch: 4420, Train Loss: 0.9989\n",
      "Epoch [1/3], Batch: 4430, Train Loss: 0.9740\n",
      "Epoch [1/3], Batch: 4440, Train Loss: 0.9513\n",
      "Epoch [1/3], Batch: 4450, Train Loss: 0.8911\n",
      "Epoch [1/3], Batch: 4460, Train Loss: 0.9801\n",
      "Epoch [1/3], Batch: 4470, Train Loss: 0.9755\n",
      "Epoch [1/3], Batch: 4480, Train Loss: 0.9313\n",
      "Epoch [1/3], Batch: 4490, Train Loss: 0.9221\n",
      "Epoch [1/3], Batch: 4500, Train Loss: 0.9825\n",
      "Epoch [1/3], Batch: 4510, Train Loss: 0.9668\n",
      "Epoch [1/3], Batch: 4520, Train Loss: 0.9919\n",
      "Epoch [1/3], Batch: 4530, Train Loss: 0.8557\n",
      "Epoch [1/3], Batch: 4540, Train Loss: 0.9833\n",
      "Epoch [1/3], Batch: 4550, Train Loss: 0.9292\n",
      "Epoch [1/3], Batch: 4560, Train Loss: 0.9116\n",
      "Epoch [1/3], Batch: 4570, Train Loss: 0.9615\n",
      "Epoch [1/3], Batch: 4580, Train Loss: 0.9725\n",
      "Epoch [1/3], Batch: 4590, Train Loss: 0.9950\n",
      "Epoch [1/3], Batch: 4600, Train Loss: 0.9400\n",
      "Epoch [1/3], Batch: 4610, Train Loss: 0.9681\n",
      "Epoch [1/3], Batch: 4620, Train Loss: 0.9365\n",
      "Epoch [1/3], Batch: 4630, Train Loss: 0.8937\n",
      "Epoch [1/3], Batch: 4640, Train Loss: 1.0608\n",
      "Epoch [1/3], Batch: 4650, Train Loss: 1.0122\n",
      "Epoch [1/3], Batch: 4660, Train Loss: 0.9401\n",
      "Epoch [1/3], Batch: 4670, Train Loss: 0.9579\n",
      "Epoch [1/3], Batch: 4680, Train Loss: 0.9147\n",
      "Epoch [1/3], Batch: 4690, Train Loss: 0.9537\n",
      "Epoch [1/3], Batch: 4700, Train Loss: 1.0146\n",
      "Epoch [1/3], Batch: 4710, Train Loss: 0.9337\n",
      "Epoch [1/3], Batch: 4720, Train Loss: 0.9416\n",
      "Epoch [1/3], Batch: 4730, Train Loss: 0.9315\n",
      "Epoch [1/3], Batch: 4740, Train Loss: 0.8461\n",
      "Epoch [1/3], Batch: 4750, Train Loss: 0.9369\n",
      "Epoch [1/3], Batch: 4760, Train Loss: 0.9955\n",
      "Epoch [1/3], Batch: 4770, Train Loss: 0.9139\n",
      "Epoch [1/3], Batch: 4780, Train Loss: 0.9274\n",
      "Epoch [1/3], Batch: 4790, Train Loss: 0.9153\n",
      "Epoch [1/3], Batch: 4800, Train Loss: 0.9620\n",
      "Epoch [1/3], Batch: 4810, Train Loss: 0.9138\n",
      "Epoch [1/3], Batch: 4820, Train Loss: 1.0044\n",
      "Epoch [1/3], Batch: 4830, Train Loss: 0.9015\n",
      "Epoch [1/3], Batch: 4840, Train Loss: 0.9682\n",
      "Epoch [1/3], Batch: 4850, Train Loss: 0.9553\n",
      "Epoch [1/3], Batch: 4860, Train Loss: 1.0005\n",
      "Epoch [1/3], Batch: 4870, Train Loss: 0.9364\n",
      "Epoch [1/3], Batch: 4880, Train Loss: 0.8979\n",
      "Epoch [1/3], Batch: 4890, Train Loss: 0.9816\n",
      "Epoch [1/3], Batch: 4900, Train Loss: 0.9854\n",
      "Epoch [1/3], Batch: 4910, Train Loss: 0.9429\n",
      "Epoch [1/3], Batch: 4920, Train Loss: 0.8853\n",
      "Epoch [1/3], Batch: 4930, Train Loss: 0.9502\n",
      "Epoch [1/3], Batch: 4940, Train Loss: 1.0077\n",
      "Epoch [1/3], Batch: 4950, Train Loss: 0.9531\n",
      "Epoch [1/3], Batch: 4960, Train Loss: 0.8877\n",
      "Epoch [1/3], Batch: 4970, Train Loss: 0.8827\n",
      "Epoch [1/3], Batch: 4980, Train Loss: 0.9777\n",
      "Epoch [1/3], Batch: 4990, Train Loss: 0.9681\n",
      "Epoch [1/3], Batch: 5000, Train Loss: 0.8630\n",
      "Epoch [1/3], Batch: 5010, Train Loss: 0.9783\n",
      "Epoch [1/3], Batch: 5020, Train Loss: 1.0008\n",
      "Epoch [1/3], Batch: 5030, Train Loss: 0.9148\n",
      "Epoch [1/3], Batch: 5040, Train Loss: 0.9688\n",
      "Epoch [1/3], Batch: 5050, Train Loss: 0.9562\n",
      "Epoch [1/3], Batch: 5060, Train Loss: 0.9057\n",
      "Epoch [1/3], Batch: 5070, Train Loss: 1.0086\n",
      "Epoch [1/3], Batch: 5080, Train Loss: 0.9248\n",
      "Epoch [1/3], Batch: 5090, Train Loss: 0.8824\n",
      "Epoch [1/3], Batch: 5100, Train Loss: 0.8932\n",
      "Epoch [1/3], Batch: 5110, Train Loss: 0.9317\n",
      "Epoch [1/3], Batch: 5120, Train Loss: 0.9141\n",
      "Epoch [1/3], Batch: 5130, Train Loss: 0.9245\n",
      "Epoch [1/3], Batch: 5140, Train Loss: 0.9635\n",
      "Epoch [1/3], Batch: 5150, Train Loss: 1.0123\n",
      "Epoch [1/3], Batch: 5160, Train Loss: 0.9696\n",
      "Epoch [1/3], Batch: 5170, Train Loss: 1.0334\n",
      "Epoch [1/3], Batch: 5180, Train Loss: 0.9310\n",
      "Epoch [1/3], Batch: 5190, Train Loss: 0.9643\n",
      "Epoch [1/3], Batch: 5200, Train Loss: 0.9539\n",
      "Epoch [1/3], Batch: 5210, Train Loss: 0.9467\n",
      "Epoch [1/3], Batch: 5220, Train Loss: 0.9741\n",
      "Epoch [1/3], Batch: 5230, Train Loss: 0.9666\n",
      "Epoch [1/3], Batch: 5240, Train Loss: 0.9750\n",
      "Epoch [1/3], Batch: 5250, Train Loss: 0.9336\n",
      "Epoch [1/3], Batch: 5260, Train Loss: 0.9730\n",
      "Epoch [1/3], Batch: 5270, Train Loss: 0.9635\n",
      "Epoch [1/3], Batch: 5280, Train Loss: 0.9667\n",
      "Epoch [1/3], Batch: 5290, Train Loss: 1.0799\n",
      "Epoch [1/3], Batch: 5300, Train Loss: 0.9423\n",
      "Epoch [1/3], Batch: 5310, Train Loss: 1.0561\n",
      "Epoch [1/3], Batch: 5320, Train Loss: 0.9521\n",
      "Epoch [1/3], Batch: 5330, Train Loss: 0.8771\n",
      "Epoch [1/3], Batch: 5340, Train Loss: 1.0386\n",
      "Epoch [1/3], Batch: 5350, Train Loss: 0.8899\n",
      "Epoch [1/3], Batch: 5360, Train Loss: 0.9728\n",
      "Epoch [1/3], Batch: 5370, Train Loss: 0.8767\n",
      "Epoch [1/3], Batch: 5380, Train Loss: 0.9558\n",
      "Epoch [1/3], Batch: 5390, Train Loss: 0.9487\n",
      "Epoch [1/3], Batch: 5400, Train Loss: 0.9206\n",
      "Epoch [1/3], Batch: 5410, Train Loss: 0.9537\n",
      "Epoch [1/3], Batch: 5420, Train Loss: 0.8782\n",
      "Epoch [1/3], Batch: 5430, Train Loss: 1.0413\n",
      "Epoch [1/3], Batch: 5440, Train Loss: 1.0048\n",
      "Epoch [1/3], Batch: 5450, Train Loss: 1.0184\n",
      "Epoch [1/3], Batch: 5460, Train Loss: 0.9735\n",
      "Epoch [1/3], Batch: 5470, Train Loss: 0.9453\n",
      "Epoch [1/3], Batch: 5480, Train Loss: 0.9346\n",
      "Epoch [1/3], Batch: 5490, Train Loss: 0.8732\n",
      "Epoch [1/3], Batch: 5500, Train Loss: 0.9253\n",
      "Epoch [1/3], Batch: 5510, Train Loss: 1.0549\n",
      "Epoch [1/3], Batch: 5520, Train Loss: 1.0252\n",
      "Epoch [1/3], Batch: 5530, Train Loss: 0.9850\n",
      "Epoch [1/3], Batch: 5540, Train Loss: 0.9012\n",
      "Epoch [1/3], Batch: 5550, Train Loss: 0.9832\n",
      "Epoch [1/3], Batch: 5560, Train Loss: 0.9982\n",
      "Epoch [1/3], Batch: 5570, Train Loss: 0.9477\n",
      "Epoch [1/3], Batch: 5580, Train Loss: 0.9824\n",
      "Epoch [1/3], Batch: 5590, Train Loss: 0.9095\n",
      "Epoch [1/3], Batch: 5600, Train Loss: 0.9728\n",
      "Epoch [1/3], Batch: 5610, Train Loss: 0.9160\n",
      "Epoch [1/3], Batch: 5620, Train Loss: 0.9771\n",
      "Epoch [1/3], Batch: 5630, Train Loss: 0.9601\n",
      "Epoch [1/3], Batch: 5640, Train Loss: 0.9692\n",
      "Epoch [1/3], Batch: 5650, Train Loss: 0.9261\n",
      "Epoch [1/3], Batch: 5660, Train Loss: 0.9126\n",
      "Epoch [1/3], Batch: 5670, Train Loss: 0.9190\n",
      "Epoch [1/3], Batch: 5680, Train Loss: 0.9312\n",
      "Epoch [1/3], Batch: 5690, Train Loss: 0.9854\n",
      "Epoch [1/3], Batch: 5700, Train Loss: 0.9300\n",
      "Epoch [1/3], Batch: 5710, Train Loss: 0.9341\n",
      "Epoch [1/3], Batch: 5720, Train Loss: 0.9096\n",
      "Epoch [1/3], Batch: 5730, Train Loss: 0.9477\n",
      "Epoch [1/3], Batch: 5740, Train Loss: 0.9639\n",
      "Epoch [1/3], Batch: 5750, Train Loss: 0.8792\n",
      "Epoch [1/3], Batch: 5760, Train Loss: 0.9670\n",
      "Epoch [1/3], Batch: 5770, Train Loss: 1.0003\n",
      "Epoch [1/3], Batch: 5780, Train Loss: 0.9107\n",
      "Epoch [1/3], Batch: 5790, Train Loss: 1.0320\n",
      "Epoch [1/3], Batch: 5800, Train Loss: 0.9983\n",
      "Epoch [1/3], Batch: 5810, Train Loss: 0.8342\n",
      "Epoch [1/3], Batch: 5820, Train Loss: 0.9448\n",
      "Epoch [1/3], Batch: 5830, Train Loss: 1.0074\n",
      "Epoch [1/3], Batch: 5840, Train Loss: 0.9616\n",
      "Epoch [1/3], Batch: 5850, Train Loss: 0.9695\n",
      "Epoch [1/3], Batch: 5860, Train Loss: 0.9990\n",
      "Epoch [1/3], Batch: 5870, Train Loss: 0.8895\n",
      "Epoch [1/3], Batch: 5880, Train Loss: 0.9368\n",
      "Epoch [1/3], Batch: 5890, Train Loss: 0.8899\n",
      "Epoch [1/3], Batch: 5900, Train Loss: 0.9650\n",
      "Epoch [1/3], Batch: 5910, Train Loss: 0.9733\n",
      "Epoch [1/3], Batch: 5920, Train Loss: 0.9503\n",
      "Epoch [1/3], Batch: 5930, Train Loss: 0.9216\n",
      "Epoch [1/3], Batch: 5940, Train Loss: 1.0112\n",
      "Epoch [1/3], Batch: 5950, Train Loss: 0.9210\n",
      "Epoch [1/3], Batch: 5960, Train Loss: 0.9069\n",
      "Epoch [1/3], Batch: 5970, Train Loss: 0.9279\n",
      "Epoch [1/3], Batch: 5980, Train Loss: 0.9318\n",
      "Epoch [1/3], Batch: 5990, Train Loss: 1.0160\n",
      "Epoch [1/3], Batch: 6000, Train Loss: 0.8891\n",
      "Epoch [1/3], Batch: 6010, Train Loss: 0.9403\n",
      "Epoch [1/3], Batch: 6020, Train Loss: 0.9232\n",
      "Epoch [1/3], Batch: 6030, Train Loss: 0.9310\n",
      "Epoch [1/3], Batch: 6040, Train Loss: 0.9909\n",
      "Epoch [1/3], Batch: 6050, Train Loss: 0.9466\n",
      "Epoch [1/3], Batch: 6060, Train Loss: 0.9612\n",
      "Epoch [1/3], Batch: 6070, Train Loss: 0.8804\n",
      "Epoch [1/3], Batch: 6080, Train Loss: 0.9348\n",
      "Epoch [1/3], Batch: 6090, Train Loss: 0.9872\n",
      "Epoch [1/3], Batch: 6100, Train Loss: 0.9944\n",
      "Epoch [1/3], Batch: 6110, Train Loss: 0.9271\n",
      "Epoch [1/3], Batch: 6120, Train Loss: 0.9497\n",
      "Epoch [1/3], Batch: 6130, Train Loss: 0.9651\n",
      "Epoch [1/3], Batch: 6140, Train Loss: 0.8888\n",
      "Epoch [1/3], Batch: 6150, Train Loss: 1.0170\n",
      "Epoch [1/3], Batch: 6160, Train Loss: 0.9327\n",
      "Epoch [1/3], Batch: 6170, Train Loss: 0.9514\n",
      "Epoch [1/3], Batch: 6180, Train Loss: 0.9119\n",
      "Epoch [1/3], Batch: 6190, Train Loss: 1.0031\n",
      "Epoch [1/3], Batch: 6200, Train Loss: 0.9837\n",
      "Epoch [1/3], Batch: 6210, Train Loss: 0.9375\n",
      "Epoch [1/3], Batch: 6220, Train Loss: 0.9194\n",
      "Epoch [1/3], Batch: 6230, Train Loss: 0.9347\n",
      "Epoch [1/3], Batch: 6240, Train Loss: 0.9130\n",
      "Epoch [1/3], Batch: 6250, Train Loss: 1.0274\n",
      "Epoch [1/3], Batch: 6260, Train Loss: 0.9463\n",
      "Epoch [1/3], Batch: 6270, Train Loss: 0.9516\n",
      "Epoch [1/3], Batch: 6280, Train Loss: 0.9155\n",
      "Epoch [1/3], Batch: 6290, Train Loss: 1.0165\n",
      "Epoch [1/3], Batch: 6300, Train Loss: 0.9152\n",
      "Epoch [1/3], Batch: 6310, Train Loss: 0.9298\n",
      "Epoch [1/3], Batch: 6320, Train Loss: 0.9418\n",
      "Epoch [1/3], Batch: 6330, Train Loss: 0.9722\n",
      "Epoch [1/3], Batch: 6340, Train Loss: 1.0414\n",
      "Epoch [1/3], Batch: 6350, Train Loss: 1.0306\n",
      "Epoch [1/3], Batch: 6360, Train Loss: 0.9613\n",
      "Epoch [1/3], Batch: 6370, Train Loss: 0.9110\n",
      "Epoch [1/3], Batch: 6380, Train Loss: 0.9662\n",
      "Epoch [1/3], Batch: 6390, Train Loss: 0.9261\n",
      "Epoch [1/3], Batch: 6400, Train Loss: 0.8807\n",
      "Epoch [1/3], Batch: 6410, Train Loss: 0.9485\n",
      "Epoch [1/3], Batch: 6420, Train Loss: 0.9543\n",
      "Epoch [1/3], Batch: 6430, Train Loss: 0.9177\n",
      "Epoch [1/3], Batch: 6440, Train Loss: 0.9411\n",
      "Epoch [1/3], Batch: 6450, Train Loss: 1.0094\n",
      "Epoch [1/3], Batch: 6460, Train Loss: 1.0065\n",
      "Epoch [1/3], Batch: 6470, Train Loss: 0.9065\n",
      "Epoch [1/3], Batch: 6480, Train Loss: 1.0246\n",
      "Epoch [1/3], Batch: 6490, Train Loss: 0.8390\n",
      "Epoch [1/3], Batch: 6500, Train Loss: 0.8972\n",
      "Epoch [1/3], Batch: 6510, Train Loss: 0.9597\n",
      "Epoch [1/3], Batch: 6520, Train Loss: 0.8996\n",
      "Epoch [1/3], Batch: 6530, Train Loss: 0.8612\n",
      "Epoch [1/3], Batch: 6540, Train Loss: 0.9216\n",
      "Epoch [1/3], Batch: 6550, Train Loss: 0.9304\n",
      "Epoch [1/3], Batch: 6560, Train Loss: 1.0275\n",
      "Epoch [1/3], Batch: 6570, Train Loss: 1.0113\n",
      "Epoch [1/3], Batch: 6580, Train Loss: 0.9484\n",
      "Epoch [1/3], Batch: 6590, Train Loss: 0.9724\n",
      "Epoch [1/3], Batch: 6600, Train Loss: 0.9855\n",
      "Epoch [1/3], Batch: 6610, Train Loss: 0.9106\n",
      "Epoch [1/3], Batch: 6620, Train Loss: 0.9072\n",
      "Epoch [1/3], Batch: 6630, Train Loss: 1.0098\n",
      "Epoch [1/3], Batch: 6640, Train Loss: 0.9831\n",
      "Epoch [1/3], Batch: 6650, Train Loss: 0.9467\n",
      "Epoch [1/3], Batch: 6660, Train Loss: 0.9307\n",
      "Epoch [1/3], Batch: 6670, Train Loss: 0.9908\n",
      "Epoch [1/3], Batch: 6680, Train Loss: 0.8745\n",
      "Epoch [1/3], Batch: 6690, Train Loss: 0.9441\n",
      "Epoch [1/3], Batch: 6700, Train Loss: 0.9795\n",
      "Epoch [1/3], Batch: 6710, Train Loss: 0.9977\n",
      "Epoch [1/3], Batch: 6720, Train Loss: 0.9918\n",
      "Epoch [1/3], Batch: 6730, Train Loss: 0.9064\n",
      "Epoch [1/3], Batch: 6740, Train Loss: 0.8884\n",
      "Epoch [1/3], Batch: 6750, Train Loss: 0.9469\n",
      "Epoch [1/3], Batch: 6760, Train Loss: 0.9801\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'relevant_doc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# No need to track gradients for testing\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m TestingDataloader:\n\u001b[1;32m     34\u001b[0m         query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings \u001b[38;5;241m=\u001b[39m Towers(\n\u001b[0;32m---> 35\u001b[0m             batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelevant_doc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mirrelevant_doc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     36\u001b[0m         )\n\u001b[1;32m     38\u001b[0m         loss \u001b[38;5;241m=\u001b[39m triplet_loss_function_cosine(query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings, margin)\n\u001b[1;32m     39\u001b[0m         test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'relevant_doc'"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    Towers.train() \n",
    "    train_loss = 0.0\n",
    "\n",
    "    i = 0\n",
    "    for batch in TrainingDataloader:\n",
    "        i +=1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model to get embeddings\n",
    "        query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings = Towers(\n",
    "            batch['query'], \n",
    "            batch['relevant'], \n",
    "            batch['irrelevant']\n",
    "        )\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = triplet_loss_function_cosine(query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings, margin)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        if (i) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch: {i}, Train Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # break\n",
    "    \n",
    "    # Testing phase\n",
    "    Towers.eval()  # Set model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():  # No need to track gradients for testing\n",
    "        for batch in TestingDataloader:\n",
    "            query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings = Towers(\n",
    "                batch['query'], \n",
    "                batch['relevant'], \n",
    "                batch['irrelevant']\n",
    "            )\n",
    "            \n",
    "            loss = triplet_loss_function_cosine(query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings, margin)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "    avg_train_loss = train_loss / len(TestingDataloader)\n",
    "    avg_test_loss = test_loss / len(TestingDataloader)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
