{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from tqdm.auto import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ms_marco\", \"v1.1\")\n",
    "\n",
    "# train_data = pd.DataFrame(dataset['train'])\n",
    "test_data = pd.DataFrame(dataset['test'])\n",
    "# validation_data = pd.DataFrame(dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_passages(dataframe):\n",
    "    unraveled_rows = []\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "        # query_id = row['query_id']\n",
    "        query = row['query']\n",
    "        # answers = row['answers']\n",
    "        count = len(row['passages']['passage_text'])\n",
    "        \n",
    "        for passage, url in zip(row['passages']['passage_text'], row['passages']['url']):\n",
    "            unraveled_rows.append({\n",
    "                # 'query_id': query_id,\n",
    "                'query': query,\n",
    "                # 'answers': answers,\n",
    "                'passage': passage,\n",
    "                'url': url,\n",
    "                'count': count,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(unraveled_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unravel_train_data = unravel_passages(train_data)\n",
    "unravel_test_data = unravel_passages(test_data)\n",
    "# unravel_val_data = unravel_passages(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_queries = pd.DataFrame(unravel_test_data['query'].unique(), columns=['query'])\n",
    "unique_passages = pd.DataFrame(unravel_test_data['passage'].unique(), columns=['passage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_queries['query_id'] = range(len(unique_queries))\n",
    "unique_passages['passage_id'] = range(len(unique_passages))\n",
    "\n",
    "query_to_id = {row['query']: row['query_id'] for index, row in unique_queries.iterrows()}\n",
    "passage_to_id = {row['passage']: row['passage_id'] for index, row in unique_passages.iterrows()}\n",
    "\n",
    "unravel_test_data['query_id'] = unravel_test_data['query'].map(query_to_id)\n",
    "unravel_test_data['passage_id'] = unravel_test_data['passage'].map(passage_to_id)\n",
    "\n",
    "relevant_passages = unravel_test_data.groupby('query_id')['passage_id'].apply(list).reset_index(name='relevant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   query_id                                  relevant\n",
      "0         0                     [0, 1, 2, 3, 4, 5, 6]\n",
      "1         1         [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "2         2      [16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "3         3      [25, 26, 27, 28, 29, 30, 31, 32, 33]\n",
      "4         4  [34, 35, 36, 37, 38, 39, 40, 41, 42, 43]\n"
     ]
    }
   ],
   "source": [
    "print(relevant_passages.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_irrelevant(relevant_list, all_passages, num_samples):\n",
    "    possible_irrelevant = list(set(all_passages) - set(relevant_list))\n",
    "    return random.sample(possible_irrelevant, num_samples)\n",
    "\n",
    "\n",
    "def sample_irrelevant_optimized(relevant_list, all_passages_ids, irrelevant_cache, num_samples):\n",
    "    relevant_set = frozenset(relevant_list)  # Convert to frozenset for hashing\n",
    "    if relevant_set not in irrelevant_cache:\n",
    "        possible_irrelevant = list(all_passages_ids - relevant_set)\n",
    "        irrelevant_cache[relevant_set] = possible_irrelevant\n",
    "    else:\n",
    "        possible_irrelevant = irrelevant_cache[relevant_set]\n",
    "    \n",
    "    return random.sample(possible_irrelevant, min(len(possible_irrelevant), num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_passages_ids = set(unique_passages['passage_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def worker(args):\n",
    "    relevant_list, all_passages_ids, num_samples = args\n",
    "    return sample_irrelevant_optimized(relevant_list, all_passages_ids, {}, num_samples)\n",
    "\n",
    "def apply_in_parallel(df, all_passages_ids, num_processes=None):\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        # Prepare arguments for parallel processing\n",
    "        args = [(row['relevant'], all_passages_ids, len(row['relevant'])) for index, row in df.iterrows()]\n",
    "        \n",
    "        # Execute the function in parallel\n",
    "        results = pool.map(worker, args)\n",
    "        \n",
    "    # Update the DataFrame with results\n",
    "    df['irrelevant'] = results\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_cache = {}  # Initialize cache outside the function for reuse\n",
    "relevant_passages['irrelevant'] = relevant_passages['relevant'].apply(\n",
    "    lambda x: sample_irrelevant_optimized(x, all_passages_ids, irrelevant_cache, len(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_passages['irrelevant'] = relevant_passages['relevant'].apply(lambda x: sample_irrelevant(x, all_passages_ids, len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>relevant</th>\n",
       "      <th>irrelevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
       "      <td>[23177, 70775, 35696, 53719, 48244, 9643, 47114]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[7, 8, 9, 10, 11, 12, 13, 14, 15]</td>\n",
       "      <td>[74064, 16659, 7625, 24907, 69326, 15534, 2459...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[16, 17, 18, 19, 20, 21, 22, 23, 24]</td>\n",
       "      <td>[24129, 7719, 28653, 7868, 76809, 58615, 49393...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[25, 26, 27, 28, 29, 30, 31, 32, 33]</td>\n",
       "      <td>[40729, 14303, 57113, 54914, 19875, 30572, 581...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[34, 35, 36, 37, 38, 39, 40, 41, 42, 43]</td>\n",
       "      <td>[45232, 75588, 17959, 36800, 40898, 70599, 729...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9645</th>\n",
       "      <td>9645</td>\n",
       "      <td>[77856, 77857, 77858, 77859, 77860, 77861, 77862]</td>\n",
       "      <td>[55471, 30431, 42547, 52805, 9398, 72762, 49530]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9646</th>\n",
       "      <td>9646</td>\n",
       "      <td>[47682, 77863, 77864, 77865, 77866, 77867, 778...</td>\n",
       "      <td>[60553, 67130, 36098, 55344, 52781, 68109, 312...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9647</th>\n",
       "      <td>9647</td>\n",
       "      <td>[77871, 77872, 77873, 77874, 77875, 77876, 778...</td>\n",
       "      <td>[60575, 31463, 18748, 73243, 28724, 55078, 771...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9648</th>\n",
       "      <td>9648</td>\n",
       "      <td>[77880, 77881, 77882, 77883, 54246, 77884, 778...</td>\n",
       "      <td>[66949, 20480, 57526, 55277, 7823, 42425, 4208...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9649</th>\n",
       "      <td>9649</td>\n",
       "      <td>[77887, 77888, 77889, 77890, 77891, 77892, 778...</td>\n",
       "      <td>[19217, 58984, 52056, 7155, 6210, 48170, 57092...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9650 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      query_id                                           relevant  \\\n",
       "0            0                              [0, 1, 2, 3, 4, 5, 6]   \n",
       "1            1                  [7, 8, 9, 10, 11, 12, 13, 14, 15]   \n",
       "2            2               [16, 17, 18, 19, 20, 21, 22, 23, 24]   \n",
       "3            3               [25, 26, 27, 28, 29, 30, 31, 32, 33]   \n",
       "4            4           [34, 35, 36, 37, 38, 39, 40, 41, 42, 43]   \n",
       "...        ...                                                ...   \n",
       "9645      9645  [77856, 77857, 77858, 77859, 77860, 77861, 77862]   \n",
       "9646      9646  [47682, 77863, 77864, 77865, 77866, 77867, 778...   \n",
       "9647      9647  [77871, 77872, 77873, 77874, 77875, 77876, 778...   \n",
       "9648      9648  [77880, 77881, 77882, 77883, 54246, 77884, 778...   \n",
       "9649      9649  [77887, 77888, 77889, 77890, 77891, 77892, 778...   \n",
       "\n",
       "                                             irrelevant  \n",
       "0      [23177, 70775, 35696, 53719, 48244, 9643, 47114]  \n",
       "1     [74064, 16659, 7625, 24907, 69326, 15534, 2459...  \n",
       "2     [24129, 7719, 28653, 7868, 76809, 58615, 49393...  \n",
       "3     [40729, 14303, 57113, 54914, 19875, 30572, 581...  \n",
       "4     [45232, 75588, 17959, 36800, 40898, 70599, 729...  \n",
       "...                                                 ...  \n",
       "9645   [55471, 30431, 42547, 52805, 9398, 72762, 49530]  \n",
       "9646  [60553, 67130, 36098, 55344, 52781, 68109, 312...  \n",
       "9647  [60575, 31463, 18748, 73243, 28724, 55078, 771...  \n",
       "9648  [66949, 20480, 57526, 55277, 7823, 42425, 4208...  \n",
       "9649  [19217, 58984, 52056, 7155, 6210, 48170, 57092...  \n",
       "\n",
       "[9650 rows x 3 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[199], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m corpus \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m Word2Vec(corpus, vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/gensim/models/word2vec.py:430\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m(epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_vocab(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule)\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m    431\u001b[0m         corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, total_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count,\n\u001b[1;32m    432\u001b[0m         total_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs, start_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha,\n\u001b[1;32m    433\u001b[0m         end_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha, compute_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trim_rule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/gensim/models/word2vec.py:1073\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_epoch_begin(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1073\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch(\n\u001b[1;32m   1074\u001b[0m         corpus_iterable, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples,\n\u001b[1;32m   1075\u001b[0m         total_words\u001b[38;5;241m=\u001b[39mtotal_words, queue_factor\u001b[38;5;241m=\u001b[39mqueue_factor, report_delay\u001b[38;5;241m=\u001b[39mreport_delay,\n\u001b[1;32m   1076\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1078\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch_corpusfile(\n\u001b[1;32m   1079\u001b[0m         corpus_file, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples, total_words\u001b[38;5;241m=\u001b[39mtotal_words,\n\u001b[1;32m   1080\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/gensim/models/word2vec.py:1434\u001b[0m, in \u001b[0;36mWord2Vec._train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m   1431\u001b[0m     thread\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# make interrupting the process with ctrl+c easier\u001b[39;00m\n\u001b[1;32m   1432\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m-> 1434\u001b[0m trained_word_count, raw_word_count, job_tally \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_epoch_progress(\n\u001b[1;32m   1435\u001b[0m     progress_queue, job_queue, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples,\n\u001b[1;32m   1436\u001b[0m     total_words\u001b[38;5;241m=\u001b[39mtotal_words, report_delay\u001b[38;5;241m=\u001b[39mreport_delay, is_corpus_file_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1437\u001b[0m )\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trained_word_count, raw_word_count, job_tally\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/site-packages/gensim/models/word2vec.py:1289\u001b[0m, in \u001b[0;36mWord2Vec._log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m   1286\u001b[0m unfinished_worker_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unfinished_worker_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1289\u001b[0m     report \u001b[38;5;241m=\u001b[39m progress_queue\u001b[38;5;241m.\u001b[39mget()  \u001b[38;5;66;03m# blocks if workers too slow\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# a thread reporting that it finished\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m         unfinished_worker_count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus = api.load('text8')\n",
    "model = Word2Vec(corpus, vector_size=300, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_padded_embeddings(text, word2vec_model, max_length):\n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Initialize an empty matrix for embeddings with shape (max_length, vector_size)\n",
    "    embeddings_matrix = np.zeros((max_length, word2vec_model.vector_size))\n",
    "\n",
    "    # Iterate over the tokens to retrieve embeddings, up to the max_length\n",
    "    for i, token in enumerate(tokens[:max_length]):\n",
    "        if token in word2vec_model.wv:\n",
    "            embeddings_matrix[i] = word2vec_model.wv[token]\n",
    "\n",
    "    return embeddings_matrix\n",
    "\n",
    "def create_sequence_embeddings_dataframe(triplets_dataframe, word2vec_model, query_length=15, doc_length=50):\n",
    "    embeddings_data = []\n",
    "\n",
    "    for idx, row in triplets_dataframe.iterrows():\n",
    "        # Generate embeddings for the query, relevant document, and irrelevant document\n",
    "        query_embeddings = text_to_padded_embeddings(row['query'], word2vec_model, query_length)\n",
    "        relevant_doc_embeddings = text_to_padded_embeddings(row['relevant_doc'], word2vec_model, doc_length)\n",
    "        irrelevant_doc_embeddings = text_to_padded_embeddings(row['irrelevant_doc'], word2vec_model, doc_length)\n",
    "        \n",
    "        # Append the embeddings as matrices to the list\n",
    "        embeddings_data.append({\n",
    "            'query_embeddings': query_embeddings,\n",
    "            'relevant_doc_embeddings': relevant_doc_embeddings,\n",
    "            'irrelevant_doc_embeddings': irrelevant_doc_embeddings\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(embeddings_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'triplets_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embeddings_df \u001b[38;5;241m=\u001b[39m create_sequence_embeddings_dataframe(triplets_train, model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'triplets_train' is not defined"
     ]
    }
   ],
   "source": [
    "embeddings_df = create_sequence_embeddings_dataframe(triplets_train, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceEmbeddingTripletsDataset(Dataset):\n",
    "    def __init__(self, embeddings_dataframe):\n",
    "        self.embeddings_df = embeddings_dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.embeddings_df.iloc[idx]\n",
    "        query_embeddings = torch.tensor(row['query_embeddings'], dtype=torch.float)\n",
    "        relevant_doc_embeddings = torch.tensor(row['relevant_doc_embeddings'], dtype=torch.float)\n",
    "        irrelevant_doc_embeddings = torch.tensor(row['irrelevant_doc_embeddings'], dtype=torch.float)\n",
    "        return query_embeddings, relevant_doc_embeddings, irrelevant_doc_embeddings\n",
    "\n",
    "# Create the DataLoader\n",
    "sequence_embedding_dataset = SequenceEmbeddingTripletsDataset(embeddings_df)\n",
    "sequence_embedding_dataloader = DataLoader(sequence_embedding_dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletsDataset(Dataset):\n",
    "    def __init__(self, triplets_dataframe):\n",
    "        self.dataframe = triplets_dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        query = row['query']\n",
    "        relevant_doc = row['relevant_doc']\n",
    "        irrelevant_doc = row['irrelevant_doc']\n",
    "        return query, relevant_doc, irrelevant_doc\n",
    "\n",
    "# Example usage\n",
    "train_dataset = TripletsDataset(triplets_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define dataset parameters\n",
    "# num_examples = 64\n",
    "# seq_length = 10\n",
    "# embedding_dim = 300\n",
    "\n",
    "# # Generate synthetic data\n",
    "# queries = torch.randn(num_examples, seq_length, embedding_dim)\n",
    "# relevant_docs = torch.randn(num_examples, seq_length, embedding_dim)\n",
    "# irrelevant_docs = torch.randn(num_examples, seq_length, embedding_dim)\n",
    "\n",
    "# # Convert to a DataFrame\n",
    "# df = pd.DataFrame({\n",
    "#     'queries': list(queries),\n",
    "#     'relevant_docs': list(relevant_docs),\n",
    "#     'irrelevant_docs': list(irrelevant_docs)\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TripletsDataset(Dataset):\n",
    "#     def __init__(self, dataframe):\n",
    "#         self.dataframe = dataframe\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.dataframe)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         return (self.dataframe.iloc[idx]['queries'],\n",
    "#                 self.dataframe.iloc[idx]['relevant_docs'],\n",
    "#                 self.dataframe.iloc[idx]['irrelevant_docs'])\n",
    "\n",
    "# dataset = TripletsDataset(df)\n",
    "# dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        self.query_encoder = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.doc_encoder = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "    def forward(self, query, doc):\n",
    "        _, query_hidden = self.query_encoder(query)\n",
    "        _, doc_hidden = self.doc_encoder(doc)\n",
    "        return query_hidden.squeeze(0), doc_hidden.squeeze(0)\n",
    "\n",
    "model = TwoTowerModel(embedding_dim=embedding_dim, hidden_dim=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.TripletMarginLoss(margin=1.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0147972106933594\n",
      "Epoch 2, Loss: 0.0\n",
      "Epoch 3, Loss: 0.0\n",
      "Epoch 4, Loss: 0.0\n",
      "Epoch 5, Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for queries, relevant_docs, irrelevant_docs in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Encode queries and documents\n",
    "        query_enc, rel_doc_enc = model(queries, relevant_docs)\n",
    "        _, irr_doc_enc = model(queries, irrelevant_docs)\n",
    "        \n",
    "        # Compute loss and backpropagate\n",
    "        loss = criterion(query_enc, rel_doc_enc, irr_doc_enc)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
