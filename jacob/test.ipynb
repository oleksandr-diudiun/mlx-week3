{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacob/miniconda3/envs/torch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ms_marco\", \"v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_1 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_data = pd.DataFrame(dataset['train'])\n",
    "test_data = pd.DataFrame(dataset['test'])\n",
    "# validation_data = pd.DataFrame(dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_passages(dataframe):\n",
    "    unraveled_rows = []\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "        # query_id = row['query_id']\n",
    "        query = row['query']\n",
    "        # answers = row['answers']\n",
    "        count = len(row['passages']['passage_text'])\n",
    "        \n",
    "        for passage, url in zip(row['passages']['passage_text'], row['passages']['url']):\n",
    "            unraveled_rows.append({\n",
    "                # 'query_id': query_id,\n",
    "                'query': query,\n",
    "                # 'answers': answers,\n",
    "                'passage': passage,\n",
    "                'url': url,\n",
    "                'count': count,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(unraveled_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unravel_train_data = unravel_passages(train_data)\n",
    "unravel_test_data = unravel_passages(test_data)\n",
    "# unravel_val_data = unravel_passages(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_queries = pd.DataFrame(unravel_test_data['query'].unique(), columns=['query'])\n",
    "unique_passages = pd.DataFrame(unravel_test_data['passage'].unique(), columns=['passage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_queries['query_id'] = range(len(unique_queries))\n",
    "unique_passages['passage_id'] = range(len(unique_passages))\n",
    "\n",
    "query_to_id = {row['query']: row['query_id'] for index, row in unique_queries.iterrows()}\n",
    "passage_to_id = {row['passage']: row['passage_id'] for index, row in unique_passages.iterrows()}\n",
    "\n",
    "unravel_test_data['query_id'] = unravel_test_data['query'].map(query_to_id)\n",
    "unravel_test_data['passage_id'] = unravel_test_data['passage'].map(passage_to_id)\n",
    "\n",
    "relevant_passages = unravel_test_data.groupby('query_id')['passage_id'].apply(list).reset_index(name='relevant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_passages_ids = set(unique_passages['passage_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_irrelevant_optimized(relevant_list, all_passages_ids, irrelevant_cache, num_samples):\n",
    "    relevant_set = frozenset(relevant_list)  # Convert to frozenset for hashing\n",
    "    if relevant_set not in irrelevant_cache:\n",
    "        possible_irrelevant = list(all_passages_ids - relevant_set)\n",
    "        irrelevant_cache[relevant_set] = possible_irrelevant\n",
    "    else:\n",
    "        possible_irrelevant = irrelevant_cache[relevant_set]\n",
    "    \n",
    "    return random.sample(possible_irrelevant, min(len(possible_irrelevant), num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_cache = {}  # Initialize cache outside the function for reuse\n",
    "relevant_passages['irrelevant'] = relevant_passages['relevant'].apply(\n",
    "    lambda x: sample_irrelevant_optimized(x, all_passages_ids, irrelevant_cache, len(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>relevant</th>\n",
       "      <th>irrelevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
       "      <td>[30209, 27773, 8616, 38313, 56167, 44861, 52335]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[7, 8, 9, 10, 11, 12, 13, 14, 15]</td>\n",
       "      <td>[36996, 34019, 71965, 18223, 66896, 63445, 551...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[16, 17, 18, 19, 20, 21, 22, 23, 24]</td>\n",
       "      <td>[74387, 49243, 732, 12585, 53876, 67219, 61281...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[25, 26, 27, 28, 29, 30, 31, 32, 33]</td>\n",
       "      <td>[54395, 66623, 74378, 67818, 43131, 13452, 694...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[34, 35, 36, 37, 38, 39, 40, 41, 42, 43]</td>\n",
       "      <td>[14372, 49193, 17109, 50271, 39630, 42264, 404...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9645</th>\n",
       "      <td>9645</td>\n",
       "      <td>[77856, 77857, 77858, 77859, 77860, 77861, 77862]</td>\n",
       "      <td>[22876, 7481, 68605, 26103, 4918, 34289, 41588]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9646</th>\n",
       "      <td>9646</td>\n",
       "      <td>[47682, 77863, 77864, 77865, 77866, 77867, 778...</td>\n",
       "      <td>[59227, 73641, 55954, 15871, 86, 4297, 28879, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9647</th>\n",
       "      <td>9647</td>\n",
       "      <td>[77871, 77872, 77873, 77874, 77875, 77876, 778...</td>\n",
       "      <td>[33281, 71562, 1196, 74911, 37322, 45103, 1143...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9648</th>\n",
       "      <td>9648</td>\n",
       "      <td>[77880, 77881, 77882, 77883, 54246, 77884, 778...</td>\n",
       "      <td>[50982, 49385, 32305, 33354, 24871, 11452, 978...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9649</th>\n",
       "      <td>9649</td>\n",
       "      <td>[77887, 77888, 77889, 77890, 77891, 77892, 778...</td>\n",
       "      <td>[38449, 26960, 74868, 34708, 16303, 40695, 578...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9650 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      query_id                                           relevant  \\\n",
       "0            0                              [0, 1, 2, 3, 4, 5, 6]   \n",
       "1            1                  [7, 8, 9, 10, 11, 12, 13, 14, 15]   \n",
       "2            2               [16, 17, 18, 19, 20, 21, 22, 23, 24]   \n",
       "3            3               [25, 26, 27, 28, 29, 30, 31, 32, 33]   \n",
       "4            4           [34, 35, 36, 37, 38, 39, 40, 41, 42, 43]   \n",
       "...        ...                                                ...   \n",
       "9645      9645  [77856, 77857, 77858, 77859, 77860, 77861, 77862]   \n",
       "9646      9646  [47682, 77863, 77864, 77865, 77866, 77867, 778...   \n",
       "9647      9647  [77871, 77872, 77873, 77874, 77875, 77876, 778...   \n",
       "9648      9648  [77880, 77881, 77882, 77883, 54246, 77884, 778...   \n",
       "9649      9649  [77887, 77888, 77889, 77890, 77891, 77892, 778...   \n",
       "\n",
       "                                             irrelevant  \n",
       "0      [30209, 27773, 8616, 38313, 56167, 44861, 52335]  \n",
       "1     [36996, 34019, 71965, 18223, 66896, 63445, 551...  \n",
       "2     [74387, 49243, 732, 12585, 53876, 67219, 61281...  \n",
       "3     [54395, 66623, 74378, 67818, 43131, 13452, 694...  \n",
       "4     [14372, 49193, 17109, 50271, 39630, 42264, 404...  \n",
       "...                                                 ...  \n",
       "9645    [22876, 7481, 68605, 26103, 4918, 34289, 41588]  \n",
       "9646  [59227, 73641, 55954, 15871, 86, 4297, 28879, ...  \n",
       "9647  [33281, 71562, 1196, 74911, 37322, 45103, 1143...  \n",
       "9648  [50982, 49385, 32305, 33354, 24871, 11452, 978...  \n",
       "9649  [38449, 26960, 74868, 34708, 16303, 40695, 578...  \n",
       "\n",
       "[9650 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time_1 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def unravel_passages(dataframe):\n",
    "    unraveled_rows = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        query = row['query']\n",
    "        count = len(row['passages']['passage_text'])\n",
    "        for passage, url in zip(row['passages']['passage_text'], row['passages']['url']):\n",
    "            unraveled_rows.append({\n",
    "                'query': query,\n",
    "                'passage': passage,\n",
    "                'url': url,\n",
    "                'count': count,\n",
    "            })\n",
    "    return pd.DataFrame(unraveled_rows)\n",
    "\n",
    "def prepare_mappings(unraveled_data):\n",
    "    unique_queries = pd.DataFrame(unraveled_data['query'].unique(), columns=['query'])\n",
    "    unique_passages = pd.DataFrame(unraveled_data['passage'].unique(), columns=['passage'])\n",
    "    unique_queries['query_id'] = range(len(unique_queries))\n",
    "    unique_passages['passage_id'] = range(len(unique_passages))\n",
    "    return unique_queries, unique_passages\n",
    "\n",
    "def map_ids(unraveled_data, query_to_id, passage_to_id):\n",
    "    unraveled_data['query_id'] = unraveled_data['query'].map(query_to_id)\n",
    "    unraveled_data['passage_id'] = unraveled_data['passage'].map(passage_to_id)\n",
    "    return unraveled_data\n",
    "\n",
    "def sample_irrelevant_optimized(relevant_list, all_passages_ids, num_samples):\n",
    "    relevant_set = frozenset(relevant_list)\n",
    "    possible_irrelevant = list(all_passages_ids - relevant_set)\n",
    "    return random.sample(possible_irrelevant, min(len(possible_irrelevant), num_samples))\n",
    "\n",
    "def create_triplets_dataframe(unraveled_data):\n",
    "    relevant_passages = unraveled_data.groupby('query_id')['passage_id'].apply(list).reset_index(name='relevant')\n",
    "    all_passages_ids = set(unraveled_data['passage_id'])\n",
    "    irrelevant_cache = {}\n",
    "    relevant_passages['irrelevant'] = relevant_passages['relevant'].apply(\n",
    "        lambda x: sample_irrelevant_optimized(x, all_passages_ids, len(x))\n",
    "    )\n",
    "    return relevant_passages\n",
    "\n",
    "def process_dataset(dataset_split):\n",
    "    unraveled_data = unravel_passages(dataset_split)\n",
    "    unique_queries, unique_passages = prepare_mappings(unraveled_data)\n",
    "    query_to_id = {row['query']: row['query_id'] for index, row in unique_queries.iterrows()}\n",
    "    passage_to_id = {row['passage']: row['passage_id'] for index, row in unique_passages.iterrows()}\n",
    "    unraveled_data = map_ids(unraveled_data, query_to_id, passage_to_id)\n",
    "    triplets_df = create_triplets_dataframe(unraveled_data)\n",
    "    return triplets_df, unique_queries, unique_passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ms_marco\", \"v1.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_2 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_triplets, train_queries, train_passages = process_dataset(pd.DataFrame(dataset['train']))\n",
    "test_triplets, test_queries, test_passages = process_dataset(pd.DataFrame(dataset['test']))\n",
    "# validate_triplets, validate_queries, validate_passages = process_dataset(pd.DataFrame(dataset['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time_2 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 1: 11.422701625000627\n",
      "Time 2: 10.06078554099804\n"
     ]
    }
   ],
   "source": [
    "time_1 = end_time_1 - start_time_1\n",
    "time_2 = end_time_2 - start_time_2\n",
    "\n",
    "print(f\"Time 1: {time_1}\")\n",
    "print(f\"Time 2: {time_2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
