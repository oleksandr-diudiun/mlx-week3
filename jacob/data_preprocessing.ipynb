{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'profile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m     unraveled_data \u001b[38;5;241m=\u001b[39m unraveled_data\u001b[38;5;241m.\u001b[39mmerge(unique_passages, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassage\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m unraveled_data\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;129m@profile\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_dataset_optim\u001b[39m(dataset_split):\n\u001b[1;32m     48\u001b[0m     unraveled_data \u001b[38;5;241m=\u001b[39m unravel_passages(dataset_split)\n\u001b[1;32m     49\u001b[0m     unique_queries, unique_passages \u001b[38;5;241m=\u001b[39m prepare_mappings_optim(unraveled_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'profile' is not defined"
     ]
    }
   ],
   "source": [
    "def unravel_passages(dataset):\n",
    "    unraveled_rows = []\n",
    "    for i, data in enumerate(dataset['query']):\n",
    "        query = data\n",
    "        passage_texts = dataset['passages'][i]['passage_text']\n",
    "        urls = dataset['passages'][i]['url']\n",
    "        for passage_text, url in zip(passage_texts, urls):\n",
    "            unraveled_rows.append({\n",
    "                'query': query,\n",
    "                'passage': passage_text,\n",
    "                'url': url\n",
    "            })    \n",
    "    return pd.DataFrame(unraveled_rows)\n",
    "\n",
    "\n",
    "def sample_irrelevant_optimized(relevant_list, all_passages_ids, num_samples):\n",
    "    relevant_set = frozenset(relevant_list)\n",
    "    possible_irrelevant = list(all_passages_ids - relevant_set)\n",
    "    return random.sample(possible_irrelevant, min(len(possible_irrelevant), num_samples))\n",
    "\n",
    "def create_triplets_dataframe(unraveled_data):\n",
    "    relevant_passages = unraveled_data.groupby('query_id')['passage_id'].apply(list).reset_index(name='relevant')\n",
    "    all_passages_ids = set(unraveled_data['passage_id'])\n",
    "    irrelevant_cache = {}\n",
    "    relevant_passages['irrelevant'] = relevant_passages['relevant'].apply(\n",
    "        lambda x: sample_irrelevant_optimized(x, all_passages_ids, len(x))\n",
    "    )\n",
    "    return relevant_passages\n",
    "\n",
    "def prepare_mappings_optim(unraveled_data):\n",
    "    unique_queries = pd.DataFrame({'query': unraveled_data['query'].unique()})\n",
    "    unique_passages = pd.DataFrame({'passage': unraveled_data['passage'].unique()})\n",
    "    \n",
    "    # Use the index directly for ID assignment\n",
    "    unique_queries['query_id'] = unique_queries.index\n",
    "    unique_passages['passage_id'] = unique_passages.index\n",
    "    \n",
    "    return unique_queries, unique_passages\n",
    "\n",
    "def map_ids_optim(unraveled_data, unique_queries, unique_passages):\n",
    "    # Map using merge for vectorized operation, this is much faster than map for large datasets\n",
    "    unraveled_data = unraveled_data.merge(unique_queries, on='query', how='left')\n",
    "    unraveled_data = unraveled_data.merge(unique_passages, on='passage', how='left')\n",
    "    return unraveled_data\n",
    "\n",
    "def process_dataset_optim(dataset_split):\n",
    "    unraveled_data = unravel_passages(dataset_split)\n",
    "    unique_queries, unique_passages = prepare_mappings_optim(unraveled_data)\n",
    "    # Directly use DataFrames for merging, do not convert to dict\n",
    "    unraveled_data = map_ids_optim(unraveled_data, unique_queries, unique_passages)\n",
    "    triplets_df = create_triplets_dataframe(unraveled_data)\n",
    "    return triplets_df, unique_queries, unique_passages\n",
    "\n",
    "\n",
    "def expand_triplets(triplets_df):\n",
    "    expanded_triplets = []\n",
    "    for index, row in triplets_df.iterrows():\n",
    "        query_id = row['query_id']\n",
    "        positive_passages = row['relevant']\n",
    "        negative_passages = row['irrelevant']\n",
    "        \n",
    "        for positive_passage_id, negative_passage_id in zip(positive_passages, negative_passages):\n",
    "            expanded_triplets.append({\n",
    "                'query_id': query_id,\n",
    "                'positive_passage_id': positive_passage_id,\n",
    "                'negative_passage_id': negative_passage_id\n",
    "            })\n",
    "    return pd.DataFrame(expanded_triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def expand_triplets_optimized(triplets_df):\n",
    "#     # Create a list to hold DataFrames for each row's expanded triplets\n",
    "#     dfs = []\n",
    "\n",
    "#     # Iterate over the DataFrame without using iterrows()\n",
    "#     for query_id, relevant, irrelevant in zip(triplets_df['query_id'], triplets_df['relevant'], triplets_df['irrelevant']):\n",
    "#         # Generate a DataFrame from the zipped lists of positive and negative passage IDs for this query\n",
    "#         df = pd.DataFrame({\n",
    "#             'query_id': query_id,\n",
    "#             'positive_passage_id': relevant,\n",
    "#             'negative_passage_id': irrelevant\n",
    "#         })\n",
    "#         dfs.append(df)\n",
    "\n",
    "#     # Concatenate all the individual DataFrames into one\n",
    "#     expanded_triplets_df = pd.concat(dfs, ignore_index=True)\n",
    "#     return expanded_triplets_df\n",
    "\n",
    "# def expand_triplets_preallocated(triplets_df):\n",
    "#     # Calculate the total number of triplets to preallocate DataFrame\n",
    "#     total_triplets = sum(len(relevant) for relevant in triplets_df['relevant'])\n",
    "    \n",
    "#     # Preallocate DataFrame\n",
    "#     expanded_df = pd.DataFrame(index=range(total_triplets), columns=['query_id', 'positive_passage_id', 'negative_passage_id'])\n",
    "    \n",
    "#     # Fill the DataFrame\n",
    "#     idx = 0\n",
    "#     for _, row in triplets_df.iterrows():\n",
    "#         n = len(row['relevant'])\n",
    "#         expanded_df.iloc[idx:idx+n, 0] = row['query_id']\n",
    "#         expanded_df.iloc[idx:idx+n, 1] = row['relevant']\n",
    "#         expanded_df.iloc[idx:idx+n, 2] = row['irrelevant']\n",
    "#         idx += n\n",
    "    \n",
    "#     return expanded_df\n",
    "\n",
    "# def expand_triplets_preallocated_2(triplets_df):\n",
    "#     # Estimate total size needed for preallocation\n",
    "#     total_size = sum(len(rel) for rel in triplets_df['relevant'])\n",
    "\n",
    "#     # Preallocate DataFrame with appropriate data types\n",
    "#     preallocated_df = pd.DataFrame({\n",
    "#         'query_id': pd.Series(dtype='int'),\n",
    "#         'positive_passage_id': pd.Series(dtype='int'),\n",
    "#         'negative_passage_id': pd.Series(dtype='int'),\n",
    "#     }, index=pd.RangeIndex(total_size))\n",
    "\n",
    "#     # Example of bulk assignment (adapt as needed)\n",
    "#     start_idx = 0\n",
    "#     for _, row in triplets_df.iterrows():\n",
    "#         end_idx = start_idx + len(row['relevant'])\n",
    "#         preallocated_df.iloc[start_idx:end_idx] = pd.DataFrame({\n",
    "#             'query_id': row['query_id'],\n",
    "#             'positive_passage_id': row['relevant'],\n",
    "#             'negative_passage_id': row['irrelevant'],\n",
    "#         }).values  # Using .values for direct assignment to avoid index alignment issues\n",
    "#         start_idx = end_idx\n",
    "\n",
    "#     return preallocated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ms_marco\", \"v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets, train_queries, train_passages = process_dataset_optim(pd.DataFrame(dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_triplets, test_queries, test_passages = process_dataset_optim(pd.DataFrame(dataset['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_triplets, val_queries, val_passages = process_dataset_optim(pd.DataFrame(dataset['validation']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
